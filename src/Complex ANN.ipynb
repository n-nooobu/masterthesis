{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 複素ANNによる非線形歪補償\n",
    "複素数を入力とする3層ANNによる補償"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "\n",
    "sys.path.append('../')\n",
    "from pyopt.util import save_pickle, load_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 データの整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_shaping(input_signal, signal, max_tap, tap):\n",
    "    \"\"\"\n",
    "    input_signal: 伝送前の信号\n",
    "    signal: 伝送後の信号\n",
    "    max_tap: 最大の同時入力シンボル数\n",
    "    tap: 同時入力シンボル数\n",
    "    \n",
    "    signal = [x_0, x_1, ... , x_(n-1)]\n",
    "      |\n",
    "      |\n",
    "      v\n",
    "    x = [[x_0, x_1, ... , x_tap-1],\n",
    "            [x_1, x_2, ..., x_tap],\n",
    "                   .\n",
    "                   .\n",
    "                   .\n",
    "            [x_(n-tap), x_(n-tap+1), ..., x(n-1)]]\n",
    "      |\n",
    "      |\n",
    "      v\n",
    "    x = [[i_0, q_0, i_1, q_1, ... , i_(tap-1), q_(tap-1)],\n",
    "            [i_1, q_1, i_2, q_2, ... , i_tap, q_tap],\n",
    "                   .\n",
    "                   .\n",
    "                   .\n",
    "            [i_(n-tap), q_(n-tap), i_(n-tap+1), q_(n-tap+1), ..., i_(n-1), q_(n-1)]] (batch, input_dim) input_dim = tap * 2\n",
    "    \n",
    "    y  (batch, output_dim) output_dim = 2\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.zeros((len(input_signal) - (max_tap - 1), tap, 2), dtype=float)\n",
    "    y = np.zeros((len(input_signal) - (max_tap - 1), 2), dtype=float)\n",
    "    for i, j in enumerate(np.arange(max_tap // 2, len(input_signal) - max_tap // 2)):\n",
    "        x[i, :, 0] = signal[j - tap // 2: j + tap // 2 + 1].real\n",
    "        x[i, :, 1] = signal[j - tap // 2: j + tap // 2 + 1].imag\n",
    "        y[i, 0] = input_signal[j].real\n",
    "        y[i, 1] = input_signal[j].imag\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size:  (1998, 1, 2)\n",
      "y size:  (1998, 2)\n",
      "[[-53019.95429361  53670.26520552]]\n",
      "[-70474.95606832  23491.65202277]\n"
     ]
    }
   ],
   "source": [
    "#動作確認\n",
    "tap = 1\n",
    "max_tap = 51\n",
    "\n",
    "df_dir = '../data/input/prbs.csv'\n",
    "df = pd.read_csv(df_dir, index_col=0)  # dataframe読み込み\n",
    "condition = (df['N']==13) & (df['itr']==1) & (df['form']=='RZ16QAM') & (df['n']==32) & (df['equalize']==False) & (df['baudrate']==28) & (df['PdBm']==1)\n",
    "sgnl = load_pickle(df[condition].iloc[0]['data_path'])  # dataframeから条件と合う行を取得し,pickleの保存先(data_path)にアクセス\n",
    "lc = sgnl.linear_compensation(500, sgnl.signal['x_500'])\n",
    "x, y = data_shaping(sgnl.signal['x_0'][16::32], lc[16::32], max_tap, tap)  # ANNに入力できるようにデータを整形\n",
    "\n",
    "print('x size: ', x.shape)\n",
    "print('y size: ', y.shape)\n",
    "print(x[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 51598.7578-53403.3672j, -32145.8477+96508.4375j,\n",
      "         -30606.3750+95283.5859j,  ...,\n",
      "         -93925.5391-34256.1562j, -30446.5664-13048.7129j,\n",
      "           9693.4248-29001.5801j],\n",
      "        [-32145.8477+96508.4375j, -30606.3750+95283.5859j,\n",
      "          32503.6094+13910.5205j,  ...,\n",
      "         -30446.5664-13048.7129j,   9693.4248-29001.5801j,\n",
      "          53869.6953+53203.7969j],\n",
      "        [-30606.3750+95283.5859j,  32503.6094+13910.5205j,\n",
      "           8248.6230+73391.8125j,  ...,\n",
      "           9693.4248-29001.5801j,  53869.6953+53203.7969j,\n",
      "         -74834.9922+12591.7324j],\n",
      "        ...,\n",
      "        [-57464.2109-54496.4727j, -71785.1406+16135.0635j,\n",
      "         -37748.6445-11243.9277j,  ...,\n",
      "         -10821.5039-70501.7188j, -72369.8047+16814.8672j,\n",
      "         -10232.1523+33094.5391j],\n",
      "        [-71785.1406+16135.0635j, -37748.6445-11243.9277j,\n",
      "          96017.2891+27587.5176j,  ...,\n",
      "         -72369.8047+16814.8672j, -10232.1523+33094.5391j,\n",
      "         -12742.2695-76211.6172j],\n",
      "        [-37748.6445-11243.9277j,  96017.2891+27587.5176j,\n",
      "          29571.5430+13914.4229j,  ...,\n",
      "         -10232.1523+33094.5391j, -12742.2695-76211.6172j,\n",
      "          12861.2490-31992.7305j]])\n"
     ]
    }
   ],
   "source": [
    "x_tensor = torch.Tensor(x)\n",
    "x_complex = torch.view_as_complex(x_tensor)\n",
    "print(x_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 平均,標準偏差の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  1138.625923374995\n",
      "std:  52116.95097240129\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(x)\n",
    "std = np.std(x)\n",
    "\n",
    "print('mean: ', mean)\n",
    "print('std: ', std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, x, y, mean, std):\n",
    "        self.x, self.y, self.mean, self.std = x, y, mean, std\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        \n",
    "        x = (x - self.mean) / self.std\n",
    "        y = (y - self.mean) / self.std\n",
    "        \n",
    "        x = torch.Tensor(x)\n",
    "        y = torch.Tensor(y)\n",
    "        \n",
    "        x_i = x[:, 0]\n",
    "        x_q = x[:, 1]\n",
    "        y_i = y[0]\n",
    "        y_q = y[1]\n",
    "        return x_i, x_q, y_i, y_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  -1.039174\n",
      "std:  0.0\n",
      "tensor([-1.0392])\n",
      "tensor(-1.3741)\n"
     ]
    }
   ],
   "source": [
    "#動作確認\n",
    "train_dataset = Dataset(x=x, y=y, mean=mean, std=std)\n",
    "\n",
    "index = 0\n",
    "x_i, x_q, y_i, y_q = train_dataset.__getitem__(index)\n",
    "x_array = x_i.detach().numpy()\n",
    "\n",
    "print('mean: ', np.mean(x_array))\n",
    "print('std: ', np.std(x_array))\n",
    "print(x_i)\n",
    "print(y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloaders_dict = {'train': train_dataloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zrelu(x_i, x_q):\n",
    "    x_i, x_q = x_i.clone(), x_q.clone()\n",
    "    for i in range(x_i.shape[0]):\n",
    "        for j in range(x_i.shape[1]):\n",
    "            if x_i[i, j] < 0 or x_q[i, j] < 0:\n",
    "                x_i[i, j] = 0\n",
    "                x_q[i, j] = 0\n",
    "    return x_i, x_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crelu(x_i, x_q):\n",
    "    x_i, x_q = x_i.clone(), x_q.clone()\n",
    "    x_i, x_q = F.relu(x_i), F.relu(x_q)\n",
    "    return x_i, x_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # 重み定義 Xavierの初期値\n",
    "        k = 1 / in_features\n",
    "        weight_i = torch.empty(out_features, in_features).uniform_(-math.sqrt(k), math.sqrt(k))\n",
    "        self.weight_i = nn.Parameter(weight_i)\n",
    "        weight_q = torch.empty(out_features, in_features).uniform_(-math.sqrt(k), math.sqrt(k))\n",
    "        self.weight_q = nn.Parameter(weight_q)\n",
    "        \n",
    "        bias_i = torch.empty(out_features).uniform_(-k, k)\n",
    "        self.bias_i = nn.Parameter(bias_i)\n",
    "        bias_q = torch.empty(out_features).uniform_(-k, k)\n",
    "        self.bias_q = nn.Parameter(bias_q)\n",
    "        \n",
    "    def forward(self, x_i, x_q):\n",
    "        i = nn.functional.linear(x_i, self.weight_i, self.bias_i)\n",
    "        q = nn.functional.linear(x_q, self.weight_q, self.bias_q)\n",
    "        return i - q, i + q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexANN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_neuron):\n",
    "        super().__init__()\n",
    "        self.fc1 = ComplexLinear(input_dim, hidden_neuron)\n",
    "        self.fc2 = ComplexLinear(hidden_neuron, output_dim)\n",
    "    \n",
    "    def forward(self, x_i, x_q):\n",
    "        x_i, x_q = self.fc1(x_i, x_q)\n",
    "        x_i, x_q = crelu(x_i, x_q)\n",
    "        x_i, x_q = self.fc2(x_i, x_q)\n",
    "        return x_i, x_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available now: cpu\n",
      "tensor([[-0.2868],\n",
      "        [-0.2514],\n",
      "        [ 0.0588],\n",
      "        [ 0.2231],\n",
      "        [-0.4915],\n",
      "        [-0.3835],\n",
      "        [ 0.1541],\n",
      "        [ 0.0466],\n",
      "        [ 0.3044],\n",
      "        [ 0.2885],\n",
      "        [-0.2808],\n",
      "        [-0.3512],\n",
      "        [-0.5910],\n",
      "        [ 0.0301],\n",
      "        [ 0.6442],\n",
      "        [ 0.4462],\n",
      "        [ 0.1734],\n",
      "        [-0.6990],\n",
      "        [ 0.6004],\n",
      "        [ 0.1038],\n",
      "        [ 0.5551],\n",
      "        [-0.0911],\n",
      "        [ 0.0028],\n",
      "        [ 0.0315],\n",
      "        [ 0.1260],\n",
      "        [ 0.0396],\n",
      "        [ 0.0013],\n",
      "        [ 0.4779],\n",
      "        [-0.2087],\n",
      "        [ 0.3371],\n",
      "        [-0.6761],\n",
      "        [ 0.5659],\n",
      "        [ 0.5817],\n",
      "        [-0.0913],\n",
      "        [ 0.1441],\n",
      "        [ 0.0471],\n",
      "        [-0.4767],\n",
      "        [-0.0612],\n",
      "        [-0.2799],\n",
      "        [-0.1568],\n",
      "        [ 0.1588],\n",
      "        [-0.1137],\n",
      "        [ 0.3336],\n",
      "        [-0.2513],\n",
      "        [ 0.7978],\n",
      "        [ 0.0233],\n",
      "        [-0.1835],\n",
      "        [ 0.6011],\n",
      "        [ 0.0310],\n",
      "        [ 0.0768],\n",
      "        [-0.2429],\n",
      "        [ 0.3639],\n",
      "        [-0.2769],\n",
      "        [ 0.0586],\n",
      "        [ 0.4753],\n",
      "        [ 0.2043],\n",
      "        [-0.2697],\n",
      "        [ 0.4949],\n",
      "        [ 0.2047],\n",
      "        [-0.0366],\n",
      "        [-0.0629],\n",
      "        [ 0.0910],\n",
      "        [ 0.3754],\n",
      "        [ 0.0817],\n",
      "        [ 0.3861],\n",
      "        [ 0.6442],\n",
      "        [-0.2869],\n",
      "        [-0.4232],\n",
      "        [ 0.1154],\n",
      "        [ 0.1367],\n",
      "        [-0.0390],\n",
      "        [ 0.0526],\n",
      "        [ 0.5214],\n",
      "        [ 0.0674],\n",
      "        [-0.1119],\n",
      "        [ 0.3114],\n",
      "        [ 0.5276],\n",
      "        [ 0.1092],\n",
      "        [-0.2922],\n",
      "        [-0.8909],\n",
      "        [-0.1134],\n",
      "        [-0.2413],\n",
      "        [ 0.1101],\n",
      "        [-0.0170],\n",
      "        [ 0.0152],\n",
      "        [ 0.2938],\n",
      "        [ 0.8369],\n",
      "        [ 0.2179],\n",
      "        [-0.1458],\n",
      "        [ 0.4233],\n",
      "        [-0.3601],\n",
      "        [-0.2788],\n",
      "        [-0.3367],\n",
      "        [-0.2227],\n",
      "        [ 0.1127],\n",
      "        [ 0.4377],\n",
      "        [ 0.5208],\n",
      "        [ 0.1831],\n",
      "        [-0.0580],\n",
      "        [-0.3564]], grad_fn=<SubBackward0>)\n",
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "#動作確認\n",
    "hidden_neuron = 300\n",
    "device = torch.device('cpu') # 'cuda' if torch.cuda.is_available() else \n",
    "print('Device available now:', device)\n",
    "\n",
    "model = ComplexANN(input_dim=tap, output_dim=1, hidden_neuron=hidden_neuron).to(device)\n",
    "for x_i, x_q, y_i, y_q in train_dataloader:\n",
    "    out_i, out_q = model(x_i, x_q)\n",
    "    print(out_i)\n",
    "    print(out_i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. train定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evm_score(x_i, x_q, y_i, y_q):\n",
    "    tmp = 0\n",
    "    for i in range(len(x_i)):\n",
    "        tmp += ((x_i[i] - y_i[i]) ** 2 + (x_q[i] - y_q[i]) ** 2) / (y_i[i] ** 2 + y_q[i] ** 2)\n",
    "    evm = torch.sqrt(tmp / len(x_i))\n",
    "    return evm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(device, model, dataloaders_dict, criterion, optimizer, epochs, epochs_section=None):\n",
    "    for epoch in range(epochs):\n",
    "        if epochs_section is not None:\n",
    "            epoch += epochs_section[0]\n",
    "            end_epoch = epochs_section[1]\n",
    "        else:\n",
    "            end_epoch = epochs\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for phase in dataloaders_dict.keys():\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            epoch_loss = 0.0\n",
    "            epoch_evms = 0.0\n",
    "            \n",
    "            for x_i, x_q, y_i, y_q in dataloaders_dict[phase]:\n",
    "                x_i, x_q = x_i.to(device), x_q.to(device)\n",
    "                print(x_i.shape)\n",
    "                y_i, y_q = y_i.to(device), y_q.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    out_i, out_q = model(x_i, x_q)\n",
    "                    loss = evm_score(out_i, out_q, y_i, y_q)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    epoch_loss += loss.item() * x_i.size(0)\n",
    "                    epoch_evms += (evm_score(out_i, out_q, y_i, y_q)) ** 2 * x_i.size(0)\n",
    "            \n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_evm = torch.sqrt(epoch_evms / len(dataloaders_dict[phase].dataset)) * 100\n",
    "            \n",
    "            duration = str(datetime.timedelta(seconds=time.time() - start_time))[:7]\n",
    "            print('{} | Epoch: {}/{} | {} Loss: {:.4} | EVM: {:.4}'.format(duration, epoch + 1, end_epoch, phase, epoch_loss, epoch_evm[0]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([48, 201])\n",
      "0:00:00 | Epoch: 1/5 | train Loss: 0.06023 | EVM: 6.171\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([68, 201])\n",
      "0:00:11 | Epoch: 1/5 | val Loss: 0.2561 | EVM: 25.68\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 201])\n",
      "torch.Size([48, 201])\n",
      "0:00:00 | Epoch: 2/5 | train Loss: 0.08536 | EVM: 8.608\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n",
      "torch.Size([100, 201])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-5591af25e5ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataloaders_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-e145a372e719>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(device, model, dataloaders_dict, criterion, optimizer, epochs, epochs_section)\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0my_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_q\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\uenonobuaki\\pycharmprojects\\masterthesis\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#動作確認\n",
    "epochs = 5\n",
    "lr = 0.001\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "train_model(device=device, model=model, dataloaders_dict=dataloaders_dict, criterion=criterion, optimizer=optimizer, epochs=epochs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available now: cpu\n",
      "0:00:01 | Epoch: 1/500 | train Loss: 1.088 | EVM: 108.9\n",
      "0:00:11 | Epoch: 1/500 | val Loss: 0.9937 | EVM: 99.42\n",
      "0:00:01 | Epoch: 2/500 | train Loss: 0.7547 | EVM: 75.56\n",
      "0:00:11 | Epoch: 2/500 | val Loss: 0.8873 | EVM: 88.78\n",
      "0:00:01 | Epoch: 3/500 | train Loss: 0.5538 | EVM: 55.43\n",
      "0:00:11 | Epoch: 3/500 | val Loss: 0.7985 | EVM: 79.92\n",
      "0:00:00 | Epoch: 4/500 | train Loss: 0.3874 | EVM: 38.85\n",
      "0:00:11 | Epoch: 4/500 | val Loss: 0.7117 | EVM: 71.26\n",
      "0:00:00 | Epoch: 5/500 | train Loss: 0.3065 | EVM: 30.76\n",
      "0:00:10 | Epoch: 5/500 | val Loss: 0.6638 | EVM: 66.49\n",
      "0:00:00 | Epoch: 6/500 | train Loss: 0.4017 | EVM: 40.4\n",
      "0:00:11 | Epoch: 6/500 | val Loss: 0.643 | EVM: 64.39\n",
      "0:00:00 | Epoch: 7/500 | train Loss: 0.3858 | EVM: 39.13\n",
      "0:00:11 | Epoch: 7/500 | val Loss: 0.6146 | EVM: 61.54\n",
      "0:00:00 | Epoch: 8/500 | train Loss: 0.3322 | EVM: 33.65\n",
      "0:00:11 | Epoch: 8/500 | val Loss: 0.6024 | EVM: 60.33\n",
      "0:00:00 | Epoch: 9/500 | train Loss: 0.2902 | EVM: 29.27\n",
      "0:00:11 | Epoch: 9/500 | val Loss: 0.5921 | EVM: 59.29\n",
      "0:00:00 | Epoch: 10/500 | train Loss: 0.2874 | EVM: 28.98\n",
      "0:00:11 | Epoch: 10/500 | val Loss: 0.5694 | EVM: 57.02\n",
      "0:00:00 | Epoch: 11/500 | train Loss: 0.2429 | EVM: 24.66\n",
      "0:00:11 | Epoch: 11/500 | val Loss: 0.5546 | EVM: 55.54\n",
      "0:00:00 | Epoch: 12/500 | train Loss: 0.2352 | EVM: 23.75\n",
      "0:00:11 | Epoch: 12/500 | val Loss: 0.5453 | EVM: 54.61\n",
      "0:00:00 | Epoch: 13/500 | train Loss: 0.2294 | EVM: 23.33\n",
      "0:00:11 | Epoch: 13/500 | val Loss: 0.5339 | EVM: 53.47\n",
      "0:00:00 | Epoch: 14/500 | train Loss: 0.2271 | EVM: 22.87\n",
      "0:00:11 | Epoch: 14/500 | val Loss: 0.5268 | EVM: 52.75\n",
      "0:00:00 | Epoch: 15/500 | train Loss: 0.2072 | EVM: 20.91\n",
      "0:00:10 | Epoch: 15/500 | val Loss: 0.5175 | EVM: 51.82\n",
      "0:00:00 | Epoch: 16/500 | train Loss: 0.2074 | EVM: 20.98\n",
      "0:00:11 | Epoch: 16/500 | val Loss: 0.5136 | EVM: 51.44\n",
      "0:00:00 | Epoch: 17/500 | train Loss: 0.2248 | EVM: 22.69\n",
      "0:00:11 | Epoch: 17/500 | val Loss: 0.5104 | EVM: 51.12\n",
      "0:00:00 | Epoch: 18/500 | train Loss: 0.2009 | EVM: 20.29\n",
      "0:00:11 | Epoch: 18/500 | val Loss: 0.4965 | EVM: 49.72\n",
      "0:00:00 | Epoch: 19/500 | train Loss: 0.1849 | EVM: 18.62\n",
      "0:00:11 | Epoch: 19/500 | val Loss: 0.4944 | EVM: 49.51\n",
      "0:00:00 | Epoch: 20/500 | train Loss: 0.1883 | EVM: 19.13\n",
      "0:00:11 | Epoch: 20/500 | val Loss: 0.4846 | EVM: 48.53\n",
      "0:00:00 | Epoch: 21/500 | train Loss: 0.1687 | EVM: 17.03\n",
      "0:00:11 | Epoch: 21/500 | val Loss: 0.4771 | EVM: 47.77\n",
      "0:00:00 | Epoch: 22/500 | train Loss: 0.1738 | EVM: 17.71\n",
      "0:00:11 | Epoch: 22/500 | val Loss: 0.4748 | EVM: 47.55\n",
      "0:00:00 | Epoch: 23/500 | train Loss: 0.1629 | EVM: 16.55\n",
      "0:00:11 | Epoch: 23/500 | val Loss: 0.4724 | EVM: 47.31\n",
      "0:00:00 | Epoch: 24/500 | train Loss: 0.1672 | EVM: 17.14\n",
      "0:00:12 | Epoch: 24/500 | val Loss: 0.4619 | EVM: 46.25\n",
      "0:00:00 | Epoch: 25/500 | train Loss: 0.1664 | EVM: 16.89\n",
      "0:00:11 | Epoch: 25/500 | val Loss: 0.4602 | EVM: 46.09\n",
      "0:00:01 | Epoch: 26/500 | train Loss: 0.1635 | EVM: 16.51\n",
      "0:00:11 | Epoch: 26/500 | val Loss: 0.4536 | EVM: 45.43\n",
      "0:00:00 | Epoch: 27/500 | train Loss: 0.1489 | EVM: 15.02\n",
      "0:00:11 | Epoch: 27/500 | val Loss: 0.4545 | EVM: 45.52\n",
      "0:00:00 | Epoch: 28/500 | train Loss: 0.1639 | EVM: 16.5\n",
      "0:00:11 | Epoch: 28/500 | val Loss: 0.45 | EVM: 45.07\n",
      "0:00:00 | Epoch: 29/500 | train Loss: 0.1457 | EVM: 14.7\n",
      "0:00:12 | Epoch: 29/500 | val Loss: 0.444 | EVM: 44.47\n",
      "0:00:00 | Epoch: 30/500 | train Loss: 0.1486 | EVM: 15.21\n",
      "0:00:12 | Epoch: 30/500 | val Loss: 0.4367 | EVM: 43.74\n",
      "0:00:00 | Epoch: 31/500 | train Loss: 0.133 | EVM: 13.37\n",
      "0:00:11 | Epoch: 31/500 | val Loss: 0.4386 | EVM: 43.92\n",
      "0:00:00 | Epoch: 32/500 | train Loss: 0.1429 | EVM: 14.44\n",
      "0:00:11 | Epoch: 32/500 | val Loss: 0.4328 | EVM: 43.34\n",
      "0:00:01 | Epoch: 33/500 | train Loss: 0.1383 | EVM: 14.01\n",
      "0:00:11 | Epoch: 33/500 | val Loss: 0.4295 | EVM: 43.02\n",
      "0:00:00 | Epoch: 34/500 | train Loss: 0.1481 | EVM: 15.12\n",
      "0:00:11 | Epoch: 34/500 | val Loss: 0.4304 | EVM: 43.1\n",
      "0:00:00 | Epoch: 35/500 | train Loss: 0.1339 | EVM: 13.54\n",
      "0:00:12 | Epoch: 35/500 | val Loss: 0.4258 | EVM: 42.64\n",
      "0:00:01 | Epoch: 36/500 | train Loss: 0.1306 | EVM: 13.19\n",
      "0:00:12 | Epoch: 36/500 | val Loss: 0.4214 | EVM: 42.2\n",
      "0:00:00 | Epoch: 37/500 | train Loss: 0.1361 | EVM: 13.66\n",
      "0:00:11 | Epoch: 37/500 | val Loss: 0.4267 | EVM: 42.74\n",
      "0:00:00 | Epoch: 38/500 | train Loss: 0.1261 | EVM: 12.82\n",
      "0:00:11 | Epoch: 38/500 | val Loss: 0.4124 | EVM: 41.31\n",
      "0:00:00 | Epoch: 39/500 | train Loss: 0.1307 | EVM: 13.23\n",
      "0:00:11 | Epoch: 39/500 | val Loss: 0.4151 | EVM: 41.57\n",
      "0:00:00 | Epoch: 40/500 | train Loss: 0.1218 | EVM: 12.36\n",
      "0:00:11 | Epoch: 40/500 | val Loss: 0.409 | EVM: 40.97\n",
      "0:00:00 | Epoch: 41/500 | train Loss: 0.1279 | EVM: 12.88\n",
      "0:00:12 | Epoch: 41/500 | val Loss: 0.4086 | EVM: 40.92\n",
      "0:00:00 | Epoch: 42/500 | train Loss: 0.12 | EVM: 12.16\n",
      "0:00:12 | Epoch: 42/500 | val Loss: 0.4079 | EVM: 40.86\n",
      "0:00:01 | Epoch: 43/500 | train Loss: 0.1175 | EVM: 11.83\n",
      "0:00:11 | Epoch: 43/500 | val Loss: 0.4027 | EVM: 40.33\n",
      "0:00:00 | Epoch: 44/500 | train Loss: 0.1174 | EVM: 11.84\n",
      "0:00:11 | Epoch: 44/500 | val Loss: 0.4011 | EVM: 40.18\n",
      "0:00:00 | Epoch: 45/500 | train Loss: 0.1245 | EVM: 12.54\n",
      "0:00:11 | Epoch: 45/500 | val Loss: 0.4014 | EVM: 40.2\n",
      "0:00:01 | Epoch: 46/500 | train Loss: 0.1096 | EVM: 11.13\n",
      "0:00:11 | Epoch: 46/500 | val Loss: 0.3945 | EVM: 39.51\n",
      "0:00:00 | Epoch: 47/500 | train Loss: 0.1154 | EVM: 11.77\n",
      "0:00:12 | Epoch: 47/500 | val Loss: 0.3984 | EVM: 39.91\n",
      "0:00:00 | Epoch: 48/500 | train Loss: 0.112 | EVM: 11.4\n",
      "0:00:11 | Epoch: 48/500 | val Loss: 0.3898 | EVM: 39.04\n",
      "0:00:00 | Epoch: 49/500 | train Loss: 0.1157 | EVM: 11.72\n",
      "0:00:11 | Epoch: 49/500 | val Loss: 0.3888 | EVM: 38.94\n",
      "0:00:00 | Epoch: 50/500 | train Loss: 0.1156 | EVM: 11.65\n",
      "0:00:11 | Epoch: 50/500 | val Loss: 0.3894 | EVM: 39.0\n",
      "0:00:00 | Epoch: 51/500 | train Loss: 0.1153 | EVM: 11.61\n",
      "0:00:11 | Epoch: 51/500 | val Loss: 0.3873 | EVM: 38.78\n",
      "0:00:00 | Epoch: 52/500 | train Loss: 0.1125 | EVM: 11.38\n",
      "0:00:11 | Epoch: 52/500 | val Loss: 0.3851 | EVM: 38.57\n",
      "0:00:00 | Epoch: 53/500 | train Loss: 0.1082 | EVM: 10.92\n",
      "0:00:11 | Epoch: 53/500 | val Loss: 0.3849 | EVM: 38.55\n",
      "0:00:00 | Epoch: 54/500 | train Loss: 0.1152 | EVM: 11.69\n",
      "0:00:11 | Epoch: 54/500 | val Loss: 0.3863 | EVM: 38.69\n",
      "0:00:00 | Epoch: 55/500 | train Loss: 0.1186 | EVM: 11.96\n",
      "0:00:11 | Epoch: 55/500 | val Loss: 0.3819 | EVM: 38.25\n",
      "0:00:00 | Epoch: 56/500 | train Loss: 0.1166 | EVM: 11.77\n",
      "0:00:11 | Epoch: 56/500 | val Loss: 0.3762 | EVM: 37.69\n",
      "0:00:00 | Epoch: 57/500 | train Loss: 0.1017 | EVM: 10.26\n",
      "0:00:11 | Epoch: 57/500 | val Loss: 0.3774 | EVM: 37.8\n",
      "0:00:01 | Epoch: 58/500 | train Loss: 0.09768 | EVM: 9.886\n",
      "0:00:11 | Epoch: 58/500 | val Loss: 0.3727 | EVM: 37.33\n",
      "0:00:00 | Epoch: 59/500 | train Loss: 0.09511 | EVM: 9.665\n",
      "0:00:11 | Epoch: 59/500 | val Loss: 0.3732 | EVM: 37.39\n",
      "0:00:00 | Epoch: 60/500 | train Loss: 0.1027 | EVM: 10.44\n",
      "0:00:11 | Epoch: 60/500 | val Loss: 0.3743 | EVM: 37.49\n",
      "0:00:01 | Epoch: 61/500 | train Loss: 0.09797 | EVM: 9.904\n",
      "0:00:11 | Epoch: 61/500 | val Loss: 0.3724 | EVM: 37.3\n",
      "0:00:00 | Epoch: 62/500 | train Loss: 0.1033 | EVM: 10.4\n",
      "0:00:11 | Epoch: 62/500 | val Loss: 0.3712 | EVM: 37.18\n",
      "0:00:00 | Epoch: 63/500 | train Loss: 0.1058 | EVM: 10.75\n",
      "0:00:11 | Epoch: 63/500 | val Loss: 0.3732 | EVM: 37.39\n",
      "0:00:00 | Epoch: 64/500 | train Loss: 0.11 | EVM: 11.16\n",
      "0:00:11 | Epoch: 64/500 | val Loss: 0.369 | EVM: 36.96\n",
      "0:00:00 | Epoch: 65/500 | train Loss: 0.09715 | EVM: 9.879\n",
      "0:00:11 | Epoch: 65/500 | val Loss: 0.3622 | EVM: 36.28\n",
      "0:00:01 | Epoch: 66/500 | train Loss: 0.08925 | EVM: 8.975\n",
      "0:00:11 | Epoch: 66/500 | val Loss: 0.3632 | EVM: 36.38\n",
      "0:00:00 | Epoch: 67/500 | train Loss: 0.09759 | EVM: 9.854\n",
      "0:00:10 | Epoch: 67/500 | val Loss: 0.3651 | EVM: 36.57\n",
      "0:00:00 | Epoch: 68/500 | train Loss: 0.09597 | EVM: 9.769\n",
      "0:00:11 | Epoch: 68/500 | val Loss: 0.365 | EVM: 36.56\n",
      "0:00:00 | Epoch: 69/500 | train Loss: 0.1054 | EVM: 10.65\n",
      "0:00:11 | Epoch: 69/500 | val Loss: 0.3613 | EVM: 36.2\n",
      "0:00:00 | Epoch: 70/500 | train Loss: 0.1035 | EVM: 10.45\n",
      "0:00:11 | Epoch: 70/500 | val Loss: 0.3644 | EVM: 36.5\n",
      "0:00:00 | Epoch: 71/500 | train Loss: 0.09647 | EVM: 9.781\n",
      "0:00:11 | Epoch: 71/500 | val Loss: 0.3579 | EVM: 35.85\n",
      "0:00:00 | Epoch: 72/500 | train Loss: 0.1001 | EVM: 10.17\n",
      "0:00:11 | Epoch: 72/500 | val Loss: 0.3572 | EVM: 35.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01 | Epoch: 73/500 | train Loss: 0.09286 | EVM: 9.385\n",
      "0:00:12 | Epoch: 73/500 | val Loss: 0.3553 | EVM: 35.59\n",
      "0:00:00 | Epoch: 74/500 | train Loss: 0.09838 | EVM: 10.02\n",
      "0:00:11 | Epoch: 74/500 | val Loss: 0.3555 | EVM: 35.61\n",
      "0:00:00 | Epoch: 75/500 | train Loss: 0.1056 | EVM: 10.67\n",
      "0:00:11 | Epoch: 75/500 | val Loss: 0.352 | EVM: 35.26\n",
      "0:00:01 | Epoch: 76/500 | train Loss: 0.09235 | EVM: 9.322\n",
      "0:00:12 | Epoch: 76/500 | val Loss: 0.3534 | EVM: 35.41\n",
      "0:00:01 | Epoch: 77/500 | train Loss: 0.1021 | EVM: 10.4\n",
      "0:00:12 | Epoch: 77/500 | val Loss: 0.3505 | EVM: 35.12\n",
      "0:00:00 | Epoch: 78/500 | train Loss: 0.08869 | EVM: 9.005\n",
      "0:00:11 | Epoch: 78/500 | val Loss: 0.3496 | EVM: 35.02\n",
      "0:00:00 | Epoch: 79/500 | train Loss: 0.08887 | EVM: 8.986\n",
      "0:00:11 | Epoch: 79/500 | val Loss: 0.3525 | EVM: 35.32\n",
      "0:00:00 | Epoch: 80/500 | train Loss: 0.1001 | EVM: 10.15\n",
      "0:00:11 | Epoch: 80/500 | val Loss: 0.3511 | EVM: 35.17\n",
      "0:00:00 | Epoch: 81/500 | train Loss: 0.09684 | EVM: 9.754\n",
      "0:00:11 | Epoch: 81/500 | val Loss: 0.3466 | EVM: 34.72\n",
      "0:00:01 | Epoch: 82/500 | train Loss: 0.0866 | EVM: 8.737\n",
      "0:00:11 | Epoch: 82/500 | val Loss: 0.3433 | EVM: 34.39\n",
      "0:00:00 | Epoch: 83/500 | train Loss: 0.09185 | EVM: 9.328\n",
      "0:00:11 | Epoch: 83/500 | val Loss: 0.342 | EVM: 34.27\n",
      "0:00:00 | Epoch: 84/500 | train Loss: 0.08857 | EVM: 9.005\n",
      "0:00:11 | Epoch: 84/500 | val Loss: 0.3402 | EVM: 34.08\n",
      "0:00:00 | Epoch: 85/500 | train Loss: 0.09332 | EVM: 9.466\n",
      "0:00:11 | Epoch: 85/500 | val Loss: 0.3464 | EVM: 34.7\n",
      "0:00:00 | Epoch: 86/500 | train Loss: 0.08924 | EVM: 9.041\n",
      "0:00:11 | Epoch: 86/500 | val Loss: 0.3415 | EVM: 34.21\n",
      "0:00:01 | Epoch: 87/500 | train Loss: 0.09249 | EVM: 9.397\n",
      "0:00:12 | Epoch: 87/500 | val Loss: 0.3407 | EVM: 34.14\n",
      "0:00:00 | Epoch: 88/500 | train Loss: 0.08599 | EVM: 8.675\n",
      "0:00:12 | Epoch: 88/500 | val Loss: 0.3361 | EVM: 33.67\n",
      "0:00:00 | Epoch: 89/500 | train Loss: 0.09012 | EVM: 9.136\n",
      "0:00:12 | Epoch: 89/500 | val Loss: 0.3367 | EVM: 33.74\n",
      "0:00:00 | Epoch: 90/500 | train Loss: 0.08728 | EVM: 8.832\n",
      "0:00:11 | Epoch: 90/500 | val Loss: 0.3375 | EVM: 33.81\n",
      "0:00:00 | Epoch: 91/500 | train Loss: 0.09393 | EVM: 9.483\n",
      "0:00:11 | Epoch: 91/500 | val Loss: 0.3384 | EVM: 33.9\n",
      "0:00:01 | Epoch: 92/500 | train Loss: 0.07928 | EVM: 8.031\n",
      "0:00:11 | Epoch: 92/500 | val Loss: 0.3363 | EVM: 33.69\n",
      "0:00:00 | Epoch: 93/500 | train Loss: 0.08762 | EVM: 8.861\n",
      "0:00:11 | Epoch: 93/500 | val Loss: 0.3323 | EVM: 33.3\n",
      "0:00:01 | Epoch: 94/500 | train Loss: 0.08621 | EVM: 8.768\n",
      "0:00:11 | Epoch: 94/500 | val Loss: 0.3376 | EVM: 33.82\n",
      "0:00:00 | Epoch: 95/500 | train Loss: 0.08392 | EVM: 8.519\n",
      "0:00:11 | Epoch: 95/500 | val Loss: 0.3285 | EVM: 32.92\n",
      "0:00:00 | Epoch: 96/500 | train Loss: 0.0794 | EVM: 8.063\n",
      "0:00:11 | Epoch: 96/500 | val Loss: 0.3263 | EVM: 32.69\n",
      "0:00:00 | Epoch: 97/500 | train Loss: 0.08431 | EVM: 8.58\n",
      "0:00:11 | Epoch: 97/500 | val Loss: 0.3351 | EVM: 33.57\n",
      "0:00:01 | Epoch: 98/500 | train Loss: 0.08939 | EVM: 9.041\n",
      "0:00:12 | Epoch: 98/500 | val Loss: 0.3331 | EVM: 33.37\n",
      "0:00:00 | Epoch: 99/500 | train Loss: 0.09009 | EVM: 9.126\n",
      "0:00:12 | Epoch: 99/500 | val Loss: 0.3281 | EVM: 32.87\n",
      "0:00:00 | Epoch: 100/500 | train Loss: 0.07734 | EVM: 7.794\n",
      "0:00:12 | Epoch: 100/500 | val Loss: 0.3299 | EVM: 33.05\n",
      "0:00:00 | Epoch: 101/500 | train Loss: 0.07684 | EVM: 7.743\n",
      "0:00:11 | Epoch: 101/500 | val Loss: 0.3275 | EVM: 32.82\n",
      "0:00:00 | Epoch: 102/500 | train Loss: 0.08008 | EVM: 8.131\n",
      "0:00:11 | Epoch: 102/500 | val Loss: 0.325 | EVM: 32.56\n",
      "0:00:00 | Epoch: 103/500 | train Loss: 0.07887 | EVM: 7.975\n",
      "0:00:11 | Epoch: 103/500 | val Loss: 0.3237 | EVM: 32.43\n",
      "0:00:00 | Epoch: 104/500 | train Loss: 0.07917 | EVM: 8.026\n",
      "0:00:12 | Epoch: 104/500 | val Loss: 0.3261 | EVM: 32.67\n",
      "0:00:00 | Epoch: 105/500 | train Loss: 0.08136 | EVM: 8.256\n",
      "0:00:12 | Epoch: 105/500 | val Loss: 0.3253 | EVM: 32.59\n",
      "0:00:00 | Epoch: 106/500 | train Loss: 0.08626 | EVM: 8.815\n",
      "0:00:11 | Epoch: 106/500 | val Loss: 0.3248 | EVM: 32.55\n",
      "0:00:00 | Epoch: 107/500 | train Loss: 0.08436 | EVM: 8.561\n",
      "0:00:11 | Epoch: 107/500 | val Loss: 0.325 | EVM: 32.56\n",
      "0:00:00 | Epoch: 108/500 | train Loss: 0.08285 | EVM: 8.404\n",
      "0:00:11 | Epoch: 108/500 | val Loss: 0.3231 | EVM: 32.37\n",
      "0:00:00 | Epoch: 109/500 | train Loss: 0.08315 | EVM: 8.378\n",
      "0:00:11 | Epoch: 109/500 | val Loss: 0.3254 | EVM: 32.6\n",
      "0:00:00 | Epoch: 110/500 | train Loss: 0.08339 | EVM: 8.427\n",
      "0:00:11 | Epoch: 110/500 | val Loss: 0.3218 | EVM: 32.24\n",
      "0:00:00 | Epoch: 111/500 | train Loss: 0.07838 | EVM: 7.954\n",
      "0:00:12 | Epoch: 111/500 | val Loss: 0.3193 | EVM: 31.99\n",
      "0:00:01 | Epoch: 112/500 | train Loss: 0.07896 | EVM: 8.033\n",
      "0:00:11 | Epoch: 112/500 | val Loss: 0.3208 | EVM: 32.14\n",
      "0:00:00 | Epoch: 113/500 | train Loss: 0.07762 | EVM: 7.852\n",
      "0:00:11 | Epoch: 113/500 | val Loss: 0.3184 | EVM: 31.9\n",
      "0:00:00 | Epoch: 114/500 | train Loss: 0.0723 | EVM: 7.327\n",
      "0:00:11 | Epoch: 114/500 | val Loss: 0.3211 | EVM: 32.17\n",
      "0:00:00 | Epoch: 115/500 | train Loss: 0.07848 | EVM: 7.895\n",
      "0:00:11 | Epoch: 115/500 | val Loss: 0.3148 | EVM: 31.55\n",
      "0:00:00 | Epoch: 116/500 | train Loss: 0.07866 | EVM: 7.899\n",
      "0:00:11 | Epoch: 116/500 | val Loss: 0.3211 | EVM: 32.17\n",
      "0:00:00 | Epoch: 117/500 | train Loss: 0.08144 | EVM: 8.22\n",
      "0:00:11 | Epoch: 117/500 | val Loss: 0.3164 | EVM: 31.7\n",
      "0:00:00 | Epoch: 118/500 | train Loss: 0.08039 | EVM: 8.082\n",
      "0:00:11 | Epoch: 118/500 | val Loss: 0.3172 | EVM: 31.78\n",
      "0:00:00 | Epoch: 119/500 | train Loss: 0.0744 | EVM: 7.497\n",
      "0:00:11 | Epoch: 119/500 | val Loss: 0.3183 | EVM: 31.89\n",
      "0:00:01 | Epoch: 120/500 | train Loss: 0.08082 | EVM: 8.239\n",
      "0:00:11 | Epoch: 120/500 | val Loss: 0.3146 | EVM: 31.52\n",
      "0:00:00 | Epoch: 121/500 | train Loss: 0.07833 | EVM: 7.904\n",
      "0:00:11 | Epoch: 121/500 | val Loss: 0.3132 | EVM: 31.38\n",
      "0:00:00 | Epoch: 122/500 | train Loss: 0.07469 | EVM: 7.563\n",
      "0:00:11 | Epoch: 122/500 | val Loss: 0.3119 | EVM: 31.25\n",
      "0:00:00 | Epoch: 123/500 | train Loss: 0.07765 | EVM: 7.798\n",
      "0:00:11 | Epoch: 123/500 | val Loss: 0.3122 | EVM: 31.28\n",
      "0:00:00 | Epoch: 124/500 | train Loss: 0.07338 | EVM: 7.416\n",
      "0:00:11 | Epoch: 124/500 | val Loss: 0.3179 | EVM: 31.85\n",
      "0:00:00 | Epoch: 125/500 | train Loss: 0.08411 | EVM: 8.521\n",
      "0:00:11 | Epoch: 125/500 | val Loss: 0.3148 | EVM: 31.55\n",
      "0:00:00 | Epoch: 126/500 | train Loss: 0.08574 | EVM: 8.787\n",
      "0:00:11 | Epoch: 126/500 | val Loss: 0.3124 | EVM: 31.31\n",
      "0:00:00 | Epoch: 127/500 | train Loss: 0.07554 | EVM: 7.631\n",
      "0:00:11 | Epoch: 127/500 | val Loss: 0.306 | EVM: 30.66\n",
      "0:00:01 | Epoch: 128/500 | train Loss: 0.07204 | EVM: 7.352\n",
      "0:00:11 | Epoch: 128/500 | val Loss: 0.3092 | EVM: 30.98\n",
      "0:00:00 | Epoch: 129/500 | train Loss: 0.07274 | EVM: 7.374\n",
      "0:00:11 | Epoch: 129/500 | val Loss: 0.3094 | EVM: 31.0\n",
      "0:00:01 | Epoch: 130/500 | train Loss: 0.07174 | EVM: 7.274\n",
      "0:00:11 | Epoch: 130/500 | val Loss: 0.3111 | EVM: 31.17\n",
      "0:00:00 | Epoch: 131/500 | train Loss: 0.07884 | EVM: 7.941\n",
      "0:00:11 | Epoch: 131/500 | val Loss: 0.3085 | EVM: 30.91\n",
      "0:00:01 | Epoch: 132/500 | train Loss: 0.08207 | EVM: 8.297\n",
      "0:00:11 | Epoch: 132/500 | val Loss: 0.3077 | EVM: 30.83\n",
      "0:00:00 | Epoch: 133/500 | train Loss: 0.07359 | EVM: 7.473\n",
      "0:00:11 | Epoch: 133/500 | val Loss: 0.3068 | EVM: 30.74\n",
      "0:00:00 | Epoch: 134/500 | train Loss: 0.07686 | EVM: 7.775\n",
      "0:00:11 | Epoch: 134/500 | val Loss: 0.3071 | EVM: 30.77\n",
      "0:00:00 | Epoch: 135/500 | train Loss: 0.07263 | EVM: 7.321\n",
      "0:00:11 | Epoch: 135/500 | val Loss: 0.3069 | EVM: 30.75\n",
      "0:00:00 | Epoch: 136/500 | train Loss: 0.07595 | EVM: 7.737\n",
      "0:00:11 | Epoch: 136/500 | val Loss: 0.3085 | EVM: 30.92\n",
      "0:00:00 | Epoch: 137/500 | train Loss: 0.07549 | EVM: 7.656\n",
      "0:00:12 | Epoch: 137/500 | val Loss: 0.3045 | EVM: 30.51\n",
      "0:00:00 | Epoch: 138/500 | train Loss: 0.07302 | EVM: 7.428\n",
      "0:00:11 | Epoch: 138/500 | val Loss: 0.3047 | EVM: 30.53\n",
      "0:00:01 | Epoch: 139/500 | train Loss: 0.07939 | EVM: 8.118\n",
      "0:00:12 | Epoch: 139/500 | val Loss: 0.3002 | EVM: 30.08\n",
      "0:00:00 | Epoch: 140/500 | train Loss: 0.07436 | EVM: 7.493\n",
      "0:00:11 | Epoch: 140/500 | val Loss: 0.3018 | EVM: 30.24\n",
      "0:00:00 | Epoch: 141/500 | train Loss: 0.07008 | EVM: 7.065\n",
      "0:00:11 | Epoch: 141/500 | val Loss: 0.303 | EVM: 30.36\n",
      "0:00:00 | Epoch: 142/500 | train Loss: 0.06988 | EVM: 7.071\n",
      "0:00:11 | Epoch: 142/500 | val Loss: 0.3007 | EVM: 30.13\n",
      "0:00:00 | Epoch: 143/500 | train Loss: 0.0673 | EVM: 6.779\n",
      "0:00:12 | Epoch: 143/500 | val Loss: 0.3023 | EVM: 30.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00 | Epoch: 144/500 | train Loss: 0.07083 | EVM: 7.134\n",
      "0:00:12 | Epoch: 144/500 | val Loss: 0.3012 | EVM: 30.18\n",
      "0:00:00 | Epoch: 145/500 | train Loss: 0.07256 | EVM: 7.393\n",
      "0:00:11 | Epoch: 145/500 | val Loss: 0.3009 | EVM: 30.15\n",
      "0:00:00 | Epoch: 146/500 | train Loss: 0.07288 | EVM: 7.328\n",
      "0:00:11 | Epoch: 146/500 | val Loss: 0.2977 | EVM: 29.83\n",
      "0:00:00 | Epoch: 147/500 | train Loss: 0.06959 | EVM: 7.077\n",
      "0:00:11 | Epoch: 147/500 | val Loss: 0.3014 | EVM: 30.21\n",
      "0:00:00 | Epoch: 148/500 | train Loss: 0.07081 | EVM: 7.213\n",
      "0:00:11 | Epoch: 148/500 | val Loss: 0.3016 | EVM: 30.22\n",
      "0:00:00 | Epoch: 149/500 | train Loss: 0.06907 | EVM: 6.998\n",
      "0:00:12 | Epoch: 149/500 | val Loss: 0.2994 | EVM: 30.01\n",
      "0:00:00 | Epoch: 150/500 | train Loss: 0.07178 | EVM: 7.315\n",
      "0:00:11 | Epoch: 150/500 | val Loss: 0.3002 | EVM: 30.08\n",
      "0:00:00 | Epoch: 151/500 | train Loss: 0.07133 | EVM: 7.23\n",
      "0:00:12 | Epoch: 151/500 | val Loss: 0.2983 | EVM: 29.89\n",
      "0:00:00 | Epoch: 152/500 | train Loss: 0.07327 | EVM: 7.368\n",
      "0:00:12 | Epoch: 152/500 | val Loss: 0.2969 | EVM: 29.76\n",
      "0:00:00 | Epoch: 153/500 | train Loss: 0.07382 | EVM: 7.507\n",
      "0:00:11 | Epoch: 153/500 | val Loss: 0.2967 | EVM: 29.73\n",
      "0:00:00 | Epoch: 154/500 | train Loss: 0.07063 | EVM: 7.09\n",
      "0:00:12 | Epoch: 154/500 | val Loss: 0.2939 | EVM: 29.46\n",
      "0:00:01 | Epoch: 155/500 | train Loss: 0.06834 | EVM: 6.901\n",
      "0:00:11 | Epoch: 155/500 | val Loss: 0.2952 | EVM: 29.59\n",
      "0:00:00 | Epoch: 156/500 | train Loss: 0.06797 | EVM: 6.907\n",
      "0:00:11 | Epoch: 156/500 | val Loss: 0.2938 | EVM: 29.45\n",
      "0:00:00 | Epoch: 157/500 | train Loss: 0.07263 | EVM: 7.378\n",
      "0:00:11 | Epoch: 157/500 | val Loss: 0.2962 | EVM: 29.68\n",
      "0:00:00 | Epoch: 158/500 | train Loss: 0.06766 | EVM: 6.822\n",
      "0:00:11 | Epoch: 158/500 | val Loss: 0.2919 | EVM: 29.25\n",
      "0:00:00 | Epoch: 159/500 | train Loss: 0.06466 | EVM: 6.535\n",
      "0:00:11 | Epoch: 159/500 | val Loss: 0.2949 | EVM: 29.55\n",
      "0:00:00 | Epoch: 160/500 | train Loss: 0.07037 | EVM: 7.109\n",
      "0:00:11 | Epoch: 160/500 | val Loss: 0.2964 | EVM: 29.71\n",
      "0:00:00 | Epoch: 161/500 | train Loss: 0.06871 | EVM: 6.947\n",
      "0:00:11 | Epoch: 161/500 | val Loss: 0.2942 | EVM: 29.49\n",
      "0:00:00 | Epoch: 162/500 | train Loss: 0.0681 | EVM: 6.865\n",
      "0:00:11 | Epoch: 162/500 | val Loss: 0.2992 | EVM: 29.99\n",
      "0:00:00 | Epoch: 163/500 | train Loss: 0.07654 | EVM: 7.788\n",
      "0:00:11 | Epoch: 163/500 | val Loss: 0.2907 | EVM: 29.14\n",
      "0:00:00 | Epoch: 164/500 | train Loss: 0.06978 | EVM: 7.064\n",
      "0:00:11 | Epoch: 164/500 | val Loss: 0.2919 | EVM: 29.25\n",
      "0:00:00 | Epoch: 165/500 | train Loss: 0.07239 | EVM: 7.297\n",
      "0:00:11 | Epoch: 165/500 | val Loss: 0.2941 | EVM: 29.47\n",
      "0:00:00 | Epoch: 166/500 | train Loss: 0.07179 | EVM: 7.271\n",
      "0:00:11 | Epoch: 166/500 | val Loss: 0.29 | EVM: 29.06\n",
      "0:00:01 | Epoch: 167/500 | train Loss: 0.06927 | EVM: 6.98\n",
      "0:00:11 | Epoch: 167/500 | val Loss: 0.2911 | EVM: 29.17\n",
      "0:00:00 | Epoch: 168/500 | train Loss: 0.07044 | EVM: 7.076\n",
      "0:00:12 | Epoch: 168/500 | val Loss: 0.2883 | EVM: 28.89\n",
      "0:00:00 | Epoch: 169/500 | train Loss: 0.07027 | EVM: 7.073\n",
      "0:00:11 | Epoch: 169/500 | val Loss: 0.2929 | EVM: 29.35\n",
      "0:00:00 | Epoch: 170/500 | train Loss: 0.06978 | EVM: 7.074\n",
      "0:00:11 | Epoch: 170/500 | val Loss: 0.2934 | EVM: 29.4\n",
      "0:00:00 | Epoch: 171/500 | train Loss: 0.06425 | EVM: 6.5\n",
      "0:00:11 | Epoch: 171/500 | val Loss: 0.2895 | EVM: 29.01\n",
      "0:00:00 | Epoch: 172/500 | train Loss: 0.06717 | EVM: 6.85\n",
      "0:00:11 | Epoch: 172/500 | val Loss: 0.2876 | EVM: 28.82\n",
      "0:00:00 | Epoch: 173/500 | train Loss: 0.06591 | EVM: 6.68\n",
      "0:00:11 | Epoch: 173/500 | val Loss: 0.2868 | EVM: 28.75\n",
      "0:00:00 | Epoch: 174/500 | train Loss: 0.0679 | EVM: 6.865\n",
      "0:00:11 | Epoch: 174/500 | val Loss: 0.284 | EVM: 28.47\n",
      "0:00:00 | Epoch: 175/500 | train Loss: 0.06545 | EVM: 6.622\n",
      "0:00:11 | Epoch: 175/500 | val Loss: 0.2865 | EVM: 28.72\n",
      "0:00:00 | Epoch: 176/500 | train Loss: 0.07132 | EVM: 7.19\n",
      "0:00:11 | Epoch: 176/500 | val Loss: 0.2897 | EVM: 29.03\n",
      "0:00:00 | Epoch: 177/500 | train Loss: 0.07078 | EVM: 7.118\n",
      "0:00:11 | Epoch: 177/500 | val Loss: 0.2917 | EVM: 29.23\n",
      "0:00:00 | Epoch: 178/500 | train Loss: 0.07662 | EVM: 7.755\n",
      "0:00:11 | Epoch: 178/500 | val Loss: 0.2904 | EVM: 29.1\n",
      "0:00:01 | Epoch: 179/500 | train Loss: 0.07008 | EVM: 7.069\n",
      "0:00:12 | Epoch: 179/500 | val Loss: 0.2902 | EVM: 29.08\n",
      "0:00:00 | Epoch: 180/500 | train Loss: 0.06787 | EVM: 6.859\n",
      "0:00:12 | Epoch: 180/500 | val Loss: 0.2885 | EVM: 28.91\n",
      "0:00:00 | Epoch: 181/500 | train Loss: 0.06854 | EVM: 6.943\n",
      "0:00:11 | Epoch: 181/500 | val Loss: 0.2887 | EVM: 28.93\n",
      "0:00:00 | Epoch: 182/500 | train Loss: 0.0652 | EVM: 6.569\n",
      "0:00:11 | Epoch: 182/500 | val Loss: 0.282 | EVM: 28.27\n",
      "0:00:00 | Epoch: 183/500 | train Loss: 0.06356 | EVM: 6.421\n",
      "0:00:11 | Epoch: 183/500 | val Loss: 0.2846 | EVM: 28.52\n",
      "0:00:00 | Epoch: 184/500 | train Loss: 0.06479 | EVM: 6.575\n",
      "0:00:11 | Epoch: 184/500 | val Loss: 0.2831 | EVM: 28.37\n",
      "0:00:00 | Epoch: 185/500 | train Loss: 0.06249 | EVM: 6.355\n",
      "0:00:11 | Epoch: 185/500 | val Loss: 0.2842 | EVM: 28.48\n",
      "0:00:01 | Epoch: 186/500 | train Loss: 0.06358 | EVM: 6.443\n",
      "0:00:11 | Epoch: 186/500 | val Loss: 0.2856 | EVM: 28.63\n",
      "0:00:00 | Epoch: 187/500 | train Loss: 0.07074 | EVM: 7.133\n",
      "0:00:11 | Epoch: 187/500 | val Loss: 0.2816 | EVM: 28.23\n",
      "0:00:01 | Epoch: 188/500 | train Loss: 0.06824 | EVM: 6.891\n",
      "0:00:11 | Epoch: 188/500 | val Loss: 0.283 | EVM: 28.37\n",
      "0:00:00 | Epoch: 189/500 | train Loss: 0.07366 | EVM: 7.426\n",
      "0:00:11 | Epoch: 189/500 | val Loss: 0.2858 | EVM: 28.65\n",
      "0:00:00 | Epoch: 190/500 | train Loss: 0.067 | EVM: 6.782\n",
      "0:00:11 | Epoch: 190/500 | val Loss: 0.281 | EVM: 28.17\n",
      "0:00:00 | Epoch: 191/500 | train Loss: 0.0653 | EVM: 6.619\n",
      "0:00:11 | Epoch: 191/500 | val Loss: 0.2857 | EVM: 28.64\n",
      "0:00:01 | Epoch: 192/500 | train Loss: 0.07331 | EVM: 7.468\n",
      "0:00:11 | Epoch: 192/500 | val Loss: 0.2838 | EVM: 28.44\n",
      "0:00:00 | Epoch: 193/500 | train Loss: 0.06579 | EVM: 6.641\n",
      "0:00:11 | Epoch: 193/500 | val Loss: 0.2836 | EVM: 28.43\n",
      "0:00:00 | Epoch: 194/500 | train Loss: 0.06122 | EVM: 6.168\n",
      "0:00:11 | Epoch: 194/500 | val Loss: 0.2821 | EVM: 28.27\n",
      "0:00:01 | Epoch: 195/500 | train Loss: 0.06477 | EVM: 6.552\n",
      "0:00:11 | Epoch: 195/500 | val Loss: 0.2845 | EVM: 28.51\n",
      "0:00:00 | Epoch: 196/500 | train Loss: 0.06299 | EVM: 6.36\n",
      "0:00:12 | Epoch: 196/500 | val Loss: 0.2823 | EVM: 28.29\n",
      "0:00:00 | Epoch: 197/500 | train Loss: 0.06955 | EVM: 7.023\n",
      "0:00:11 | Epoch: 197/500 | val Loss: 0.2829 | EVM: 28.36\n",
      "0:00:00 | Epoch: 198/500 | train Loss: 0.05745 | EVM: 5.808\n",
      "0:00:11 | Epoch: 198/500 | val Loss: 0.2803 | EVM: 28.09\n",
      "0:00:00 | Epoch: 199/500 | train Loss: 0.06207 | EVM: 6.237\n",
      "0:00:11 | Epoch: 199/500 | val Loss: 0.2823 | EVM: 28.29\n",
      "0:00:00 | Epoch: 200/500 | train Loss: 0.06856 | EVM: 6.919\n",
      "0:00:12 | Epoch: 200/500 | val Loss: 0.2783 | EVM: 27.9\n",
      "0:00:01 | Epoch: 201/500 | train Loss: 0.06709 | EVM: 6.764\n",
      "0:00:12 | Epoch: 201/500 | val Loss: 0.2839 | EVM: 28.45\n",
      "0:00:00 | Epoch: 202/500 | train Loss: 0.06432 | EVM: 6.492\n",
      "0:00:11 | Epoch: 202/500 | val Loss: 0.2783 | EVM: 27.9\n",
      "0:00:01 | Epoch: 203/500 | train Loss: 0.06623 | EVM: 6.704\n",
      "0:00:11 | Epoch: 203/500 | val Loss: 0.2804 | EVM: 28.11\n",
      "0:00:00 | Epoch: 204/500 | train Loss: 0.06016 | EVM: 6.083\n",
      "0:00:11 | Epoch: 204/500 | val Loss: 0.2778 | EVM: 27.85\n",
      "0:00:00 | Epoch: 205/500 | train Loss: 0.06629 | EVM: 6.706\n",
      "0:00:11 | Epoch: 205/500 | val Loss: 0.2777 | EVM: 27.83\n",
      "0:00:00 | Epoch: 206/500 | train Loss: 0.06605 | EVM: 6.69\n",
      "0:00:11 | Epoch: 206/500 | val Loss: 0.2783 | EVM: 27.89\n",
      "0:00:00 | Epoch: 207/500 | train Loss: 0.05983 | EVM: 6.051\n",
      "0:00:11 | Epoch: 207/500 | val Loss: 0.2785 | EVM: 27.91\n",
      "0:00:00 | Epoch: 208/500 | train Loss: 0.068 | EVM: 6.896\n",
      "0:00:11 | Epoch: 208/500 | val Loss: 0.2796 | EVM: 28.02\n",
      "0:00:01 | Epoch: 209/500 | train Loss: 0.06721 | EVM: 6.768\n",
      "0:00:11 | Epoch: 209/500 | val Loss: 0.2789 | EVM: 27.95\n",
      "0:00:00 | Epoch: 210/500 | train Loss: 0.06573 | EVM: 6.712\n",
      "0:00:11 | Epoch: 210/500 | val Loss: 0.2773 | EVM: 27.79\n",
      "0:00:00 | Epoch: 211/500 | train Loss: 0.06165 | EVM: 6.229\n",
      "0:00:11 | Epoch: 211/500 | val Loss: 0.2755 | EVM: 27.62\n",
      "0:00:00 | Epoch: 212/500 | train Loss: 0.06011 | EVM: 6.064\n",
      "0:00:11 | Epoch: 212/500 | val Loss: 0.279 | EVM: 27.96\n",
      "0:00:00 | Epoch: 213/500 | train Loss: 0.06314 | EVM: 6.362\n",
      "0:00:11 | Epoch: 213/500 | val Loss: 0.2744 | EVM: 27.5\n",
      "0:00:00 | Epoch: 214/500 | train Loss: 0.06779 | EVM: 6.867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:11 | Epoch: 214/500 | val Loss: 0.2814 | EVM: 28.21\n",
      "0:00:00 | Epoch: 215/500 | train Loss: 0.06347 | EVM: 6.395\n",
      "0:00:11 | Epoch: 215/500 | val Loss: 0.274 | EVM: 27.46\n",
      "0:00:00 | Epoch: 216/500 | train Loss: 0.06171 | EVM: 6.2\n",
      "0:00:12 | Epoch: 216/500 | val Loss: 0.2743 | EVM: 27.5\n",
      "0:00:01 | Epoch: 217/500 | train Loss: 0.06248 | EVM: 6.351\n",
      "0:00:12 | Epoch: 217/500 | val Loss: 0.2754 | EVM: 27.61\n",
      "0:00:00 | Epoch: 218/500 | train Loss: 0.06309 | EVM: 6.378\n",
      "0:00:12 | Epoch: 218/500 | val Loss: 0.275 | EVM: 27.57\n",
      "0:00:01 | Epoch: 219/500 | train Loss: 0.06277 | EVM: 6.331\n",
      "0:00:12 | Epoch: 219/500 | val Loss: 0.2755 | EVM: 27.62\n",
      "0:00:01 | Epoch: 220/500 | train Loss: 0.06626 | EVM: 6.693\n",
      "0:00:11 | Epoch: 220/500 | val Loss: 0.2735 | EVM: 27.41\n",
      "0:00:00 | Epoch: 221/500 | train Loss: 0.06569 | EVM: 6.651\n",
      "0:00:11 | Epoch: 221/500 | val Loss: 0.276 | EVM: 27.66\n",
      "0:00:00 | Epoch: 222/500 | train Loss: 0.06302 | EVM: 6.375\n",
      "0:00:11 | Epoch: 222/500 | val Loss: 0.273 | EVM: 27.37\n",
      "0:00:01 | Epoch: 223/500 | train Loss: 0.06352 | EVM: 6.468\n",
      "0:00:12 | Epoch: 223/500 | val Loss: 0.2796 | EVM: 28.02\n",
      "0:00:00 | Epoch: 224/500 | train Loss: 0.06624 | EVM: 6.675\n",
      "0:00:12 | Epoch: 224/500 | val Loss: 0.2764 | EVM: 27.71\n",
      "0:00:01 | Epoch: 225/500 | train Loss: 0.06091 | EVM: 6.159\n",
      "0:00:11 | Epoch: 225/500 | val Loss: 0.2714 | EVM: 27.21\n",
      "0:00:00 | Epoch: 226/500 | train Loss: 0.05989 | EVM: 6.044\n",
      "0:00:11 | Epoch: 226/500 | val Loss: 0.2752 | EVM: 27.59\n",
      "0:00:00 | Epoch: 227/500 | train Loss: 0.05844 | EVM: 5.891\n",
      "0:00:11 | Epoch: 227/500 | val Loss: 0.2713 | EVM: 27.2\n",
      "0:00:00 | Epoch: 228/500 | train Loss: 0.06064 | EVM: 6.132\n",
      "0:00:11 | Epoch: 228/500 | val Loss: 0.2747 | EVM: 27.54\n",
      "0:00:00 | Epoch: 229/500 | train Loss: 0.0622 | EVM: 6.265\n",
      "0:00:11 | Epoch: 229/500 | val Loss: 0.2717 | EVM: 27.23\n",
      "0:00:00 | Epoch: 230/500 | train Loss: 0.05839 | EVM: 5.899\n",
      "0:00:11 | Epoch: 230/500 | val Loss: 0.2735 | EVM: 27.42\n",
      "0:00:00 | Epoch: 231/500 | train Loss: 0.06786 | EVM: 6.858\n",
      "0:00:11 | Epoch: 231/500 | val Loss: 0.2723 | EVM: 27.29\n",
      "0:00:00 | Epoch: 232/500 | train Loss: 0.06407 | EVM: 6.501\n",
      "0:00:12 | Epoch: 232/500 | val Loss: 0.2726 | EVM: 27.33\n",
      "0:00:00 | Epoch: 233/500 | train Loss: 0.05895 | EVM: 5.966\n",
      "0:00:11 | Epoch: 233/500 | val Loss: 0.2709 | EVM: 27.15\n",
      "0:00:00 | Epoch: 234/500 | train Loss: 0.06095 | EVM: 6.237\n",
      "0:00:11 | Epoch: 234/500 | val Loss: 0.2717 | EVM: 27.24\n",
      "0:00:00 | Epoch: 235/500 | train Loss: 0.06572 | EVM: 6.673\n",
      "0:00:11 | Epoch: 235/500 | val Loss: 0.2753 | EVM: 27.59\n",
      "0:00:00 | Epoch: 236/500 | train Loss: 0.0667 | EVM: 6.728\n",
      "0:00:11 | Epoch: 236/500 | val Loss: 0.2743 | EVM: 27.49\n",
      "0:00:01 | Epoch: 237/500 | train Loss: 0.06159 | EVM: 6.215\n",
      "0:00:12 | Epoch: 237/500 | val Loss: 0.2704 | EVM: 27.11\n",
      "0:00:00 | Epoch: 238/500 | train Loss: 0.0608 | EVM: 6.167\n",
      "0:00:11 | Epoch: 238/500 | val Loss: 0.2721 | EVM: 27.28\n",
      "0:00:00 | Epoch: 239/500 | train Loss: 0.06531 | EVM: 6.68\n",
      "0:00:11 | Epoch: 239/500 | val Loss: 0.2703 | EVM: 27.09\n",
      "0:00:00 | Epoch: 240/500 | train Loss: 0.05775 | EVM: 5.806\n",
      "0:00:11 | Epoch: 240/500 | val Loss: 0.2722 | EVM: 27.29\n",
      "0:00:00 | Epoch: 241/500 | train Loss: 0.06101 | EVM: 6.184\n",
      "0:00:12 | Epoch: 241/500 | val Loss: 0.2701 | EVM: 27.08\n",
      "0:00:00 | Epoch: 242/500 | train Loss: 0.06189 | EVM: 6.236\n",
      "0:00:11 | Epoch: 242/500 | val Loss: 0.2712 | EVM: 27.19\n",
      "0:00:00 | Epoch: 243/500 | train Loss: 0.06012 | EVM: 6.131\n",
      "0:00:11 | Epoch: 243/500 | val Loss: 0.2758 | EVM: 27.65\n",
      "0:00:00 | Epoch: 244/500 | train Loss: 0.06583 | EVM: 6.685\n",
      "0:00:11 | Epoch: 244/500 | val Loss: 0.2693 | EVM: 27.0\n",
      "0:00:00 | Epoch: 245/500 | train Loss: 0.0611 | EVM: 6.171\n",
      "0:00:11 | Epoch: 245/500 | val Loss: 0.2707 | EVM: 27.14\n",
      "0:00:00 | Epoch: 246/500 | train Loss: 0.05655 | EVM: 5.701\n",
      "0:00:11 | Epoch: 246/500 | val Loss: 0.2696 | EVM: 27.03\n",
      "0:00:00 | Epoch: 247/500 | train Loss: 0.06097 | EVM: 6.144\n",
      "0:00:11 | Epoch: 247/500 | val Loss: 0.2677 | EVM: 26.84\n",
      "0:00:00 | Epoch: 248/500 | train Loss: 0.05743 | EVM: 5.8\n",
      "0:00:11 | Epoch: 248/500 | val Loss: 0.2735 | EVM: 27.41\n",
      "0:00:00 | Epoch: 249/500 | train Loss: 0.06859 | EVM: 6.904\n",
      "0:00:11 | Epoch: 249/500 | val Loss: 0.2684 | EVM: 26.91\n",
      "0:00:00 | Epoch: 250/500 | train Loss: 0.06525 | EVM: 6.614\n",
      "0:00:11 | Epoch: 250/500 | val Loss: 0.2694 | EVM: 27.01\n",
      "0:00:00 | Epoch: 251/500 | train Loss: 0.05832 | EVM: 5.887\n",
      "0:00:11 | Epoch: 251/500 | val Loss: 0.2705 | EVM: 27.11\n",
      "0:00:00 | Epoch: 252/500 | train Loss: 0.0568 | EVM: 5.762\n",
      "0:00:11 | Epoch: 252/500 | val Loss: 0.2665 | EVM: 26.72\n",
      "0:00:00 | Epoch: 253/500 | train Loss: 0.06273 | EVM: 6.338\n",
      "0:00:11 | Epoch: 253/500 | val Loss: 0.2699 | EVM: 27.05\n",
      "0:00:01 | Epoch: 254/500 | train Loss: 0.06056 | EVM: 6.117\n",
      "0:00:11 | Epoch: 254/500 | val Loss: 0.2694 | EVM: 27.01\n",
      "0:00:00 | Epoch: 255/500 | train Loss: 0.06071 | EVM: 6.097\n",
      "0:00:11 | Epoch: 255/500 | val Loss: 0.2682 | EVM: 26.88\n",
      "0:00:00 | Epoch: 256/500 | train Loss: 0.05611 | EVM: 5.657\n",
      "0:00:11 | Epoch: 256/500 | val Loss: 0.2685 | EVM: 26.92\n",
      "0:00:00 | Epoch: 257/500 | train Loss: 0.05796 | EVM: 5.861\n",
      "0:00:11 | Epoch: 257/500 | val Loss: 0.2682 | EVM: 26.89\n",
      "0:00:01 | Epoch: 258/500 | train Loss: 0.06102 | EVM: 6.169\n",
      "0:00:11 | Epoch: 258/500 | val Loss: 0.2684 | EVM: 26.91\n",
      "0:00:00 | Epoch: 259/500 | train Loss: 0.06013 | EVM: 6.098\n",
      "0:00:11 | Epoch: 259/500 | val Loss: 0.2637 | EVM: 26.44\n",
      "0:00:00 | Epoch: 260/500 | train Loss: 0.05941 | EVM: 5.995\n",
      "0:00:11 | Epoch: 260/500 | val Loss: 0.2705 | EVM: 27.11\n",
      "0:00:00 | Epoch: 261/500 | train Loss: 0.05745 | EVM: 5.802\n",
      "0:00:11 | Epoch: 261/500 | val Loss: 0.2643 | EVM: 26.5\n",
      "0:00:01 | Epoch: 262/500 | train Loss: 0.05862 | EVM: 5.927\n",
      "0:00:11 | Epoch: 262/500 | val Loss: 0.266 | EVM: 26.67\n",
      "0:00:00 | Epoch: 263/500 | train Loss: 0.05868 | EVM: 5.922\n",
      "0:00:12 | Epoch: 263/500 | val Loss: 0.2665 | EVM: 26.72\n",
      "0:00:00 | Epoch: 264/500 | train Loss: 0.06214 | EVM: 6.253\n",
      "0:00:12 | Epoch: 264/500 | val Loss: 0.2663 | EVM: 26.7\n",
      "0:00:00 | Epoch: 265/500 | train Loss: 0.06143 | EVM: 6.232\n",
      "0:00:12 | Epoch: 265/500 | val Loss: 0.2638 | EVM: 26.44\n",
      "0:00:00 | Epoch: 266/500 | train Loss: 0.0549 | EVM: 5.565\n",
      "0:00:11 | Epoch: 266/500 | val Loss: 0.2654 | EVM: 26.61\n",
      "0:00:00 | Epoch: 267/500 | train Loss: 0.06342 | EVM: 6.421\n",
      "0:00:11 | Epoch: 267/500 | val Loss: 0.2682 | EVM: 26.88\n",
      "0:00:00 | Epoch: 268/500 | train Loss: 0.05958 | EVM: 5.998\n",
      "0:00:11 | Epoch: 268/500 | val Loss: 0.2634 | EVM: 26.41\n",
      "0:00:00 | Epoch: 269/500 | train Loss: 0.05814 | EVM: 5.902\n",
      "0:00:11 | Epoch: 269/500 | val Loss: 0.267 | EVM: 26.76\n",
      "0:00:00 | Epoch: 270/500 | train Loss: 0.05463 | EVM: 5.529\n",
      "0:00:11 | Epoch: 270/500 | val Loss: 0.2638 | EVM: 26.44\n",
      "0:00:00 | Epoch: 271/500 | train Loss: 0.05617 | EVM: 5.658\n",
      "0:00:11 | Epoch: 271/500 | val Loss: 0.2676 | EVM: 26.83\n",
      "0:00:00 | Epoch: 272/500 | train Loss: 0.06174 | EVM: 6.28\n",
      "0:00:11 | Epoch: 272/500 | val Loss: 0.2641 | EVM: 26.48\n",
      "0:00:00 | Epoch: 273/500 | train Loss: 0.06081 | EVM: 6.105\n",
      "0:00:11 | Epoch: 273/500 | val Loss: 0.2622 | EVM: 26.29\n",
      "0:00:00 | Epoch: 274/500 | train Loss: 0.05867 | EVM: 5.932\n",
      "0:00:11 | Epoch: 274/500 | val Loss: 0.2671 | EVM: 26.78\n",
      "0:00:00 | Epoch: 275/500 | train Loss: 0.06412 | EVM: 6.466\n",
      "0:00:11 | Epoch: 275/500 | val Loss: 0.266 | EVM: 26.67\n",
      "0:00:00 | Epoch: 276/500 | train Loss: 0.05878 | EVM: 5.93\n",
      "0:00:11 | Epoch: 276/500 | val Loss: 0.2689 | EVM: 26.95\n",
      "0:00:00 | Epoch: 277/500 | train Loss: 0.05522 | EVM: 5.618\n",
      "0:00:11 | Epoch: 277/500 | val Loss: 0.2625 | EVM: 26.32\n",
      "0:00:00 | Epoch: 278/500 | train Loss: 0.05304 | EVM: 5.361\n",
      "0:00:11 | Epoch: 278/500 | val Loss: 0.2646 | EVM: 26.53\n",
      "0:00:00 | Epoch: 279/500 | train Loss: 0.05861 | EVM: 5.922\n",
      "0:00:11 | Epoch: 279/500 | val Loss: 0.2654 | EVM: 26.61\n",
      "0:00:00 | Epoch: 280/500 | train Loss: 0.05566 | EVM: 5.632\n",
      "0:00:11 | Epoch: 280/500 | val Loss: 0.2633 | EVM: 26.4\n",
      "0:00:00 | Epoch: 281/500 | train Loss: 0.05776 | EVM: 5.888\n",
      "0:00:11 | Epoch: 281/500 | val Loss: 0.265 | EVM: 26.57\n",
      "0:00:00 | Epoch: 282/500 | train Loss: 0.05754 | EVM: 5.796\n",
      "0:00:12 | Epoch: 282/500 | val Loss: 0.2638 | EVM: 26.45\n",
      "0:00:00 | Epoch: 283/500 | train Loss: 0.05498 | EVM: 5.566\n",
      "0:00:12 | Epoch: 283/500 | val Loss: 0.2635 | EVM: 26.42\n",
      "0:00:00 | Epoch: 284/500 | train Loss: 0.05706 | EVM: 5.751\n",
      "0:00:12 | Epoch: 284/500 | val Loss: 0.2641 | EVM: 26.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00 | Epoch: 285/500 | train Loss: 0.05754 | EVM: 5.797\n",
      "0:00:11 | Epoch: 285/500 | val Loss: 0.2649 | EVM: 26.56\n",
      "0:00:00 | Epoch: 286/500 | train Loss: 0.05664 | EVM: 5.724\n",
      "0:00:12 | Epoch: 286/500 | val Loss: 0.2614 | EVM: 26.21\n",
      "0:00:00 | Epoch: 287/500 | train Loss: 0.06019 | EVM: 6.05\n",
      "0:00:11 | Epoch: 287/500 | val Loss: 0.2623 | EVM: 26.3\n",
      "0:00:00 | Epoch: 288/500 | train Loss: 0.05852 | EVM: 5.902\n",
      "0:00:11 | Epoch: 288/500 | val Loss: 0.2651 | EVM: 26.58\n",
      "0:00:01 | Epoch: 289/500 | train Loss: 0.05639 | EVM: 5.694\n",
      "0:00:11 | Epoch: 289/500 | val Loss: 0.2644 | EVM: 26.51\n",
      "0:00:00 | Epoch: 290/500 | train Loss: 0.05486 | EVM: 5.599\n",
      "0:00:12 | Epoch: 290/500 | val Loss: 0.2593 | EVM: 25.99\n",
      "0:00:00 | Epoch: 291/500 | train Loss: 0.05543 | EVM: 5.594\n",
      "0:00:11 | Epoch: 291/500 | val Loss: 0.2655 | EVM: 26.61\n",
      "0:00:01 | Epoch: 292/500 | train Loss: 0.05549 | EVM: 5.606\n",
      "0:00:11 | Epoch: 292/500 | val Loss: 0.2642 | EVM: 26.48\n",
      "0:00:00 | Epoch: 293/500 | train Loss: 0.05971 | EVM: 6.055\n",
      "0:00:11 | Epoch: 293/500 | val Loss: 0.264 | EVM: 26.47\n",
      "0:00:00 | Epoch: 294/500 | train Loss: 0.0516 | EVM: 5.232\n",
      "0:00:11 | Epoch: 294/500 | val Loss: 0.261 | EVM: 26.16\n",
      "0:00:00 | Epoch: 295/500 | train Loss: 0.06189 | EVM: 6.339\n",
      "0:00:11 | Epoch: 295/500 | val Loss: 0.2621 | EVM: 26.28\n",
      "0:00:00 | Epoch: 296/500 | train Loss: 0.05517 | EVM: 5.555\n",
      "0:00:11 | Epoch: 296/500 | val Loss: 0.2619 | EVM: 26.26\n",
      "0:00:00 | Epoch: 297/500 | train Loss: 0.06394 | EVM: 6.461\n",
      "0:00:11 | Epoch: 297/500 | val Loss: 0.2628 | EVM: 26.34\n",
      "0:00:00 | Epoch: 298/500 | train Loss: 0.06071 | EVM: 6.223\n",
      "0:00:11 | Epoch: 298/500 | val Loss: 0.265 | EVM: 26.57\n",
      "0:00:01 | Epoch: 299/500 | train Loss: 0.06139 | EVM: 6.205\n",
      "0:00:11 | Epoch: 299/500 | val Loss: 0.2631 | EVM: 26.37\n",
      "0:00:01 | Epoch: 300/500 | train Loss: 0.05803 | EVM: 5.862\n",
      "0:00:11 | Epoch: 300/500 | val Loss: 0.2577 | EVM: 25.84\n",
      "0:00:00 | Epoch: 301/500 | train Loss: 0.05468 | EVM: 5.535\n",
      "0:00:11 | Epoch: 301/500 | val Loss: 0.2649 | EVM: 26.56\n",
      "0:00:01 | Epoch: 302/500 | train Loss: 0.05435 | EVM: 5.475\n",
      "0:00:11 | Epoch: 302/500 | val Loss: 0.261 | EVM: 26.17\n",
      "0:00:00 | Epoch: 303/500 | train Loss: 0.05422 | EVM: 5.49\n",
      "0:00:12 | Epoch: 303/500 | val Loss: 0.2608 | EVM: 26.14\n",
      "0:00:00 | Epoch: 304/500 | train Loss: 0.05112 | EVM: 5.203\n",
      "0:00:11 | Epoch: 304/500 | val Loss: 0.2617 | EVM: 26.24\n",
      "0:00:00 | Epoch: 305/500 | train Loss: 0.05866 | EVM: 5.929\n",
      "0:00:11 | Epoch: 305/500 | val Loss: 0.2644 | EVM: 26.5\n",
      "0:00:00 | Epoch: 306/500 | train Loss: 0.05655 | EVM: 5.712\n",
      "0:00:11 | Epoch: 306/500 | val Loss: 0.2637 | EVM: 26.44\n",
      "0:00:00 | Epoch: 307/500 | train Loss: 0.05663 | EVM: 5.715\n",
      "0:00:11 | Epoch: 307/500 | val Loss: 0.2569 | EVM: 25.76\n",
      "0:00:00 | Epoch: 308/500 | train Loss: 0.05756 | EVM: 5.795\n",
      "0:00:11 | Epoch: 308/500 | val Loss: 0.2633 | EVM: 26.4\n",
      "0:00:01 | Epoch: 309/500 | train Loss: 0.05638 | EVM: 5.676\n",
      "0:00:12 | Epoch: 309/500 | val Loss: 0.2588 | EVM: 25.95\n",
      "0:00:00 | Epoch: 310/500 | train Loss: 0.05396 | EVM: 5.452\n",
      "0:00:11 | Epoch: 310/500 | val Loss: 0.2609 | EVM: 26.16\n",
      "0:00:00 | Epoch: 311/500 | train Loss: 0.05522 | EVM: 5.589\n",
      "0:00:12 | Epoch: 311/500 | val Loss: 0.2607 | EVM: 26.14\n",
      "0:00:00 | Epoch: 312/500 | train Loss: 0.05473 | EVM: 5.541\n",
      "0:00:11 | Epoch: 312/500 | val Loss: 0.2593 | EVM: 26.0\n",
      "0:00:00 | Epoch: 313/500 | train Loss: 0.05333 | EVM: 5.421\n",
      "0:00:12 | Epoch: 313/500 | val Loss: 0.2609 | EVM: 26.16\n",
      "0:00:00 | Epoch: 314/500 | train Loss: 0.05609 | EVM: 5.661\n",
      "0:00:11 | Epoch: 314/500 | val Loss: 0.259 | EVM: 25.97\n",
      "0:00:00 | Epoch: 315/500 | train Loss: 0.05659 | EVM: 5.739\n",
      "0:00:11 | Epoch: 315/500 | val Loss: 0.2649 | EVM: 26.55\n",
      "0:00:00 | Epoch: 316/500 | train Loss: 0.05456 | EVM: 5.523\n",
      "0:00:11 | Epoch: 316/500 | val Loss: 0.2605 | EVM: 26.13\n",
      "0:00:00 | Epoch: 317/500 | train Loss: 0.05861 | EVM: 5.921\n",
      "0:00:11 | Epoch: 317/500 | val Loss: 0.2595 | EVM: 26.01\n",
      "0:00:00 | Epoch: 318/500 | train Loss: 0.05442 | EVM: 5.475\n",
      "0:00:12 | Epoch: 318/500 | val Loss: 0.2615 | EVM: 26.22\n",
      "0:00:00 | Epoch: 319/500 | train Loss: 0.0581 | EVM: 5.856\n",
      "0:00:12 | Epoch: 319/500 | val Loss: 0.2591 | EVM: 25.98\n",
      "0:00:01 | Epoch: 320/500 | train Loss: 0.05035 | EVM: 5.079\n",
      "0:00:12 | Epoch: 320/500 | val Loss: 0.2567 | EVM: 25.74\n",
      "0:00:00 | Epoch: 321/500 | train Loss: 0.05563 | EVM: 5.653\n",
      "0:00:11 | Epoch: 321/500 | val Loss: 0.259 | EVM: 25.97\n",
      "0:00:00 | Epoch: 322/500 | train Loss: 0.05874 | EVM: 5.958\n",
      "0:00:12 | Epoch: 322/500 | val Loss: 0.2584 | EVM: 25.91\n",
      "0:00:00 | Epoch: 323/500 | train Loss: 0.06202 | EVM: 6.251\n",
      "0:00:11 | Epoch: 323/500 | val Loss: 0.2631 | EVM: 26.38\n",
      "0:00:01 | Epoch: 324/500 | train Loss: 0.05881 | EVM: 5.941\n",
      "0:00:11 | Epoch: 324/500 | val Loss: 0.2564 | EVM: 25.72\n",
      "0:00:00 | Epoch: 325/500 | train Loss: 0.05563 | EVM: 5.592\n",
      "0:00:11 | Epoch: 325/500 | val Loss: 0.2618 | EVM: 26.25\n",
      "0:00:00 | Epoch: 326/500 | train Loss: 0.05433 | EVM: 5.53\n",
      "0:00:11 | Epoch: 326/500 | val Loss: 0.2577 | EVM: 25.84\n",
      "0:00:00 | Epoch: 327/500 | train Loss: 0.05689 | EVM: 5.739\n",
      "0:00:11 | Epoch: 327/500 | val Loss: 0.2612 | EVM: 26.19\n",
      "0:00:00 | Epoch: 328/500 | train Loss: 0.05682 | EVM: 5.764\n",
      "0:00:11 | Epoch: 328/500 | val Loss: 0.2575 | EVM: 25.81\n",
      "0:00:00 | Epoch: 329/500 | train Loss: 0.05383 | EVM: 5.42\n",
      "0:00:11 | Epoch: 329/500 | val Loss: 0.2568 | EVM: 25.75\n",
      "0:00:00 | Epoch: 330/500 | train Loss: 0.05572 | EVM: 5.61\n",
      "0:00:11 | Epoch: 330/500 | val Loss: 0.2619 | EVM: 26.26\n",
      "0:00:00 | Epoch: 331/500 | train Loss: 0.05359 | EVM: 5.42\n",
      "0:00:11 | Epoch: 331/500 | val Loss: 0.2557 | EVM: 25.64\n",
      "0:00:01 | Epoch: 332/500 | train Loss: 0.05483 | EVM: 5.586\n",
      "0:00:11 | Epoch: 332/500 | val Loss: 0.2594 | EVM: 26.01\n",
      "0:00:00 | Epoch: 333/500 | train Loss: 0.05622 | EVM: 5.702\n",
      "0:00:11 | Epoch: 333/500 | val Loss: 0.2579 | EVM: 25.86\n",
      "0:00:01 | Epoch: 334/500 | train Loss: 0.05536 | EVM: 5.599\n",
      "0:00:11 | Epoch: 334/500 | val Loss: 0.2586 | EVM: 25.93\n",
      "0:00:00 | Epoch: 335/500 | train Loss: 0.05948 | EVM: 6.045\n",
      "0:00:11 | Epoch: 335/500 | val Loss: 0.2623 | EVM: 26.3\n",
      "0:00:00 | Epoch: 336/500 | train Loss: 0.05202 | EVM: 5.265\n",
      "0:00:11 | Epoch: 336/500 | val Loss: 0.2553 | EVM: 25.6\n",
      "0:00:00 | Epoch: 337/500 | train Loss: 0.05381 | EVM: 5.421\n",
      "0:00:11 | Epoch: 337/500 | val Loss: 0.2618 | EVM: 26.25\n",
      "0:00:01 | Epoch: 338/500 | train Loss: 0.06016 | EVM: 6.076\n",
      "0:00:11 | Epoch: 338/500 | val Loss: 0.2614 | EVM: 26.21\n",
      "0:00:01 | Epoch: 339/500 | train Loss: 0.05829 | EVM: 5.879\n",
      "0:00:12 | Epoch: 339/500 | val Loss: 0.2608 | EVM: 26.15\n",
      "0:00:00 | Epoch: 340/500 | train Loss: 0.05558 | EVM: 5.649\n",
      "0:00:11 | Epoch: 340/500 | val Loss: 0.2584 | EVM: 25.9\n",
      "0:00:00 | Epoch: 341/500 | train Loss: 0.05522 | EVM: 5.583\n",
      "0:00:11 | Epoch: 341/500 | val Loss: 0.2581 | EVM: 25.88\n",
      "0:00:00 | Epoch: 342/500 | train Loss: 0.05256 | EVM: 5.335\n",
      "0:00:11 | Epoch: 342/500 | val Loss: 0.258 | EVM: 25.87\n",
      "0:00:00 | Epoch: 343/500 | train Loss: 0.0572 | EVM: 5.82\n",
      "0:00:11 | Epoch: 343/500 | val Loss: 0.2559 | EVM: 25.66\n",
      "0:00:00 | Epoch: 344/500 | train Loss: 0.05478 | EVM: 5.528\n",
      "0:00:11 | Epoch: 344/500 | val Loss: 0.2587 | EVM: 25.93\n",
      "0:00:00 | Epoch: 345/500 | train Loss: 0.05492 | EVM: 5.535\n",
      "0:00:11 | Epoch: 345/500 | val Loss: 0.2591 | EVM: 25.98\n",
      "0:00:00 | Epoch: 346/500 | train Loss: 0.05713 | EVM: 5.761\n",
      "0:00:10 | Epoch: 346/500 | val Loss: 0.2572 | EVM: 25.79\n",
      "0:00:00 | Epoch: 347/500 | train Loss: 0.05336 | EVM: 5.394\n",
      "0:00:11 | Epoch: 347/500 | val Loss: 0.257 | EVM: 25.77\n",
      "0:00:00 | Epoch: 348/500 | train Loss: 0.05518 | EVM: 5.566\n",
      "0:00:11 | Epoch: 348/500 | val Loss: 0.2555 | EVM: 25.61\n",
      "0:00:00 | Epoch: 349/500 | train Loss: 0.05377 | EVM: 5.444\n",
      "0:00:10 | Epoch: 349/500 | val Loss: 0.2588 | EVM: 25.95\n",
      "0:00:00 | Epoch: 350/500 | train Loss: 0.05016 | EVM: 5.073\n",
      "0:00:11 | Epoch: 350/500 | val Loss: 0.2588 | EVM: 25.95\n",
      "0:00:00 | Epoch: 351/500 | train Loss: 0.05291 | EVM: 5.344\n",
      "0:00:11 | Epoch: 351/500 | val Loss: 0.2578 | EVM: 25.85\n",
      "0:00:00 | Epoch: 352/500 | train Loss: 0.05583 | EVM: 5.643\n",
      "0:00:11 | Epoch: 352/500 | val Loss: 0.2575 | EVM: 25.82\n",
      "0:00:00 | Epoch: 353/500 | train Loss: 0.05949 | EVM: 6.037\n",
      "0:00:11 | Epoch: 353/500 | val Loss: 0.2558 | EVM: 25.65\n",
      "0:00:00 | Epoch: 354/500 | train Loss: 0.05336 | EVM: 5.372\n",
      "0:00:11 | Epoch: 354/500 | val Loss: 0.256 | EVM: 25.66\n",
      "0:00:00 | Epoch: 355/500 | train Loss: 0.05674 | EVM: 5.722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:11 | Epoch: 355/500 | val Loss: 0.2571 | EVM: 25.78\n",
      "0:00:00 | Epoch: 356/500 | train Loss: 0.05655 | EVM: 5.717\n",
      "0:00:11 | Epoch: 356/500 | val Loss: 0.2569 | EVM: 25.76\n",
      "0:00:00 | Epoch: 357/500 | train Loss: 0.05157 | EVM: 5.206\n",
      "0:00:11 | Epoch: 357/500 | val Loss: 0.2586 | EVM: 25.92\n",
      "0:00:00 | Epoch: 358/500 | train Loss: 0.05101 | EVM: 5.122\n",
      "0:00:10 | Epoch: 358/500 | val Loss: 0.2565 | EVM: 25.72\n",
      "0:00:00 | Epoch: 359/500 | train Loss: 0.0592 | EVM: 5.97\n",
      "0:00:11 | Epoch: 359/500 | val Loss: 0.2573 | EVM: 25.8\n",
      "0:00:00 | Epoch: 360/500 | train Loss: 0.05758 | EVM: 5.808\n",
      "0:00:11 | Epoch: 360/500 | val Loss: 0.2574 | EVM: 25.81\n",
      "0:00:00 | Epoch: 361/500 | train Loss: 0.05317 | EVM: 5.439\n",
      "0:00:10 | Epoch: 361/500 | val Loss: 0.2567 | EVM: 25.74\n",
      "0:00:00 | Epoch: 362/500 | train Loss: 0.05305 | EVM: 5.375\n",
      "0:00:11 | Epoch: 362/500 | val Loss: 0.2563 | EVM: 25.7\n",
      "0:00:00 | Epoch: 363/500 | train Loss: 0.05087 | EVM: 5.124\n",
      "0:00:11 | Epoch: 363/500 | val Loss: 0.2571 | EVM: 25.78\n",
      "0:00:00 | Epoch: 364/500 | train Loss: 0.05421 | EVM: 5.468\n",
      "0:00:11 | Epoch: 364/500 | val Loss: 0.2588 | EVM: 25.95\n",
      "0:00:00 | Epoch: 365/500 | train Loss: 0.05255 | EVM: 5.35\n",
      "0:00:11 | Epoch: 365/500 | val Loss: 0.2564 | EVM: 25.71\n",
      "0:00:00 | Epoch: 366/500 | train Loss: 0.05664 | EVM: 5.71\n",
      "0:00:11 | Epoch: 366/500 | val Loss: 0.2552 | EVM: 25.59\n",
      "0:00:00 | Epoch: 367/500 | train Loss: 0.05674 | EVM: 5.794\n",
      "0:00:11 | Epoch: 367/500 | val Loss: 0.2555 | EVM: 25.62\n",
      "0:00:00 | Epoch: 368/500 | train Loss: 0.05288 | EVM: 5.358\n",
      "0:00:11 | Epoch: 368/500 | val Loss: 0.2594 | EVM: 26.01\n",
      "0:00:00 | Epoch: 369/500 | train Loss: 0.05355 | EVM: 5.375\n",
      "0:00:10 | Epoch: 369/500 | val Loss: 0.2535 | EVM: 25.42\n",
      "0:00:00 | Epoch: 370/500 | train Loss: 0.05608 | EVM: 5.676\n",
      "0:00:11 | Epoch: 370/500 | val Loss: 0.2566 | EVM: 25.73\n",
      "0:00:00 | Epoch: 371/500 | train Loss: 0.05055 | EVM: 5.105\n",
      "0:00:11 | Epoch: 371/500 | val Loss: 0.2539 | EVM: 25.46\n",
      "0:00:00 | Epoch: 372/500 | train Loss: 0.05635 | EVM: 5.688\n",
      "0:00:11 | Epoch: 372/500 | val Loss: 0.258 | EVM: 25.87\n",
      "0:00:00 | Epoch: 373/500 | train Loss: 0.05131 | EVM: 5.206\n",
      "0:00:11 | Epoch: 373/500 | val Loss: 0.2554 | EVM: 25.62\n",
      "0:00:00 | Epoch: 374/500 | train Loss: 0.05487 | EVM: 5.533\n",
      "0:00:10 | Epoch: 374/500 | val Loss: 0.2552 | EVM: 25.59\n",
      "0:00:00 | Epoch: 375/500 | train Loss: 0.04809 | EVM: 4.842\n",
      "0:00:11 | Epoch: 375/500 | val Loss: 0.2534 | EVM: 25.41\n",
      "0:00:00 | Epoch: 376/500 | train Loss: 0.05431 | EVM: 5.507\n",
      "0:00:11 | Epoch: 376/500 | val Loss: 0.2558 | EVM: 25.64\n",
      "0:00:00 | Epoch: 377/500 | train Loss: 0.05349 | EVM: 5.379\n",
      "0:00:11 | Epoch: 377/500 | val Loss: 0.2528 | EVM: 25.36\n",
      "0:00:00 | Epoch: 378/500 | train Loss: 0.05441 | EVM: 5.491\n",
      "0:00:11 | Epoch: 378/500 | val Loss: 0.2566 | EVM: 25.73\n",
      "0:00:00 | Epoch: 379/500 | train Loss: 0.05404 | EVM: 5.441\n",
      "0:00:11 | Epoch: 379/500 | val Loss: 0.2564 | EVM: 25.7\n",
      "0:00:00 | Epoch: 380/500 | train Loss: 0.05254 | EVM: 5.302\n",
      "0:00:11 | Epoch: 380/500 | val Loss: 0.2529 | EVM: 25.36\n",
      "0:00:00 | Epoch: 381/500 | train Loss: 0.05333 | EVM: 5.391\n",
      "0:00:11 | Epoch: 381/500 | val Loss: 0.2581 | EVM: 25.88\n",
      "0:00:00 | Epoch: 382/500 | train Loss: 0.05378 | EVM: 5.419\n",
      "0:00:11 | Epoch: 382/500 | val Loss: 0.2515 | EVM: 25.22\n",
      "0:00:00 | Epoch: 383/500 | train Loss: 0.04932 | EVM: 4.988\n",
      "0:00:11 | Epoch: 383/500 | val Loss: 0.2544 | EVM: 25.51\n",
      "0:00:00 | Epoch: 384/500 | train Loss: 0.05038 | EVM: 5.105\n",
      "0:00:11 | Epoch: 384/500 | val Loss: 0.2527 | EVM: 25.34\n",
      "0:00:00 | Epoch: 385/500 | train Loss: 0.05648 | EVM: 5.708\n",
      "0:00:11 | Epoch: 385/500 | val Loss: 0.2551 | EVM: 25.58\n",
      "0:00:00 | Epoch: 386/500 | train Loss: 0.05144 | EVM: 5.193\n",
      "0:00:11 | Epoch: 386/500 | val Loss: 0.2524 | EVM: 25.31\n",
      "0:00:00 | Epoch: 387/500 | train Loss: 0.05178 | EVM: 5.235\n",
      "0:00:11 | Epoch: 387/500 | val Loss: 0.2529 | EVM: 25.36\n",
      "0:00:00 | Epoch: 388/500 | train Loss: 0.0504 | EVM: 5.114\n",
      "0:00:11 | Epoch: 388/500 | val Loss: 0.2549 | EVM: 25.56\n",
      "0:00:00 | Epoch: 389/500 | train Loss: 0.05029 | EVM: 5.07\n",
      "0:00:11 | Epoch: 389/500 | val Loss: 0.2547 | EVM: 25.54\n",
      "0:00:00 | Epoch: 390/500 | train Loss: 0.05393 | EVM: 5.419\n",
      "0:00:11 | Epoch: 390/500 | val Loss: 0.2562 | EVM: 25.69\n",
      "0:00:00 | Epoch: 391/500 | train Loss: 0.04887 | EVM: 4.947\n",
      "0:00:11 | Epoch: 391/500 | val Loss: 0.2533 | EVM: 25.4\n",
      "0:00:00 | Epoch: 392/500 | train Loss: 0.05484 | EVM: 5.554\n",
      "0:00:11 | Epoch: 392/500 | val Loss: 0.2556 | EVM: 25.62\n",
      "0:00:00 | Epoch: 393/500 | train Loss: 0.04887 | EVM: 4.963\n",
      "0:00:11 | Epoch: 393/500 | val Loss: 0.2538 | EVM: 25.45\n",
      "0:00:00 | Epoch: 394/500 | train Loss: 0.05206 | EVM: 5.264\n",
      "0:00:11 | Epoch: 394/500 | val Loss: 0.253 | EVM: 25.36\n",
      "0:00:00 | Epoch: 395/500 | train Loss: 0.05344 | EVM: 5.381\n",
      "0:00:11 | Epoch: 395/500 | val Loss: 0.2569 | EVM: 25.75\n",
      "0:00:00 | Epoch: 396/500 | train Loss: 0.05063 | EVM: 5.12\n",
      "0:00:11 | Epoch: 396/500 | val Loss: 0.2557 | EVM: 25.64\n",
      "0:00:00 | Epoch: 397/500 | train Loss: 0.05067 | EVM: 5.108\n",
      "0:00:11 | Epoch: 397/500 | val Loss: 0.2545 | EVM: 25.52\n",
      "0:00:00 | Epoch: 398/500 | train Loss: 0.05013 | EVM: 5.071\n",
      "0:00:11 | Epoch: 398/500 | val Loss: 0.253 | EVM: 25.37\n",
      "0:00:00 | Epoch: 399/500 | train Loss: 0.05181 | EVM: 5.21\n",
      "0:00:11 | Epoch: 399/500 | val Loss: 0.2566 | EVM: 25.73\n",
      "0:00:00 | Epoch: 400/500 | train Loss: 0.05641 | EVM: 5.702\n",
      "0:00:11 | Epoch: 400/500 | val Loss: 0.2536 | EVM: 25.43\n",
      "0:00:01 | Epoch: 401/500 | train Loss: 0.05289 | EVM: 5.346\n",
      "0:00:11 | Epoch: 401/500 | val Loss: 0.2538 | EVM: 25.45\n",
      "0:00:00 | Epoch: 402/500 | train Loss: 0.0466 | EVM: 4.725\n",
      "0:00:11 | Epoch: 402/500 | val Loss: 0.2534 | EVM: 25.4\n",
      "0:00:00 | Epoch: 403/500 | train Loss: 0.04979 | EVM: 5.004\n",
      "0:00:11 | Epoch: 403/500 | val Loss: 0.2542 | EVM: 25.49\n",
      "0:00:00 | Epoch: 404/500 | train Loss: 0.05119 | EVM: 5.183\n",
      "0:00:11 | Epoch: 404/500 | val Loss: 0.2536 | EVM: 25.43\n",
      "0:00:00 | Epoch: 405/500 | train Loss: 0.0508 | EVM: 5.164\n",
      "0:00:11 | Epoch: 405/500 | val Loss: 0.2579 | EVM: 25.87\n",
      "0:00:00 | Epoch: 406/500 | train Loss: 0.05568 | EVM: 5.641\n",
      "0:00:11 | Epoch: 406/500 | val Loss: 0.255 | EVM: 25.57\n",
      "0:00:00 | Epoch: 407/500 | train Loss: 0.05402 | EVM: 5.498\n",
      "0:00:11 | Epoch: 407/500 | val Loss: 0.2525 | EVM: 25.33\n",
      "0:00:00 | Epoch: 408/500 | train Loss: 0.05203 | EVM: 5.32\n",
      "0:00:11 | Epoch: 408/500 | val Loss: 0.2516 | EVM: 25.23\n",
      "0:00:00 | Epoch: 409/500 | train Loss: 0.05077 | EVM: 5.134\n",
      "0:00:12 | Epoch: 409/500 | val Loss: 0.2539 | EVM: 25.46\n",
      "0:00:00 | Epoch: 410/500 | train Loss: 0.05135 | EVM: 5.237\n",
      "0:00:11 | Epoch: 410/500 | val Loss: 0.2512 | EVM: 25.19\n",
      "0:00:00 | Epoch: 411/500 | train Loss: 0.05186 | EVM: 5.238\n",
      "0:00:11 | Epoch: 411/500 | val Loss: 0.2551 | EVM: 25.58\n",
      "0:00:00 | Epoch: 412/500 | train Loss: 0.04958 | EVM: 5.009\n",
      "0:00:11 | Epoch: 412/500 | val Loss: 0.2522 | EVM: 25.28\n",
      "0:00:00 | Epoch: 413/500 | train Loss: 0.05518 | EVM: 5.624\n",
      "0:00:10 | Epoch: 413/500 | val Loss: 0.2533 | EVM: 25.4\n",
      "0:00:00 | Epoch: 414/500 | train Loss: 0.04993 | EVM: 5.03\n",
      "0:00:10 | Epoch: 414/500 | val Loss: 0.2505 | EVM: 25.12\n",
      "0:00:00 | Epoch: 415/500 | train Loss: 0.0528 | EVM: 5.343\n",
      "0:00:10 | Epoch: 415/500 | val Loss: 0.2562 | EVM: 25.69\n",
      "0:00:01 | Epoch: 416/500 | train Loss: 0.04775 | EVM: 4.797\n",
      "0:00:11 | Epoch: 416/500 | val Loss: 0.2523 | EVM: 25.29\n",
      "0:00:00 | Epoch: 417/500 | train Loss: 0.05176 | EVM: 5.225\n",
      "0:00:10 | Epoch: 417/500 | val Loss: 0.2549 | EVM: 25.56\n",
      "0:00:00 | Epoch: 418/500 | train Loss: 0.05144 | EVM: 5.212\n",
      "0:00:11 | Epoch: 418/500 | val Loss: 0.2515 | EVM: 25.22\n",
      "0:00:00 | Epoch: 419/500 | train Loss: 0.04835 | EVM: 4.903\n",
      "0:00:11 | Epoch: 419/500 | val Loss: 0.2518 | EVM: 25.25\n",
      "0:00:00 | Epoch: 420/500 | train Loss: 0.04811 | EVM: 4.894\n",
      "0:00:11 | Epoch: 420/500 | val Loss: 0.25 | EVM: 25.07\n",
      "0:00:00 | Epoch: 421/500 | train Loss: 0.04639 | EVM: 4.699\n",
      "0:00:11 | Epoch: 421/500 | val Loss: 0.2532 | EVM: 25.39\n",
      "0:00:00 | Epoch: 422/500 | train Loss: 0.05002 | EVM: 5.055\n",
      "0:00:11 | Epoch: 422/500 | val Loss: 0.2512 | EVM: 25.19\n",
      "0:00:00 | Epoch: 423/500 | train Loss: 0.05242 | EVM: 5.29\n",
      "0:00:11 | Epoch: 423/500 | val Loss: 0.256 | EVM: 25.67\n",
      "0:00:01 | Epoch: 424/500 | train Loss: 0.0493 | EVM: 4.954\n",
      "0:00:11 | Epoch: 424/500 | val Loss: 0.2515 | EVM: 25.22\n",
      "0:00:00 | Epoch: 425/500 | train Loss: 0.05075 | EVM: 5.114\n",
      "0:00:11 | Epoch: 425/500 | val Loss: 0.2533 | EVM: 25.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00 | Epoch: 426/500 | train Loss: 0.05639 | EVM: 5.709\n",
      "0:00:11 | Epoch: 426/500 | val Loss: 0.2503 | EVM: 25.1\n",
      "0:00:00 | Epoch: 427/500 | train Loss: 0.048 | EVM: 4.865\n",
      "0:00:11 | Epoch: 427/500 | val Loss: 0.2535 | EVM: 25.42\n",
      "0:00:00 | Epoch: 428/500 | train Loss: 0.0477 | EVM: 4.822\n",
      "0:00:11 | Epoch: 428/500 | val Loss: 0.2524 | EVM: 25.31\n",
      "0:00:00 | Epoch: 429/500 | train Loss: 0.05224 | EVM: 5.282\n",
      "0:00:11 | Epoch: 429/500 | val Loss: 0.2547 | EVM: 25.54\n",
      "0:00:00 | Epoch: 430/500 | train Loss: 0.0518 | EVM: 5.206\n",
      "0:00:10 | Epoch: 430/500 | val Loss: 0.2528 | EVM: 25.34\n",
      "0:00:00 | Epoch: 431/500 | train Loss: 0.04965 | EVM: 5.008\n",
      "0:00:11 | Epoch: 431/500 | val Loss: 0.2547 | EVM: 25.54\n",
      "0:00:00 | Epoch: 432/500 | train Loss: 0.04994 | EVM: 5.033\n",
      "0:00:11 | Epoch: 432/500 | val Loss: 0.2528 | EVM: 25.35\n",
      "0:00:00 | Epoch: 433/500 | train Loss: 0.04851 | EVM: 4.933\n",
      "0:00:11 | Epoch: 433/500 | val Loss: 0.2545 | EVM: 25.52\n",
      "0:00:00 | Epoch: 434/500 | train Loss: 0.04995 | EVM: 5.039\n",
      "0:00:11 | Epoch: 434/500 | val Loss: 0.2517 | EVM: 25.24\n",
      "0:00:01 | Epoch: 435/500 | train Loss: 0.05029 | EVM: 5.079\n",
      "0:00:11 | Epoch: 435/500 | val Loss: 0.2509 | EVM: 25.16\n",
      "0:00:00 | Epoch: 436/500 | train Loss: 0.04872 | EVM: 4.925\n",
      "0:00:11 | Epoch: 436/500 | val Loss: 0.2544 | EVM: 25.51\n",
      "0:00:00 | Epoch: 437/500 | train Loss: 0.04712 | EVM: 4.753\n",
      "0:00:10 | Epoch: 437/500 | val Loss: 0.2501 | EVM: 25.08\n",
      "0:00:00 | Epoch: 438/500 | train Loss: 0.04755 | EVM: 4.806\n",
      "0:00:10 | Epoch: 438/500 | val Loss: 0.2506 | EVM: 25.13\n",
      "0:00:00 | Epoch: 439/500 | train Loss: 0.04829 | EVM: 4.89\n",
      "0:00:11 | Epoch: 439/500 | val Loss: 0.2503 | EVM: 25.09\n",
      "0:00:01 | Epoch: 440/500 | train Loss: 0.04942 | EVM: 4.985\n",
      "0:00:11 | Epoch: 440/500 | val Loss: 0.2522 | EVM: 25.29\n",
      "0:00:00 | Epoch: 441/500 | train Loss: 0.05158 | EVM: 5.224\n",
      "0:00:11 | Epoch: 441/500 | val Loss: 0.2509 | EVM: 25.16\n",
      "0:00:00 | Epoch: 442/500 | train Loss: 0.04904 | EVM: 4.952\n",
      "0:00:11 | Epoch: 442/500 | val Loss: 0.2548 | EVM: 25.56\n",
      "0:00:00 | Epoch: 443/500 | train Loss: 0.05364 | EVM: 5.403\n",
      "0:00:11 | Epoch: 443/500 | val Loss: 0.2506 | EVM: 25.13\n",
      "0:00:00 | Epoch: 444/500 | train Loss: 0.04751 | EVM: 4.781\n",
      "0:00:11 | Epoch: 444/500 | val Loss: 0.2557 | EVM: 25.64\n",
      "0:00:00 | Epoch: 445/500 | train Loss: 0.04994 | EVM: 5.033\n",
      "0:00:11 | Epoch: 445/500 | val Loss: 0.2516 | EVM: 25.23\n",
      "0:00:00 | Epoch: 446/500 | train Loss: 0.04838 | EVM: 4.87\n",
      "0:00:11 | Epoch: 446/500 | val Loss: 0.2497 | EVM: 25.04\n",
      "0:00:00 | Epoch: 447/500 | train Loss: 0.04837 | EVM: 4.879\n",
      "0:00:11 | Epoch: 447/500 | val Loss: 0.2522 | EVM: 25.29\n",
      "0:00:00 | Epoch: 448/500 | train Loss: 0.04589 | EVM: 4.658\n",
      "0:00:11 | Epoch: 448/500 | val Loss: 0.251 | EVM: 25.18\n",
      "0:00:00 | Epoch: 449/500 | train Loss: 0.048 | EVM: 4.844\n",
      "0:00:11 | Epoch: 449/500 | val Loss: 0.2518 | EVM: 25.25\n",
      "0:00:00 | Epoch: 450/500 | train Loss: 0.05286 | EVM: 5.346\n",
      "0:00:11 | Epoch: 450/500 | val Loss: 0.2523 | EVM: 25.29\n",
      "0:00:00 | Epoch: 451/500 | train Loss: 0.05494 | EVM: 5.568\n",
      "0:00:11 | Epoch: 451/500 | val Loss: 0.2528 | EVM: 25.35\n",
      "0:00:00 | Epoch: 452/500 | train Loss: 0.0578 | EVM: 5.796\n",
      "0:00:11 | Epoch: 452/500 | val Loss: 0.2567 | EVM: 25.74\n",
      "0:00:00 | Epoch: 453/500 | train Loss: 0.05131 | EVM: 5.158\n",
      "0:00:11 | Epoch: 453/500 | val Loss: 0.2486 | EVM: 24.93\n",
      "0:00:00 | Epoch: 454/500 | train Loss: 0.04767 | EVM: 4.816\n",
      "0:00:11 | Epoch: 454/500 | val Loss: 0.2507 | EVM: 25.14\n",
      "0:00:00 | Epoch: 455/500 | train Loss: 0.04676 | EVM: 4.726\n",
      "0:00:11 | Epoch: 455/500 | val Loss: 0.2515 | EVM: 25.22\n",
      "0:00:00 | Epoch: 456/500 | train Loss: 0.04759 | EVM: 4.831\n",
      "0:00:11 | Epoch: 456/500 | val Loss: 0.2536 | EVM: 25.43\n",
      "0:00:00 | Epoch: 457/500 | train Loss: 0.04663 | EVM: 4.686\n",
      "0:00:11 | Epoch: 457/500 | val Loss: 0.2487 | EVM: 24.95\n",
      "0:00:00 | Epoch: 458/500 | train Loss: 0.0482 | EVM: 4.87\n",
      "0:00:11 | Epoch: 458/500 | val Loss: 0.2518 | EVM: 25.25\n",
      "0:00:00 | Epoch: 459/500 | train Loss: 0.04612 | EVM: 4.646\n",
      "0:00:11 | Epoch: 459/500 | val Loss: 0.2529 | EVM: 25.36\n",
      "0:00:00 | Epoch: 460/500 | train Loss: 0.04808 | EVM: 4.875\n",
      "0:00:11 | Epoch: 460/500 | val Loss: 0.2537 | EVM: 25.44\n",
      "0:00:00 | Epoch: 461/500 | train Loss: 0.0503 | EVM: 5.112\n",
      "0:00:11 | Epoch: 461/500 | val Loss: 0.2517 | EVM: 25.24\n",
      "0:00:00 | Epoch: 462/500 | train Loss: 0.04763 | EVM: 4.79\n",
      "0:00:10 | Epoch: 462/500 | val Loss: 0.2496 | EVM: 25.03\n",
      "0:00:00 | Epoch: 463/500 | train Loss: 0.04987 | EVM: 5.03\n",
      "0:00:11 | Epoch: 463/500 | val Loss: 0.2523 | EVM: 25.3\n",
      "0:00:00 | Epoch: 464/500 | train Loss: 0.0492 | EVM: 4.991\n",
      "0:00:11 | Epoch: 464/500 | val Loss: 0.2506 | EVM: 25.13\n",
      "0:00:00 | Epoch: 465/500 | train Loss: 0.04822 | EVM: 4.887\n",
      "0:00:11 | Epoch: 465/500 | val Loss: 0.2543 | EVM: 25.5\n",
      "0:00:00 | Epoch: 466/500 | train Loss: 0.04447 | EVM: 4.47\n",
      "0:00:11 | Epoch: 466/500 | val Loss: 0.25 | EVM: 25.07\n",
      "0:00:00 | Epoch: 467/500 | train Loss: 0.04788 | EVM: 4.816\n",
      "0:00:11 | Epoch: 467/500 | val Loss: 0.2538 | EVM: 25.45\n",
      "0:00:00 | Epoch: 468/500 | train Loss: 0.04495 | EVM: 4.532\n",
      "0:00:11 | Epoch: 468/500 | val Loss: 0.251 | EVM: 25.17\n",
      "0:00:00 | Epoch: 469/500 | train Loss: 0.04713 | EVM: 4.832\n",
      "0:00:11 | Epoch: 469/500 | val Loss: 0.2483 | EVM: 24.9\n",
      "0:00:00 | Epoch: 470/500 | train Loss: 0.04906 | EVM: 4.943\n",
      "0:00:11 | Epoch: 470/500 | val Loss: 0.2511 | EVM: 25.18\n",
      "0:00:00 | Epoch: 471/500 | train Loss: 0.04669 | EVM: 4.702\n",
      "0:00:11 | Epoch: 471/500 | val Loss: 0.2514 | EVM: 25.21\n",
      "0:00:00 | Epoch: 472/500 | train Loss: 0.04531 | EVM: 4.556\n",
      "0:00:11 | Epoch: 472/500 | val Loss: 0.2489 | EVM: 24.97\n",
      "0:00:00 | Epoch: 473/500 | train Loss: 0.04875 | EVM: 4.95\n",
      "0:00:11 | Epoch: 473/500 | val Loss: 0.2547 | EVM: 25.54\n",
      "0:00:00 | Epoch: 474/500 | train Loss: 0.04639 | EVM: 4.691\n",
      "0:00:11 | Epoch: 474/500 | val Loss: 0.2498 | EVM: 25.05\n",
      "0:00:00 | Epoch: 475/500 | train Loss: 0.04414 | EVM: 4.474\n",
      "0:00:11 | Epoch: 475/500 | val Loss: 0.2488 | EVM: 24.96\n",
      "0:00:00 | Epoch: 476/500 | train Loss: 0.04649 | EVM: 4.689\n",
      "0:00:10 | Epoch: 476/500 | val Loss: 0.2531 | EVM: 25.38\n",
      "0:00:00 | Epoch: 477/500 | train Loss: 0.04484 | EVM: 4.528\n",
      "0:00:11 | Epoch: 477/500 | val Loss: 0.2497 | EVM: 25.04\n",
      "0:00:00 | Epoch: 478/500 | train Loss: 0.04811 | EVM: 4.89\n",
      "0:00:11 | Epoch: 478/500 | val Loss: 0.2512 | EVM: 25.19\n",
      "0:00:00 | Epoch: 479/500 | train Loss: 0.05057 | EVM: 5.138\n",
      "0:00:11 | Epoch: 479/500 | val Loss: 0.2492 | EVM: 24.99\n",
      "0:00:00 | Epoch: 480/500 | train Loss: 0.04891 | EVM: 4.923\n",
      "0:00:11 | Epoch: 480/500 | val Loss: 0.2539 | EVM: 25.46\n",
      "0:00:00 | Epoch: 481/500 | train Loss: 0.0516 | EVM: 5.217\n",
      "0:00:11 | Epoch: 481/500 | val Loss: 0.2505 | EVM: 25.12\n",
      "0:00:00 | Epoch: 482/500 | train Loss: 0.04606 | EVM: 4.676\n",
      "0:00:11 | Epoch: 482/500 | val Loss: 0.2495 | EVM: 25.02\n",
      "0:00:00 | Epoch: 483/500 | train Loss: 0.04441 | EVM: 4.481\n",
      "0:00:11 | Epoch: 483/500 | val Loss: 0.2522 | EVM: 25.29\n",
      "0:00:00 | Epoch: 484/500 | train Loss: 0.04608 | EVM: 4.677\n",
      "0:00:10 | Epoch: 484/500 | val Loss: 0.2492 | EVM: 24.99\n",
      "0:00:00 | Epoch: 485/500 | train Loss: 0.05164 | EVM: 5.256\n",
      "0:00:11 | Epoch: 485/500 | val Loss: 0.2501 | EVM: 25.08\n",
      "0:00:00 | Epoch: 486/500 | train Loss: 0.04316 | EVM: 4.385\n",
      "0:00:10 | Epoch: 486/500 | val Loss: 0.2505 | EVM: 25.12\n",
      "0:00:00 | Epoch: 487/500 | train Loss: 0.04707 | EVM: 4.738\n",
      "0:00:11 | Epoch: 487/500 | val Loss: 0.2508 | EVM: 25.16\n",
      "0:00:00 | Epoch: 488/500 | train Loss: 0.04592 | EVM: 4.633\n",
      "0:00:11 | Epoch: 488/500 | val Loss: 0.2505 | EVM: 25.12\n",
      "0:00:00 | Epoch: 489/500 | train Loss: 0.04738 | EVM: 4.784\n",
      "0:00:10 | Epoch: 489/500 | val Loss: 0.2525 | EVM: 25.32\n",
      "0:00:00 | Epoch: 490/500 | train Loss: 0.04654 | EVM: 4.735\n",
      "0:00:11 | Epoch: 490/500 | val Loss: 0.2504 | EVM: 25.11\n",
      "0:00:00 | Epoch: 491/500 | train Loss: 0.04813 | EVM: 4.902\n",
      "0:00:11 | Epoch: 491/500 | val Loss: 0.2487 | EVM: 24.94\n",
      "0:00:00 | Epoch: 492/500 | train Loss: 0.04513 | EVM: 4.531\n",
      "0:00:11 | Epoch: 492/500 | val Loss: 0.252 | EVM: 25.27\n",
      "0:00:01 | Epoch: 493/500 | train Loss: 0.0448 | EVM: 4.509\n",
      "0:00:11 | Epoch: 493/500 | val Loss: 0.2503 | EVM: 25.11\n",
      "0:00:00 | Epoch: 494/500 | train Loss: 0.04408 | EVM: 4.449\n",
      "0:00:11 | Epoch: 494/500 | val Loss: 0.2503 | EVM: 25.1\n",
      "0:00:00 | Epoch: 495/500 | train Loss: 0.05037 | EVM: 5.106\n",
      "0:00:12 | Epoch: 495/500 | val Loss: 0.2499 | EVM: 25.06\n",
      "0:00:00 | Epoch: 496/500 | train Loss: 0.0449 | EVM: 4.538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:11 | Epoch: 496/500 | val Loss: 0.2515 | EVM: 25.22\n",
      "0:00:00 | Epoch: 497/500 | train Loss: 0.05107 | EVM: 5.181\n",
      "0:00:11 | Epoch: 497/500 | val Loss: 0.2517 | EVM: 25.24\n",
      "0:00:00 | Epoch: 498/500 | train Loss: 0.04572 | EVM: 4.64\n",
      "0:00:11 | Epoch: 498/500 | val Loss: 0.2481 | EVM: 24.88\n",
      "0:00:00 | Epoch: 499/500 | train Loss: 0.04847 | EVM: 4.873\n",
      "0:00:11 | Epoch: 499/500 | val Loss: 0.2505 | EVM: 25.12\n",
      "0:00:00 | Epoch: 500/500 | train Loss: 0.04482 | EVM: 4.538\n",
      "0:00:11 | Epoch: 500/500 | val Loss: 0.2538 | EVM: 25.45\n"
     ]
    }
   ],
   "source": [
    "#結果を保存しない\n",
    "tap = 201\n",
    "max_tap = 501\n",
    "batch_size = 100\n",
    "hidden_neuron = 300\n",
    "epochs = 500\n",
    "lr = 0.001\n",
    "\n",
    "device = torch.device('cpu') # 'cuda' if torch.cuda.is_available() else \n",
    "print('Device available now:', device)\n",
    "\n",
    "df_dir = '../data/input/'\n",
    "df0 = pd.read_csv(df_dir+'prbs.csv', index_col=0)\n",
    "\n",
    "condition0 = (df0['N']==13) & (df0['itr']==1) & (df0['form']=='RZ16QAM') & (df0['n']==32) & (df0['equalize']==False) & (df0['baudrate']==28) & (df0['PdBm']==1)\n",
    "sgnl0 = load_pickle(df0[condition0].iloc[0]['data_path'])\n",
    "lc0 = sgnl0.linear_compensation(2500, sgnl0.signal['x_2500'])\n",
    "x0, y0 = data_shaping(sgnl0.signal['x_0'][16::32], lc0[16::32], max_tap, tap)\n",
    "\n",
    "condition1 = (df0['N']==17) & (df0['itr']==1) & (df0['form']=='RZ16QAM') & (df0['n']==32) & (df0['equalize']==False) & (df0['baudrate']==28) & (df0['PdBm']==1)\n",
    "sgnl1 = load_pickle(df0[condition1].iloc[0]['data_path'])\n",
    "lc1 = sgnl1.linear_compensation(2500, sgnl1.signal['x_2500'])\n",
    "x1, y1 = data_shaping(sgnl1.signal['x_0'][16::32], lc1[16::32], max_tap, tap)\n",
    "\n",
    "mean = np.mean(x0)\n",
    "std = np.std(x0)\n",
    "\n",
    "train_dataset = Dataset(x=x0, y=y0, mean=mean, std=std)\n",
    "val_dataset = Dataset(x=x1, y=y1, mean=mean, std=std)\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "dataloaders_dict = {'train': train_dataloader, 'val': val_dataloader}\n",
    "\n",
    "model = ComplexANN(input_dim=tap, output_dim=1, hidden_neuron=hidden_neuron).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "model = train_model(device=device, model=model, dataloaders_dict=dataloaders_dict, criterion=criterion, optimizer=optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可視化\n",
    "annc0 = model(train_dataset[:][0]).detach().numpy()\n",
    "annc0 = annc0 * std + mean\n",
    "annc0 = annc0[:, 0] + annc0[:, 1] * 1j\n",
    "y0 = train_dataset[:][1].detach().numpy()\n",
    "y0 = y0 * std + mean\n",
    "y0 = y0.astype(np.int32)\n",
    "y0 = y0[:, 0] + y0[:, 1] * 1j\n",
    "annc1 = model(val_dataset[:][0]).detach().numpy()\n",
    "annc1 = annc1 * std + mean\n",
    "annc1 = annc1[:, 0] + annc1[:, 1] * 1j\n",
    "y1 = val_dataset[:][1].detach().numpy()\n",
    "y1 = y1 * std + mean\n",
    "y1 = y1.astype(np.int32)\n",
    "y1 = y1[:, 0] + y1[:, 1] * 1j\n",
    "\n",
    "lim = 110000\n",
    "cm = plt.get_cmap('rainbow', 16)\n",
    "\n",
    "seq0 = sgnl0.signal['x_0'][16::32]\n",
    "seq1 = sgnl1.signal['x_0'][16::32]\n",
    "symbol, inverse, counts = np.unique(seq0, return_inverse=True, return_counts=True)\n",
    "symbol_int = symbol.real.astype(np.int32) + symbol.imag.astype(np.int32) * 1j\n",
    "\n",
    "fig = plt.figure(figsize=(16, 17))\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "\n",
    "for i in range(len(symbol)):\n",
    "    ax1.plot(lc0[16::32][seq0 == symbol[i]].real, lc0[16::32][seq0 == symbol[i]].imag, '.', color=[cm(i)[0], cm(i)[1], cm(i)[2]], label=str(counts[i]))\n",
    "for i in range(len(symbol)):\n",
    "    ax1.plot(seq0[seq0 == symbol[i]].real, seq0[seq0 == symbol[i]].imag, 'o', color=[cm(i)[0], cm(i)[1], cm(i)[2]], markersize=10, markeredgewidth=1, markeredgecolor='black')\n",
    "ax1.set_title('constellation PRBS13(train) 100%linear comp.')\n",
    "ax1.set_xlim(-lim, lim)\n",
    "ax1.set_ylim(-lim, lim)\n",
    "\n",
    "for i in range(len(symbol)):\n",
    "    ax2.plot(lc1[16::32][seq1 == symbol[i]].real, lc1[16::32][seq1 == symbol[i]].imag, '.', color=[cm(i)[0], cm(i)[1], cm(i)[2]], label=str(counts[i]))\n",
    "for i in range(len(symbol)):\n",
    "    ax2.plot(seq1[seq1 == symbol[i]].real, seq1[seq1 == symbol[i]].imag, 'o', color=[cm(i)[0], cm(i)[1], cm(i)[2]], markersize=10, markeredgewidth=1, markeredgecolor='black')\n",
    "ax2.set_title('constellation PRBS17(test) 100%linear comp.')\n",
    "ax2.set_xlim(-lim, lim)\n",
    "ax2.set_ylim(-lim, lim)\n",
    "\n",
    "for i in range(len(symbol)):\n",
    "    ax3.plot(annc0[y0 == symbol_int[i]].real, annc0[y0 == symbol_int[i]].imag, '.', color=[cm(i)[0], cm(i)[1], cm(i)[2]])\n",
    "for i in range(len(symbol)):\n",
    "    ax3.plot(y0[y0 == symbol_int[i]].real, y0[y0 == symbol_int[i]].imag, 'o', color=[cm(i)[0], cm(i)[1], cm(i)[2]], markersize=10, markeredgewidth=1, markeredgecolor='black')\n",
    "ax3.set_title('constellation PRBS13(train) 100%linear+ANN comp.')\n",
    "ax3.set_xlim(-lim, lim)\n",
    "ax3.set_ylim(-lim, lim)\n",
    "\n",
    "for i in range(len(symbol)):\n",
    "    ax4.plot(annc1[y1 == symbol_int[i]].real, annc1[y1 == symbol_int[i]].imag, '.', color=[cm(i)[0], cm(i)[1], cm(i)[2]])\n",
    "for i in range(len(symbol)):\n",
    "    ax4.plot(y1[y1 == symbol_int[i]].real, y1[y1 == symbol_int[i]].imag, 'o', color=[cm(i)[0], cm(i)[1], cm(i)[2]], markersize=10, markeredgewidth=1, markeredgecolor='black')\n",
    "ax4.set_title('constellation PRBS17(test) 100%linear+ANN comp.')\n",
    "ax4.set_xlim(-lim, lim)\n",
    "ax4.set_ylim(-lim, lim);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
