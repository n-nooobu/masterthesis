{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMによる非線形歪補償\n",
    "時系列データを考慮できるLSTMによる補償"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "\n",
    "sys.path.append('../')\n",
    "from pyopt.util import save_pickle, load_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 データの整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_shaping(input_signal, signal, max_tap, tap):\n",
    "    x = np.zeros((len(input_signal) - (max_tap - 1), tap, 2), dtype=float)\n",
    "    y = np.zeros((len(input_signal) - (max_tap - 1), 2), dtype=float)\n",
    "    for i, j in enumerate(np.arange(int((max_tap - 1) / 2), len(input_signal) - int((max_tap - 1) / 2))):\n",
    "        x[i, :, 0] = signal[j - int((tap - 1) / 2): j + int((tap - 1) / 2) + 1].real\n",
    "        x[i, :, 1] = signal[j - int((tap - 1) / 2): j + int((tap - 1) / 2) + 1].imag\n",
    "        y[i, 0] = input_signal[j].real\n",
    "        y[i, 1] = input_signal[j].imag\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_shaping2(input_signal, signal, max_tap, tap):\n",
    "    x, y = data_shaping(input_signal, signal, max_tap, tap)\n",
    "    x_flap = x[:, int((tap - 1) / 2):, :][:, ::-1, :]\n",
    "    x = np.concatenate([x[:, :int((tap + 1) / 2), :], x_flap], axis=2)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size:  (2038, 5, 4)\n",
      "y size:  (2038, 2)\n"
     ]
    }
   ],
   "source": [
    "#動作確認\n",
    "tap = 9\n",
    "max_tap = 11\n",
    "\n",
    "df_dir = '../data/input/prbs.csv'\n",
    "df = pd.read_csv(df_dir, index_col=0)  # dataframe読み込み\n",
    "condition = (df['N']==13) & (df['itr']==1) & (df['form']=='RZ16QAM') & (df['n']==32) & (df['equalize']==False) & (df['baudrate']==28) & (df['PdBm']==1)\n",
    "sgnl = load_pickle(df[condition].iloc[0]['data_path'])  # dataframeから条件と合う行を取得し,pickleの保存先(data_path)にアクセス\n",
    "lc = sgnl.linear_compensation(500, sgnl.signal['x_500'])\n",
    "x, y = data_shaping2(sgnl.signal['x_0'][16::32], lc[16::32], max_tap, tap)  # ANNに入力できるようにデータを整形\n",
    "\n",
    "print('x size: ', x.shape)\n",
    "print('y size: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 平均,標準偏差の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  1122.4889875685737\n",
      "std:  52164.373260725646\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(x)\n",
    "std = np.std(x)\n",
    "\n",
    "print('mean: ', mean)\n",
    "print('std: ', std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, x, y, mean, std):\n",
    "        self.x, self.y, self.mean, self.std = x, y, mean, std\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "        \n",
    "        x = (x - self.mean) / self.std\n",
    "        y = (y - self.mean) / self.std\n",
    "        return torch.Tensor(x), torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  0.41994858\n",
      "std:  1.0200968\n",
      "tensor([[ 0.0977, -0.5941,  1.8054,  0.5386],\n",
      "        [ 0.1692, -0.6423, -0.6453,  1.8167],\n",
      "        [ 0.5423, -1.8595,  1.7882,  0.5346],\n",
      "        [ 1.4128, -0.2311, -0.6323, -0.2189],\n",
      "        [ 1.7284,  0.5301,  1.7284,  0.5301]])\n",
      "tensor([1.3295, 1.3295])\n"
     ]
    }
   ],
   "source": [
    "#動作確認\n",
    "train_dataset = Dataset(x=x, y=y, mean=mean, std=std)\n",
    "\n",
    "index = 0\n",
    "x_normalized, y_normalized = train_dataset.__getitem__(index)\n",
    "x_array = x_normalized.detach().numpy()\n",
    "\n",
    "print('mean: ', np.mean(x_array))\n",
    "print('std: ', np.std(x_array))\n",
    "print(x_normalized)\n",
    "print(y_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloaders_dict = {'train': train_dataloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, hidden0=None):\n",
    "        x, (hidden, cell) = self.rnn(x, hidden0)\n",
    "        x = self.fc(x[:, -1, :])  # int((x.shape[1] - 1) / 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0847, -0.0189],\n",
      "        [ 0.0995, -0.0231],\n",
      "        [ 0.0972, -0.0327],\n",
      "        [ 0.0919, -0.0341],\n",
      "        [ 0.1034, -0.0234],\n",
      "        [ 0.0946, -0.0539]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "#動作確認\n",
    "hidden_dim = 100\n",
    "model = LSTM(input_dim=4, hidden_dim=hidden_dim, output_dim=2)\n",
    "for x, y in train_dataloader:\n",
    "    output = model(x)\n",
    "    print(output[:6])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evm_score(y_true, y_pred):\n",
    "    if y_true.ndim == 2:\n",
    "        y_true = y_true[:, 0] + 1j * y_true[:, 1]\n",
    "        y_pred = y_pred[:, 0] + 1j * y_pred[:, 1]\n",
    "    tmp = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        tmp += abs(y_pred[i] - y_true[i]) ** 2 / abs(y_true[i]) ** 2\n",
    "    evm = torch.sqrt(tmp / len(y_pred)) * 100\n",
    "    return evm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders_dict, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for phase in dataloaders_dict.keys():\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            epoch_loss = 0.0\n",
    "            epoch_evms = 0.0\n",
    "            \n",
    "            for x, y in dataloaders_dict[phase]:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(x)\n",
    "                    loss = criterion(outputs, y)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    epoch_loss += loss.item() * x.size(0)\n",
    "                    epoch_evms = (evm_score(y, outputs) / 100) ** 2 * x.size(0)\n",
    "            \n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_evm = torch.sqrt(epoch_evms / len(dataloaders_dict[phase].dataset)) * 100\n",
    "            \n",
    "            duration = str(datetime.timedelta(seconds=time.time() - start_time))[:7]\n",
    "            print('{} | Epoch: {}/{} | {} Loss: {:.4} | EVM: {:.4}'.format(duration, epoch + 1, epochs, phase, epoch_loss, epoch_evm))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00 | Epoch: 1/5 | train Loss: 0.908 | EVM: 12.29\n",
      "0:00:00 | Epoch: 2/5 | train Loss: 0.6559 | EVM: 9.45\n",
      "0:00:00 | Epoch: 3/5 | train Loss: 0.226 | EVM: 3.254\n",
      "0:00:00 | Epoch: 4/5 | train Loss: 0.01303 | EVM: 1.621\n",
      "0:00:00 | Epoch: 5/5 | train Loss: 0.005479 | EVM: 1.164\n"
     ]
    }
   ],
   "source": [
    "#動作確認\n",
    "epochs = 5\n",
    "lr = 0.001\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "train_model(model=model, dataloaders_dict=dataloaders_dict, criterion=criterion, optimizer=optimizer, epochs=epochs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tap = 201\n",
    "max_tap = 501\n",
    "batch_size = 100\n",
    "hidden_dim = 100\n",
    "epochs = 500\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:02 | Epoch: 1/500 | train Loss: 0.9473 | EVM: 16.31\n",
      "0:00:25 | Epoch: 1/500 | val Loss: 0.8707 | EVM: 4.214\n",
      "0:00:02 | Epoch: 2/500 | train Loss: 0.7505 | EVM: 14.03\n",
      "0:00:25 | Epoch: 2/500 | val Loss: 0.6066 | EVM: 3.448\n",
      "0:00:02 | Epoch: 3/500 | train Loss: 0.4529 | EVM: 9.721\n",
      "0:00:25 | Epoch: 3/500 | val Loss: 0.2724 | EVM: 2.258\n",
      "0:00:02 | Epoch: 4/500 | train Loss: 0.1552 | EVM: 4.546\n",
      "0:00:26 | Epoch: 4/500 | val Loss: 0.05851 | EVM: 1.044\n",
      "0:00:02 | Epoch: 5/500 | train Loss: 0.02692 | EVM: 2.932\n",
      "0:00:26 | Epoch: 5/500 | val Loss: 0.0197 | EVM: 0.8369\n",
      "0:00:02 | Epoch: 6/500 | train Loss: 0.01524 | EVM: 3.299\n",
      "0:00:27 | Epoch: 6/500 | val Loss: 0.01706 | EVM: 0.7958\n",
      "0:00:02 | Epoch: 7/500 | train Loss: 0.01396 | EVM: 1.759\n",
      "0:00:27 | Epoch: 7/500 | val Loss: 0.01701 | EVM: 0.7768\n",
      "0:00:02 | Epoch: 8/500 | train Loss: 0.01338 | EVM: 3.089\n",
      "0:00:27 | Epoch: 8/500 | val Loss: 0.01639 | EVM: 0.7735\n",
      "0:00:02 | Epoch: 9/500 | train Loss: 0.01349 | EVM: 2.931\n",
      "0:00:27 | Epoch: 9/500 | val Loss: 0.01655 | EVM: 0.7822\n",
      "0:00:02 | Epoch: 10/500 | train Loss: 0.0133 | EVM: 3.004\n",
      "0:00:27 | Epoch: 10/500 | val Loss: 0.0173 | EVM: 0.8076\n",
      "0:00:02 | Epoch: 11/500 | train Loss: 0.01352 | EVM: 2.852\n",
      "0:00:27 | Epoch: 11/500 | val Loss: 0.01726 | EVM: 0.8006\n",
      "0:00:02 | Epoch: 12/500 | train Loss: 0.01332 | EVM: 3.199\n",
      "0:00:27 | Epoch: 12/500 | val Loss: 0.01778 | EVM: 0.8154\n",
      "0:00:02 | Epoch: 13/500 | train Loss: 0.01352 | EVM: 2.238\n",
      "0:00:27 | Epoch: 13/500 | val Loss: 0.01668 | EVM: 0.7861\n",
      "0:00:02 | Epoch: 14/500 | train Loss: 0.01314 | EVM: 3.102\n",
      "0:00:28 | Epoch: 14/500 | val Loss: 0.01653 | EVM: 0.7829\n",
      "0:00:03 | Epoch: 15/500 | train Loss: 0.01313 | EVM: 2.739\n",
      "0:00:30 | Epoch: 15/500 | val Loss: 0.01749 | EVM: 0.7971\n",
      "0:00:02 | Epoch: 16/500 | train Loss: 0.01384 | EVM: 3.098\n",
      "0:00:29 | Epoch: 16/500 | val Loss: 0.01637 | EVM: 0.7692\n",
      "0:00:02 | Epoch: 17/500 | train Loss: 0.01317 | EVM: 2.745\n",
      "0:00:32 | Epoch: 17/500 | val Loss: 0.01708 | EVM: 0.7919\n",
      "0:00:03 | Epoch: 18/500 | train Loss: 0.01304 | EVM: 2.452\n",
      "0:00:33 | Epoch: 18/500 | val Loss: 0.01638 | EVM: 0.7757\n",
      "0:00:02 | Epoch: 19/500 | train Loss: 0.01316 | EVM: 3.11\n",
      "0:00:30 | Epoch: 19/500 | val Loss: 0.01637 | EVM: 0.7778\n",
      "0:00:02 | Epoch: 20/500 | train Loss: 0.01288 | EVM: 2.986\n",
      "0:00:31 | Epoch: 20/500 | val Loss: 0.01669 | EVM: 0.7889\n",
      "0:00:02 | Epoch: 21/500 | train Loss: 0.01295 | EVM: 2.418\n",
      "0:00:30 | Epoch: 21/500 | val Loss: 0.01658 | EVM: 0.7809\n",
      "0:00:02 | Epoch: 22/500 | train Loss: 0.01293 | EVM: 3.001\n",
      "0:00:30 | Epoch: 22/500 | val Loss: 0.01645 | EVM: 0.7624\n",
      "0:00:02 | Epoch: 23/500 | train Loss: 0.01335 | EVM: 3.144\n",
      "0:00:30 | Epoch: 23/500 | val Loss: 0.01669 | EVM: 0.7756\n",
      "0:00:02 | Epoch: 24/500 | train Loss: 0.01345 | EVM: 2.612\n",
      "0:00:30 | Epoch: 24/500 | val Loss: 0.01675 | EVM: 0.7959\n",
      "0:00:02 | Epoch: 25/500 | train Loss: 0.01308 | EVM: 2.482\n",
      "0:00:32 | Epoch: 25/500 | val Loss: 0.01686 | EVM: 0.7919\n",
      "0:00:02 | Epoch: 26/500 | train Loss: 0.01315 | EVM: 2.943\n",
      "0:00:30 | Epoch: 26/500 | val Loss: 0.01672 | EVM: 0.7919\n",
      "0:00:02 | Epoch: 27/500 | train Loss: 0.01298 | EVM: 2.945\n",
      "0:00:30 | Epoch: 27/500 | val Loss: 0.01645 | EVM: 0.7758\n",
      "0:00:02 | Epoch: 28/500 | train Loss: 0.01297 | EVM: 3.097\n",
      "0:00:30 | Epoch: 28/500 | val Loss: 0.01656 | EVM: 0.7812\n",
      "0:00:02 | Epoch: 29/500 | train Loss: 0.01289 | EVM: 2.984\n",
      "0:00:30 | Epoch: 29/500 | val Loss: 0.01674 | EVM: 0.7866\n",
      "0:00:02 | Epoch: 30/500 | train Loss: 0.01332 | EVM: 2.91\n",
      "0:00:30 | Epoch: 30/500 | val Loss: 0.01722 | EVM: 0.7909\n",
      "0:00:02 | Epoch: 31/500 | train Loss: 0.0129 | EVM: 3.066\n",
      "0:00:30 | Epoch: 31/500 | val Loss: 0.0162 | EVM: 0.7748\n",
      "0:00:02 | Epoch: 32/500 | train Loss: 0.01269 | EVM: 2.832\n",
      "0:00:30 | Epoch: 32/500 | val Loss: 0.01644 | EVM: 0.7848\n",
      "0:00:02 | Epoch: 33/500 | train Loss: 0.01285 | EVM: 2.476\n",
      "0:00:30 | Epoch: 33/500 | val Loss: 0.01636 | EVM: 0.783\n",
      "0:00:02 | Epoch: 34/500 | train Loss: 0.01277 | EVM: 2.71\n",
      "0:00:30 | Epoch: 34/500 | val Loss: 0.01658 | EVM: 0.7897\n",
      "0:00:02 | Epoch: 35/500 | train Loss: 0.01269 | EVM: 3.171\n",
      "0:00:31 | Epoch: 35/500 | val Loss: 0.01621 | EVM: 0.7713\n",
      "0:00:02 | Epoch: 36/500 | train Loss: 0.01267 | EVM: 2.619\n",
      "0:00:30 | Epoch: 36/500 | val Loss: 0.01695 | EVM: 0.7882\n",
      "0:00:02 | Epoch: 37/500 | train Loss: 0.01271 | EVM: 3.031\n",
      "0:00:30 | Epoch: 37/500 | val Loss: 0.01623 | EVM: 0.7622\n",
      "0:00:02 | Epoch: 38/500 | train Loss: 0.01249 | EVM: 2.988\n",
      "0:00:30 | Epoch: 38/500 | val Loss: 0.01621 | EVM: 0.793\n",
      "0:00:02 | Epoch: 39/500 | train Loss: 0.01264 | EVM: 2.499\n",
      "0:00:30 | Epoch: 39/500 | val Loss: 0.01643 | EVM: 0.7808\n",
      "0:00:02 | Epoch: 40/500 | train Loss: 0.01258 | EVM: 3.225\n",
      "0:00:31 | Epoch: 40/500 | val Loss: 0.01687 | EVM: 0.7694\n",
      "0:00:02 | Epoch: 41/500 | train Loss: 0.01267 | EVM: 2.578\n",
      "0:00:30 | Epoch: 41/500 | val Loss: 0.01664 | EVM: 0.7786\n",
      "0:00:02 | Epoch: 42/500 | train Loss: 0.01267 | EVM: 3.301\n",
      "0:00:30 | Epoch: 42/500 | val Loss: 0.017 | EVM: 0.8156\n",
      "0:00:02 | Epoch: 43/500 | train Loss: 0.01289 | EVM: 2.689\n",
      "0:00:30 | Epoch: 43/500 | val Loss: 0.01646 | EVM: 0.7698\n",
      "0:00:02 | Epoch: 44/500 | train Loss: 0.0124 | EVM: 2.721\n",
      "0:00:30 | Epoch: 44/500 | val Loss: 0.0165 | EVM: 0.7913\n",
      "0:00:02 | Epoch: 45/500 | train Loss: 0.01249 | EVM: 2.582\n",
      "0:00:30 | Epoch: 45/500 | val Loss: 0.01634 | EVM: 0.7818\n",
      "0:00:02 | Epoch: 46/500 | train Loss: 0.0128 | EVM: 2.428\n",
      "0:00:30 | Epoch: 46/500 | val Loss: 0.01636 | EVM: 0.7743\n",
      "0:00:02 | Epoch: 47/500 | train Loss: 0.01292 | EVM: 2.86\n",
      "0:00:30 | Epoch: 47/500 | val Loss: 0.0167 | EVM: 0.7613\n",
      "0:00:02 | Epoch: 48/500 | train Loss: 0.01287 | EVM: 3.262\n",
      "0:00:30 | Epoch: 48/500 | val Loss: 0.01684 | EVM: 0.7869\n",
      "0:00:02 | Epoch: 49/500 | train Loss: 0.01257 | EVM: 2.948\n",
      "0:00:30 | Epoch: 49/500 | val Loss: 0.01641 | EVM: 0.8045\n",
      "0:00:02 | Epoch: 50/500 | train Loss: 0.01264 | EVM: 2.579\n",
      "0:00:32 | Epoch: 50/500 | val Loss: 0.01818 | EVM: 0.834\n",
      "0:00:02 | Epoch: 51/500 | train Loss: 0.01331 | EVM: 2.531\n",
      "0:00:31 | Epoch: 51/500 | val Loss: 0.01656 | EVM: 0.8048\n",
      "0:00:03 | Epoch: 52/500 | train Loss: 0.0128 | EVM: 2.785\n",
      "0:00:32 | Epoch: 52/500 | val Loss: 0.01639 | EVM: 0.7677\n",
      "0:00:02 | Epoch: 53/500 | train Loss: 0.01257 | EVM: 2.787\n",
      "0:00:31 | Epoch: 53/500 | val Loss: 0.01638 | EVM: 0.7876\n",
      "0:00:02 | Epoch: 54/500 | train Loss: 0.01229 | EVM: 2.659\n",
      "0:00:30 | Epoch: 54/500 | val Loss: 0.01635 | EVM: 0.7741\n",
      "0:00:02 | Epoch: 55/500 | train Loss: 0.01236 | EVM: 2.768\n",
      "0:00:30 | Epoch: 55/500 | val Loss: 0.01637 | EVM: 0.7818\n",
      "0:00:02 | Epoch: 56/500 | train Loss: 0.01223 | EVM: 2.881\n",
      "0:00:30 | Epoch: 56/500 | val Loss: 0.01611 | EVM: 0.7865\n",
      "0:00:02 | Epoch: 57/500 | train Loss: 0.01228 | EVM: 2.403\n",
      "0:00:31 | Epoch: 57/500 | val Loss: 0.01631 | EVM: 0.8032\n",
      "0:00:02 | Epoch: 58/500 | train Loss: 0.01226 | EVM: 2.677\n",
      "0:00:30 | Epoch: 58/500 | val Loss: 0.01655 | EVM: 0.7629\n",
      "0:00:02 | Epoch: 59/500 | train Loss: 0.01254 | EVM: 2.498\n",
      "0:00:31 | Epoch: 59/500 | val Loss: 0.01703 | EVM: 0.7991\n",
      "0:00:02 | Epoch: 60/500 | train Loss: 0.01253 | EVM: 2.941\n",
      "0:00:31 | Epoch: 60/500 | val Loss: 0.01665 | EVM: 0.7835\n",
      "0:00:02 | Epoch: 61/500 | train Loss: 0.01248 | EVM: 2.952\n",
      "0:00:30 | Epoch: 61/500 | val Loss: 0.01676 | EVM: 0.808\n",
      "0:00:02 | Epoch: 62/500 | train Loss: 0.01278 | EVM: 3.529\n",
      "0:00:30 | Epoch: 62/500 | val Loss: 0.0172 | EVM: 0.7958\n",
      "0:00:02 | Epoch: 63/500 | train Loss: 0.0126 | EVM: 3.227\n",
      "0:00:31 | Epoch: 63/500 | val Loss: 0.01742 | EVM: 0.8277\n",
      "0:00:02 | Epoch: 64/500 | train Loss: 0.01293 | EVM: 2.99\n",
      "0:00:32 | Epoch: 64/500 | val Loss: 0.01737 | EVM: 0.7882\n",
      "0:00:02 | Epoch: 65/500 | train Loss: 0.01262 | EVM: 3.002\n",
      "0:00:30 | Epoch: 65/500 | val Loss: 0.01611 | EVM: 0.7965\n",
      "0:00:02 | Epoch: 66/500 | train Loss: 0.01217 | EVM: 2.858\n",
      "0:00:30 | Epoch: 66/500 | val Loss: 0.01677 | EVM: 0.7782\n",
      "0:00:02 | Epoch: 67/500 | train Loss: 0.01235 | EVM: 3.119\n",
      "0:00:30 | Epoch: 67/500 | val Loss: 0.01674 | EVM: 0.7834\n",
      "0:00:02 | Epoch: 68/500 | train Loss: 0.01233 | EVM: 2.877\n",
      "0:00:30 | Epoch: 68/500 | val Loss: 0.01663 | EVM: 0.8113\n",
      "0:00:02 | Epoch: 69/500 | train Loss: 0.01224 | EVM: 2.49\n",
      "0:00:30 | Epoch: 69/500 | val Loss: 0.01634 | EVM: 0.7877\n",
      "0:00:02 | Epoch: 70/500 | train Loss: 0.01268 | EVM: 2.601\n",
      "0:00:30 | Epoch: 70/500 | val Loss: 0.01695 | EVM: 0.8018\n",
      "0:00:02 | Epoch: 71/500 | train Loss: 0.01249 | EVM: 2.435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:30 | Epoch: 71/500 | val Loss: 0.01691 | EVM: 0.7891\n",
      "0:00:02 | Epoch: 72/500 | train Loss: 0.01226 | EVM: 3.312\n",
      "0:00:31 | Epoch: 72/500 | val Loss: 0.01636 | EVM: 0.7971\n",
      "0:00:02 | Epoch: 73/500 | train Loss: 0.01232 | EVM: 3.076\n",
      "0:00:30 | Epoch: 73/500 | val Loss: 0.0167 | EVM: 0.8028\n",
      "0:00:02 | Epoch: 74/500 | train Loss: 0.01218 | EVM: 2.579\n",
      "0:00:31 | Epoch: 74/500 | val Loss: 0.01626 | EVM: 0.8035\n",
      "0:00:02 | Epoch: 75/500 | train Loss: 0.01258 | EVM: 1.978\n",
      "0:00:30 | Epoch: 75/500 | val Loss: 0.01747 | EVM: 0.795\n",
      "0:00:02 | Epoch: 76/500 | train Loss: 0.0122 | EVM: 2.609\n",
      "0:00:30 | Epoch: 76/500 | val Loss: 0.01638 | EVM: 0.7737\n",
      "0:00:02 | Epoch: 77/500 | train Loss: 0.01218 | EVM: 2.65\n",
      "0:00:30 | Epoch: 77/500 | val Loss: 0.01726 | EVM: 0.8157\n",
      "0:00:02 | Epoch: 78/500 | train Loss: 0.0122 | EVM: 2.581\n",
      "0:00:30 | Epoch: 78/500 | val Loss: 0.01655 | EVM: 0.8037\n",
      "0:00:02 | Epoch: 79/500 | train Loss: 0.01265 | EVM: 2.243\n",
      "0:00:30 | Epoch: 79/500 | val Loss: 0.0166 | EVM: 0.7923\n",
      "0:00:02 | Epoch: 80/500 | train Loss: 0.01224 | EVM: 2.889\n",
      "0:00:29 | Epoch: 80/500 | val Loss: 0.01691 | EVM: 0.8008\n",
      "0:00:02 | Epoch: 81/500 | train Loss: 0.01204 | EVM: 2.813\n",
      "0:00:28 | Epoch: 81/500 | val Loss: 0.01677 | EVM: 0.8055\n",
      "0:00:02 | Epoch: 82/500 | train Loss: 0.01204 | EVM: 2.624\n",
      "0:00:27 | Epoch: 82/500 | val Loss: 0.01688 | EVM: 0.7869\n",
      "0:00:02 | Epoch: 83/500 | train Loss: 0.01226 | EVM: 2.008\n",
      "0:00:27 | Epoch: 83/500 | val Loss: 0.01685 | EVM: 0.8022\n",
      "0:00:02 | Epoch: 84/500 | train Loss: 0.01197 | EVM: 2.696\n",
      "0:00:28 | Epoch: 84/500 | val Loss: 0.01625 | EVM: 0.7911\n",
      "0:00:02 | Epoch: 85/500 | train Loss: 0.01182 | EVM: 2.899\n",
      "0:00:28 | Epoch: 85/500 | val Loss: 0.01742 | EVM: 0.8127\n",
      "0:00:02 | Epoch: 86/500 | train Loss: 0.012 | EVM: 3.162\n",
      "0:00:27 | Epoch: 86/500 | val Loss: 0.01694 | EVM: 0.8173\n",
      "0:00:02 | Epoch: 87/500 | train Loss: 0.01206 | EVM: 2.518\n",
      "0:00:28 | Epoch: 87/500 | val Loss: 0.01752 | EVM: 0.8035\n",
      "0:00:02 | Epoch: 88/500 | train Loss: 0.01186 | EVM: 2.023\n",
      "0:00:27 | Epoch: 88/500 | val Loss: 0.0168 | EVM: 0.797\n",
      "0:00:02 | Epoch: 89/500 | train Loss: 0.01172 | EVM: 2.769\n",
      "0:00:27 | Epoch: 89/500 | val Loss: 0.01705 | EVM: 0.7965\n",
      "0:00:02 | Epoch: 90/500 | train Loss: 0.01228 | EVM: 2.931\n",
      "0:00:27 | Epoch: 90/500 | val Loss: 0.01737 | EVM: 0.8082\n",
      "0:00:02 | Epoch: 91/500 | train Loss: 0.01203 | EVM: 2.998\n",
      "0:00:27 | Epoch: 91/500 | val Loss: 0.01653 | EVM: 0.7898\n",
      "0:00:02 | Epoch: 92/500 | train Loss: 0.01199 | EVM: 3.149\n",
      "0:00:27 | Epoch: 92/500 | val Loss: 0.01817 | EVM: 0.8508\n",
      "0:00:02 | Epoch: 93/500 | train Loss: 0.01187 | EVM: 2.501\n",
      "0:00:27 | Epoch: 93/500 | val Loss: 0.01679 | EVM: 0.7962\n",
      "0:00:02 | Epoch: 94/500 | train Loss: 0.01172 | EVM: 2.648\n",
      "0:00:27 | Epoch: 94/500 | val Loss: 0.01709 | EVM: 0.811\n",
      "0:00:02 | Epoch: 95/500 | train Loss: 0.01161 | EVM: 3.033\n",
      "0:00:28 | Epoch: 95/500 | val Loss: 0.01649 | EVM: 0.7875\n",
      "0:00:02 | Epoch: 96/500 | train Loss: 0.01163 | EVM: 3.118\n",
      "0:00:27 | Epoch: 96/500 | val Loss: 0.01703 | EVM: 0.8098\n",
      "0:00:02 | Epoch: 97/500 | train Loss: 0.01154 | EVM: 3.056\n",
      "0:00:27 | Epoch: 97/500 | val Loss: 0.01723 | EVM: 0.787\n",
      "0:00:02 | Epoch: 98/500 | train Loss: 0.01158 | EVM: 2.384\n",
      "0:00:27 | Epoch: 98/500 | val Loss: 0.01686 | EVM: 0.7993\n",
      "0:00:02 | Epoch: 99/500 | train Loss: 0.01162 | EVM: 2.905\n",
      "0:00:29 | Epoch: 99/500 | val Loss: 0.01737 | EVM: 0.8114\n",
      "0:00:02 | Epoch: 100/500 | train Loss: 0.01152 | EVM: 3.176\n",
      "0:00:29 | Epoch: 100/500 | val Loss: 0.01741 | EVM: 0.8181\n",
      "0:00:02 | Epoch: 101/500 | train Loss: 0.01171 | EVM: 3.078\n",
      "0:00:27 | Epoch: 101/500 | val Loss: 0.017 | EVM: 0.7892\n",
      "0:00:02 | Epoch: 102/500 | train Loss: 0.01141 | EVM: 2.252\n",
      "0:00:28 | Epoch: 102/500 | val Loss: 0.01717 | EVM: 0.8045\n",
      "0:00:02 | Epoch: 103/500 | train Loss: 0.01135 | EVM: 2.699\n",
      "0:00:27 | Epoch: 103/500 | val Loss: 0.01726 | EVM: 0.7955\n",
      "0:00:02 | Epoch: 104/500 | train Loss: 0.01127 | EVM: 2.089\n",
      "0:00:27 | Epoch: 104/500 | val Loss: 0.01702 | EVM: 0.8152\n",
      "0:00:02 | Epoch: 105/500 | train Loss: 0.01147 | EVM: 2.59\n",
      "0:00:27 | Epoch: 105/500 | val Loss: 0.01812 | EVM: 0.8071\n",
      "0:00:02 | Epoch: 106/500 | train Loss: 0.01151 | EVM: 2.896\n",
      "0:00:27 | Epoch: 106/500 | val Loss: 0.01826 | EVM: 0.8452\n",
      "0:00:02 | Epoch: 107/500 | train Loss: 0.01127 | EVM: 2.385\n",
      "0:00:27 | Epoch: 107/500 | val Loss: 0.01729 | EVM: 0.7912\n",
      "0:00:02 | Epoch: 108/500 | train Loss: 0.0114 | EVM: 3.129\n",
      "0:00:28 | Epoch: 108/500 | val Loss: 0.01733 | EVM: 0.7963\n",
      "0:00:02 | Epoch: 109/500 | train Loss: 0.0111 | EVM: 2.031\n",
      "0:00:28 | Epoch: 109/500 | val Loss: 0.01814 | EVM: 0.8245\n",
      "0:00:02 | Epoch: 110/500 | train Loss: 0.01119 | EVM: 3.199\n",
      "0:00:30 | Epoch: 110/500 | val Loss: 0.01733 | EVM: 0.8043\n",
      "0:00:02 | Epoch: 111/500 | train Loss: 0.0112 | EVM: 3.244\n",
      "0:00:28 | Epoch: 111/500 | val Loss: 0.01755 | EVM: 0.7956\n",
      "0:00:03 | Epoch: 112/500 | train Loss: 0.01124 | EVM: 2.455\n",
      "0:00:28 | Epoch: 112/500 | val Loss: 0.01777 | EVM: 0.8115\n",
      "0:00:02 | Epoch: 113/500 | train Loss: 0.01148 | EVM: 2.188\n",
      "0:00:27 | Epoch: 113/500 | val Loss: 0.0174 | EVM: 0.8193\n",
      "0:00:02 | Epoch: 114/500 | train Loss: 0.01107 | EVM: 2.567\n",
      "0:00:30 | Epoch: 114/500 | val Loss: 0.01741 | EVM: 0.7853\n",
      "0:00:04 | Epoch: 115/500 | train Loss: 0.01089 | EVM: 2.168\n",
      "0:00:38 | Epoch: 115/500 | val Loss: 0.01716 | EVM: 0.801\n",
      "0:00:02 | Epoch: 116/500 | train Loss: 0.01096 | EVM: 2.417\n",
      "0:00:27 | Epoch: 116/500 | val Loss: 0.01753 | EVM: 0.8184\n",
      "0:00:02 | Epoch: 117/500 | train Loss: 0.01132 | EVM: 2.904\n",
      "0:00:28 | Epoch: 117/500 | val Loss: 0.01796 | EVM: 0.8048\n",
      "0:00:02 | Epoch: 118/500 | train Loss: 0.01115 | EVM: 2.71\n",
      "0:00:27 | Epoch: 118/500 | val Loss: 0.01787 | EVM: 0.8098\n",
      "0:00:02 | Epoch: 119/500 | train Loss: 0.01095 | EVM: 2.454\n",
      "0:00:27 | Epoch: 119/500 | val Loss: 0.01734 | EVM: 0.7894\n",
      "0:00:02 | Epoch: 120/500 | train Loss: 0.01075 | EVM: 2.285\n",
      "0:00:28 | Epoch: 120/500 | val Loss: 0.01743 | EVM: 0.8105\n",
      "0:00:02 | Epoch: 121/500 | train Loss: 0.01079 | EVM: 2.88\n",
      "0:00:28 | Epoch: 121/500 | val Loss: 0.01847 | EVM: 0.8111\n",
      "0:00:02 | Epoch: 122/500 | train Loss: 0.01103 | EVM: 2.319\n",
      "0:00:27 | Epoch: 122/500 | val Loss: 0.0177 | EVM: 0.8019\n",
      "0:00:02 | Epoch: 123/500 | train Loss: 0.01086 | EVM: 2.778\n",
      "0:00:27 | Epoch: 123/500 | val Loss: 0.0176 | EVM: 0.8043\n",
      "0:00:02 | Epoch: 124/500 | train Loss: 0.01068 | EVM: 2.03\n",
      "0:00:27 | Epoch: 124/500 | val Loss: 0.01805 | EVM: 0.8334\n",
      "0:00:02 | Epoch: 125/500 | train Loss: 0.01068 | EVM: 2.121\n",
      "0:00:27 | Epoch: 125/500 | val Loss: 0.01891 | EVM: 0.8189\n",
      "0:00:02 | Epoch: 126/500 | train Loss: 0.01035 | EVM: 2.321\n",
      "0:00:27 | Epoch: 126/500 | val Loss: 0.01735 | EVM: 0.805\n",
      "0:00:02 | Epoch: 127/500 | train Loss: 0.01055 | EVM: 2.197\n",
      "0:00:27 | Epoch: 127/500 | val Loss: 0.01879 | EVM: 0.8038\n",
      "0:00:02 | Epoch: 128/500 | train Loss: 0.01054 | EVM: 2.324\n",
      "0:00:27 | Epoch: 128/500 | val Loss: 0.01763 | EVM: 0.807\n",
      "0:00:02 | Epoch: 129/500 | train Loss: 0.01039 | EVM: 2.188\n",
      "0:00:27 | Epoch: 129/500 | val Loss: 0.01769 | EVM: 0.7754\n",
      "0:00:02 | Epoch: 130/500 | train Loss: 0.01035 | EVM: 2.673\n",
      "0:00:27 | Epoch: 130/500 | val Loss: 0.01825 | EVM: 0.8096\n",
      "0:00:02 | Epoch: 131/500 | train Loss: 0.01038 | EVM: 2.637\n",
      "0:00:27 | Epoch: 131/500 | val Loss: 0.01786 | EVM: 0.81\n",
      "0:00:02 | Epoch: 132/500 | train Loss: 0.0105 | EVM: 2.317\n",
      "0:00:27 | Epoch: 132/500 | val Loss: 0.01879 | EVM: 0.805\n",
      "0:00:02 | Epoch: 133/500 | train Loss: 0.01046 | EVM: 2.873\n",
      "0:00:28 | Epoch: 133/500 | val Loss: 0.01833 | EVM: 0.808\n",
      "0:00:02 | Epoch: 134/500 | train Loss: 0.01056 | EVM: 2.484\n",
      "0:00:27 | Epoch: 134/500 | val Loss: 0.01884 | EVM: 0.8283\n",
      "0:00:02 | Epoch: 135/500 | train Loss: 0.01041 | EVM: 2.64\n",
      "0:00:27 | Epoch: 135/500 | val Loss: 0.01805 | EVM: 0.7784\n",
      "0:00:02 | Epoch: 136/500 | train Loss: 0.01001 | EVM: 2.096\n",
      "0:00:27 | Epoch: 136/500 | val Loss: 0.01823 | EVM: 0.797\n",
      "0:00:02 | Epoch: 137/500 | train Loss: 0.009833 | EVM: 2.001\n",
      "0:00:26 | Epoch: 137/500 | val Loss: 0.01796 | EVM: 0.8011\n",
      "0:00:02 | Epoch: 138/500 | train Loss: 0.009773 | EVM: 2.473\n",
      "0:00:27 | Epoch: 138/500 | val Loss: 0.01888 | EVM: 0.8336\n",
      "0:00:02 | Epoch: 139/500 | train Loss: 0.009883 | EVM: 2.504\n",
      "0:00:27 | Epoch: 139/500 | val Loss: 0.01945 | EVM: 0.8187\n",
      "0:00:02 | Epoch: 140/500 | train Loss: 0.01005 | EVM: 2.504\n",
      "0:00:27 | Epoch: 140/500 | val Loss: 0.01853 | EVM: 0.7932\n",
      "0:00:02 | Epoch: 141/500 | train Loss: 0.009823 | EVM: 2.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:27 | Epoch: 141/500 | val Loss: 0.01848 | EVM: 0.8152\n",
      "0:00:02 | Epoch: 142/500 | train Loss: 0.009951 | EVM: 2.713\n",
      "0:00:27 | Epoch: 142/500 | val Loss: 0.01988 | EVM: 0.7919\n",
      "0:00:02 | Epoch: 143/500 | train Loss: 0.009765 | EVM: 2.621\n",
      "0:00:27 | Epoch: 143/500 | val Loss: 0.01851 | EVM: 0.7966\n",
      "0:00:02 | Epoch: 144/500 | train Loss: 0.00961 | EVM: 2.704\n",
      "0:00:27 | Epoch: 144/500 | val Loss: 0.01906 | EVM: 0.8302\n",
      "0:00:02 | Epoch: 145/500 | train Loss: 0.009782 | EVM: 2.616\n",
      "0:00:26 | Epoch: 145/500 | val Loss: 0.01826 | EVM: 0.7815\n",
      "0:00:02 | Epoch: 146/500 | train Loss: 0.009278 | EVM: 2.276\n",
      "0:00:29 | Epoch: 146/500 | val Loss: 0.01894 | EVM: 0.7782\n",
      "0:00:02 | Epoch: 147/500 | train Loss: 0.009189 | EVM: 2.94\n",
      "0:00:27 | Epoch: 147/500 | val Loss: 0.01919 | EVM: 0.8108\n",
      "0:00:02 | Epoch: 148/500 | train Loss: 0.009318 | EVM: 2.439\n",
      "0:00:29 | Epoch: 148/500 | val Loss: 0.01885 | EVM: 0.7718\n",
      "0:00:02 | Epoch: 149/500 | train Loss: 0.009461 | EVM: 1.769\n",
      "0:00:29 | Epoch: 149/500 | val Loss: 0.01963 | EVM: 0.8123\n",
      "0:00:03 | Epoch: 150/500 | train Loss: 0.00929 | EVM: 2.34\n",
      "0:00:37 | Epoch: 150/500 | val Loss: 0.01911 | EVM: 0.7711\n",
      "0:00:03 | Epoch: 151/500 | train Loss: 0.009109 | EVM: 2.036\n",
      "0:00:37 | Epoch: 151/500 | val Loss: 0.01896 | EVM: 0.8128\n",
      "0:00:03 | Epoch: 152/500 | train Loss: 0.009244 | EVM: 2.501\n",
      "0:00:37 | Epoch: 152/500 | val Loss: 0.0189 | EVM: 0.8198\n",
      "0:00:03 | Epoch: 153/500 | train Loss: 0.009105 | EVM: 1.985\n",
      "0:00:37 | Epoch: 153/500 | val Loss: 0.01914 | EVM: 0.7839\n",
      "0:00:03 | Epoch: 154/500 | train Loss: 0.008902 | EVM: 2.322\n",
      "0:00:37 | Epoch: 154/500 | val Loss: 0.01933 | EVM: 0.7844\n",
      "0:00:03 | Epoch: 155/500 | train Loss: 0.008787 | EVM: 2.048\n",
      "0:00:37 | Epoch: 155/500 | val Loss: 0.0189 | EVM: 0.8013\n",
      "0:00:03 | Epoch: 156/500 | train Loss: 0.008458 | EVM: 2.201\n",
      "0:00:37 | Epoch: 156/500 | val Loss: 0.01964 | EVM: 0.8005\n",
      "0:00:03 | Epoch: 157/500 | train Loss: 0.008538 | EVM: 2.228\n",
      "0:00:37 | Epoch: 157/500 | val Loss: 0.0198 | EVM: 0.7848\n",
      "0:00:03 | Epoch: 158/500 | train Loss: 0.008469 | EVM: 1.958\n",
      "0:00:37 | Epoch: 158/500 | val Loss: 0.0194 | EVM: 0.8073\n",
      "0:00:03 | Epoch: 159/500 | train Loss: 0.008522 | EVM: 2.611\n",
      "0:00:37 | Epoch: 159/500 | val Loss: 0.01964 | EVM: 0.8016\n",
      "0:00:03 | Epoch: 160/500 | train Loss: 0.008451 | EVM: 2.474\n",
      "0:00:37 | Epoch: 160/500 | val Loss: 0.02014 | EVM: 0.8016\n",
      "0:00:03 | Epoch: 161/500 | train Loss: 0.008068 | EVM: 2.149\n",
      "0:00:37 | Epoch: 161/500 | val Loss: 0.01981 | EVM: 0.7906\n",
      "0:00:03 | Epoch: 162/500 | train Loss: 0.007963 | EVM: 2.096\n",
      "0:00:37 | Epoch: 162/500 | val Loss: 0.02013 | EVM: 0.814\n",
      "0:00:03 | Epoch: 163/500 | train Loss: 0.0079 | EVM: 2.6\n",
      "0:00:37 | Epoch: 163/500 | val Loss: 0.02023 | EVM: 0.819\n",
      "0:00:03 | Epoch: 164/500 | train Loss: 0.007985 | EVM: 2.084\n",
      "0:00:37 | Epoch: 164/500 | val Loss: 0.02082 | EVM: 0.8224\n",
      "0:00:03 | Epoch: 165/500 | train Loss: 0.008035 | EVM: 2.105\n",
      "0:00:37 | Epoch: 165/500 | val Loss: 0.02041 | EVM: 0.8146\n",
      "0:00:03 | Epoch: 166/500 | train Loss: 0.007794 | EVM: 2.17\n",
      "0:00:37 | Epoch: 166/500 | val Loss: 0.02037 | EVM: 0.795\n",
      "0:00:03 | Epoch: 167/500 | train Loss: 0.007643 | EVM: 2.295\n",
      "0:00:37 | Epoch: 167/500 | val Loss: 0.02099 | EVM: 0.8236\n",
      "0:00:03 | Epoch: 168/500 | train Loss: 0.007476 | EVM: 2.459\n",
      "0:00:37 | Epoch: 168/500 | val Loss: 0.02095 | EVM: 0.7963\n",
      "0:00:03 | Epoch: 169/500 | train Loss: 0.007532 | EVM: 2.365\n",
      "0:00:37 | Epoch: 169/500 | val Loss: 0.02053 | EVM: 0.8151\n",
      "0:00:03 | Epoch: 170/500 | train Loss: 0.007198 | EVM: 1.905\n",
      "0:00:37 | Epoch: 170/500 | val Loss: 0.02096 | EVM: 0.8125\n",
      "0:00:03 | Epoch: 171/500 | train Loss: 0.007106 | EVM: 2.197\n",
      "0:00:37 | Epoch: 171/500 | val Loss: 0.02104 | EVM: 0.8345\n",
      "0:00:03 | Epoch: 172/500 | train Loss: 0.0069 | EVM: 2.025\n",
      "0:00:37 | Epoch: 172/500 | val Loss: 0.02162 | EVM: 0.8189\n",
      "0:00:03 | Epoch: 173/500 | train Loss: 0.006762 | EVM: 2.227\n",
      "0:00:37 | Epoch: 173/500 | val Loss: 0.02149 | EVM: 0.8106\n",
      "0:00:03 | Epoch: 174/500 | train Loss: 0.006773 | EVM: 1.79\n",
      "0:00:37 | Epoch: 174/500 | val Loss: 0.02106 | EVM: 0.8185\n",
      "0:00:03 | Epoch: 175/500 | train Loss: 0.006494 | EVM: 2.376\n",
      "0:00:37 | Epoch: 175/500 | val Loss: 0.0223 | EVM: 0.835\n",
      "0:00:03 | Epoch: 176/500 | train Loss: 0.00649 | EVM: 1.992\n",
      "0:00:38 | Epoch: 176/500 | val Loss: 0.02165 | EVM: 0.8403\n",
      "0:00:03 | Epoch: 177/500 | train Loss: 0.006387 | EVM: 1.612\n",
      "0:00:37 | Epoch: 177/500 | val Loss: 0.02232 | EVM: 0.7916\n",
      "0:00:03 | Epoch: 178/500 | train Loss: 0.006501 | EVM: 2.019\n",
      "0:00:37 | Epoch: 178/500 | val Loss: 0.02171 | EVM: 0.8681\n",
      "0:00:03 | Epoch: 179/500 | train Loss: 0.006118 | EVM: 2.275\n",
      "0:00:37 | Epoch: 179/500 | val Loss: 0.02227 | EVM: 0.8196\n",
      "0:00:03 | Epoch: 180/500 | train Loss: 0.006105 | EVM: 1.73\n",
      "0:00:37 | Epoch: 180/500 | val Loss: 0.02204 | EVM: 0.8395\n",
      "0:00:03 | Epoch: 181/500 | train Loss: 0.006049 | EVM: 1.661\n",
      "0:00:37 | Epoch: 181/500 | val Loss: 0.02226 | EVM: 0.8507\n",
      "0:00:03 | Epoch: 182/500 | train Loss: 0.005961 | EVM: 1.819\n",
      "0:00:37 | Epoch: 182/500 | val Loss: 0.02255 | EVM: 0.8448\n",
      "0:00:03 | Epoch: 183/500 | train Loss: 0.005678 | EVM: 2.452\n",
      "0:00:37 | Epoch: 183/500 | val Loss: 0.02304 | EVM: 0.8477\n",
      "0:00:03 | Epoch: 184/500 | train Loss: 0.005736 | EVM: 1.752\n",
      "0:00:37 | Epoch: 184/500 | val Loss: 0.02273 | EVM: 0.8598\n",
      "0:00:03 | Epoch: 185/500 | train Loss: 0.005536 | EVM: 1.736\n",
      "0:00:37 | Epoch: 185/500 | val Loss: 0.02235 | EVM: 0.847\n",
      "0:00:03 | Epoch: 186/500 | train Loss: 0.005248 | EVM: 1.699\n",
      "0:00:37 | Epoch: 186/500 | val Loss: 0.02315 | EVM: 0.8371\n",
      "0:00:03 | Epoch: 187/500 | train Loss: 0.005142 | EVM: 2.02\n",
      "0:00:38 | Epoch: 187/500 | val Loss: 0.02348 | EVM: 0.8574\n",
      "0:00:03 | Epoch: 188/500 | train Loss: 0.005184 | EVM: 1.719\n",
      "0:00:38 | Epoch: 188/500 | val Loss: 0.02295 | EVM: 0.9051\n",
      "0:00:03 | Epoch: 189/500 | train Loss: 0.005162 | EVM: 2.096\n",
      "0:00:37 | Epoch: 189/500 | val Loss: 0.02299 | EVM: 0.8668\n",
      "0:00:03 | Epoch: 190/500 | train Loss: 0.005022 | EVM: 1.513\n",
      "0:00:37 | Epoch: 190/500 | val Loss: 0.02378 | EVM: 0.869\n",
      "0:00:03 | Epoch: 191/500 | train Loss: 0.00484 | EVM: 1.629\n",
      "0:00:37 | Epoch: 191/500 | val Loss: 0.02346 | EVM: 0.8453\n",
      "0:00:03 | Epoch: 192/500 | train Loss: 0.004607 | EVM: 1.932\n",
      "0:00:38 | Epoch: 192/500 | val Loss: 0.02374 | EVM: 0.8755\n",
      "0:00:03 | Epoch: 193/500 | train Loss: 0.004386 | EVM: 1.466\n",
      "0:00:37 | Epoch: 193/500 | val Loss: 0.02396 | EVM: 0.8804\n",
      "0:00:03 | Epoch: 194/500 | train Loss: 0.004418 | EVM: 1.485\n",
      "0:00:37 | Epoch: 194/500 | val Loss: 0.02385 | EVM: 0.8857\n",
      "0:00:03 | Epoch: 195/500 | train Loss: 0.004328 | EVM: 1.716\n",
      "0:00:38 | Epoch: 195/500 | val Loss: 0.02501 | EVM: 0.8922\n",
      "0:00:03 | Epoch: 196/500 | train Loss: 0.0043 | EVM: 1.908\n",
      "0:00:38 | Epoch: 196/500 | val Loss: 0.02477 | EVM: 0.8673\n",
      "0:00:03 | Epoch: 197/500 | train Loss: 0.004199 | EVM: 1.792\n",
      "0:00:37 | Epoch: 197/500 | val Loss: 0.02466 | EVM: 0.9045\n",
      "0:00:03 | Epoch: 198/500 | train Loss: 0.003859 | EVM: 1.664\n",
      "0:00:37 | Epoch: 198/500 | val Loss: 0.02453 | EVM: 0.8851\n",
      "0:00:03 | Epoch: 199/500 | train Loss: 0.003906 | EVM: 1.71\n",
      "0:00:37 | Epoch: 199/500 | val Loss: 0.02498 | EVM: 0.9078\n",
      "0:00:03 | Epoch: 200/500 | train Loss: 0.003794 | EVM: 1.713\n",
      "0:00:37 | Epoch: 200/500 | val Loss: 0.02514 | EVM: 0.9204\n",
      "0:00:03 | Epoch: 201/500 | train Loss: 0.003672 | EVM: 1.272\n",
      "0:00:39 | Epoch: 201/500 | val Loss: 0.025 | EVM: 0.8724\n",
      "0:00:03 | Epoch: 202/500 | train Loss: 0.003552 | EVM: 1.776\n",
      "0:00:38 | Epoch: 202/500 | val Loss: 0.02562 | EVM: 0.8935\n",
      "0:00:03 | Epoch: 203/500 | train Loss: 0.003533 | EVM: 1.51\n",
      "0:00:37 | Epoch: 203/500 | val Loss: 0.0249 | EVM: 0.8725\n",
      "0:00:03 | Epoch: 204/500 | train Loss: 0.003165 | EVM: 1.468\n",
      "0:00:37 | Epoch: 204/500 | val Loss: 0.02582 | EVM: 0.9273\n",
      "0:00:03 | Epoch: 205/500 | train Loss: 0.003074 | EVM: 1.385\n",
      "0:00:37 | Epoch: 205/500 | val Loss: 0.02533 | EVM: 0.9092\n",
      "0:00:03 | Epoch: 206/500 | train Loss: 0.003186 | EVM: 1.676\n",
      "0:00:37 | Epoch: 206/500 | val Loss: 0.02555 | EVM: 0.9291\n",
      "0:00:03 | Epoch: 207/500 | train Loss: 0.003023 | EVM: 1.598\n",
      "0:00:37 | Epoch: 207/500 | val Loss: 0.02583 | EVM: 0.9077\n",
      "0:00:03 | Epoch: 208/500 | train Loss: 0.002973 | EVM: 1.205\n",
      "0:00:37 | Epoch: 208/500 | val Loss: 0.02579 | EVM: 0.9056\n",
      "0:00:03 | Epoch: 209/500 | train Loss: 0.002818 | EVM: 1.422\n",
      "0:00:37 | Epoch: 209/500 | val Loss: 0.02554 | EVM: 0.9046\n",
      "0:00:03 | Epoch: 210/500 | train Loss: 0.002623 | EVM: 1.099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:37 | Epoch: 210/500 | val Loss: 0.0263 | EVM: 0.9183\n",
      "0:00:03 | Epoch: 211/500 | train Loss: 0.002504 | EVM: 1.05\n",
      "0:00:37 | Epoch: 211/500 | val Loss: 0.02637 | EVM: 0.9343\n",
      "0:00:03 | Epoch: 212/500 | train Loss: 0.002557 | EVM: 1.297\n",
      "0:00:37 | Epoch: 212/500 | val Loss: 0.02644 | EVM: 0.9359\n",
      "0:00:03 | Epoch: 213/500 | train Loss: 0.002407 | EVM: 1.291\n",
      "0:00:37 | Epoch: 213/500 | val Loss: 0.02622 | EVM: 0.9404\n",
      "0:00:03 | Epoch: 214/500 | train Loss: 0.002392 | EVM: 1.452\n",
      "0:00:38 | Epoch: 214/500 | val Loss: 0.02694 | EVM: 0.9475\n",
      "0:00:03 | Epoch: 215/500 | train Loss: 0.00241 | EVM: 1.341\n",
      "0:00:37 | Epoch: 215/500 | val Loss: 0.0267 | EVM: 0.9103\n",
      "0:00:03 | Epoch: 216/500 | train Loss: 0.002263 | EVM: 1.395\n",
      "0:00:37 | Epoch: 216/500 | val Loss: 0.02691 | EVM: 0.9353\n",
      "0:00:03 | Epoch: 217/500 | train Loss: 0.002029 | EVM: 1.006\n",
      "0:00:30 | Epoch: 217/500 | val Loss: 0.02744 | EVM: 0.9606\n",
      "0:00:02 | Epoch: 218/500 | train Loss: 0.002175 | EVM: 1.432\n",
      "0:00:27 | Epoch: 218/500 | val Loss: 0.02684 | EVM: 0.9292\n",
      "0:00:02 | Epoch: 219/500 | train Loss: 0.002046 | EVM: 1.123\n",
      "0:00:27 | Epoch: 219/500 | val Loss: 0.02688 | EVM: 0.9374\n",
      "0:00:02 | Epoch: 220/500 | train Loss: 0.00185 | EVM: 1.149\n",
      "0:00:27 | Epoch: 220/500 | val Loss: 0.02757 | EVM: 0.9825\n",
      "0:00:02 | Epoch: 221/500 | train Loss: 0.001777 | EVM: 1.002\n",
      "0:00:27 | Epoch: 221/500 | val Loss: 0.02787 | EVM: 0.9354\n",
      "0:00:02 | Epoch: 222/500 | train Loss: 0.00171 | EVM: 1.266\n",
      "0:00:27 | Epoch: 222/500 | val Loss: 0.02752 | EVM: 0.9483\n",
      "0:00:02 | Epoch: 223/500 | train Loss: 0.001672 | EVM: 0.8876\n",
      "0:00:28 | Epoch: 223/500 | val Loss: 0.02795 | EVM: 0.9635\n",
      "0:00:02 | Epoch: 224/500 | train Loss: 0.001592 | EVM: 1.005\n",
      "0:00:26 | Epoch: 224/500 | val Loss: 0.02809 | EVM: 0.9434\n",
      "0:00:02 | Epoch: 225/500 | train Loss: 0.001689 | EVM: 0.9059\n",
      "0:00:27 | Epoch: 225/500 | val Loss: 0.02766 | EVM: 0.9636\n",
      "0:00:02 | Epoch: 226/500 | train Loss: 0.001614 | EVM: 1.165\n",
      "0:00:27 | Epoch: 226/500 | val Loss: 0.02836 | EVM: 0.9615\n",
      "0:00:02 | Epoch: 227/500 | train Loss: 0.001421 | EVM: 1.034\n",
      "0:00:27 | Epoch: 227/500 | val Loss: 0.02793 | EVM: 0.9667\n",
      "0:00:02 | Epoch: 228/500 | train Loss: 0.00137 | EVM: 0.8954\n",
      "0:00:27 | Epoch: 228/500 | val Loss: 0.0283 | EVM: 0.9675\n",
      "0:00:02 | Epoch: 229/500 | train Loss: 0.001299 | EVM: 0.8194\n",
      "0:00:27 | Epoch: 229/500 | val Loss: 0.02801 | EVM: 0.9576\n",
      "0:00:02 | Epoch: 230/500 | train Loss: 0.001285 | EVM: 0.9601\n",
      "0:00:27 | Epoch: 230/500 | val Loss: 0.02802 | EVM: 0.9676\n",
      "0:00:02 | Epoch: 231/500 | train Loss: 0.001254 | EVM: 0.917\n",
      "0:00:27 | Epoch: 231/500 | val Loss: 0.02855 | EVM: 0.9679\n",
      "0:00:02 | Epoch: 232/500 | train Loss: 0.001236 | EVM: 0.7751\n",
      "0:00:27 | Epoch: 232/500 | val Loss: 0.02848 | EVM: 0.9618\n",
      "0:00:02 | Epoch: 233/500 | train Loss: 0.001254 | EVM: 0.6537\n",
      "0:00:27 | Epoch: 233/500 | val Loss: 0.02839 | EVM: 0.9582\n",
      "0:00:02 | Epoch: 234/500 | train Loss: 0.001221 | EVM: 0.8959\n",
      "0:00:27 | Epoch: 234/500 | val Loss: 0.02825 | EVM: 0.9652\n",
      "0:00:02 | Epoch: 235/500 | train Loss: 0.001198 | EVM: 0.8855\n",
      "0:00:27 | Epoch: 235/500 | val Loss: 0.02833 | EVM: 0.9439\n",
      "0:00:02 | Epoch: 236/500 | train Loss: 0.001117 | EVM: 1.153\n",
      "0:00:27 | Epoch: 236/500 | val Loss: 0.02865 | EVM: 0.962\n",
      "0:00:02 | Epoch: 237/500 | train Loss: 0.001061 | EVM: 0.7189\n",
      "0:00:27 | Epoch: 237/500 | val Loss: 0.02895 | EVM: 0.9744\n",
      "0:00:02 | Epoch: 238/500 | train Loss: 0.001012 | EVM: 0.7163\n",
      "0:00:27 | Epoch: 238/500 | val Loss: 0.0291 | EVM: 0.9909\n",
      "0:00:02 | Epoch: 239/500 | train Loss: 0.0009559 | EVM: 0.7976\n",
      "0:00:27 | Epoch: 239/500 | val Loss: 0.02911 | EVM: 0.9758\n",
      "0:00:02 | Epoch: 240/500 | train Loss: 0.0008952 | EVM: 0.7322\n",
      "0:00:27 | Epoch: 240/500 | val Loss: 0.02901 | EVM: 0.9624\n",
      "0:00:02 | Epoch: 241/500 | train Loss: 0.000816 | EVM: 0.5587\n",
      "0:00:27 | Epoch: 241/500 | val Loss: 0.02916 | EVM: 0.9697\n",
      "0:00:02 | Epoch: 242/500 | train Loss: 0.0008334 | EVM: 0.6153\n",
      "0:00:27 | Epoch: 242/500 | val Loss: 0.02923 | EVM: 0.9718\n",
      "0:00:02 | Epoch: 243/500 | train Loss: 0.0007971 | EVM: 0.5457\n",
      "0:00:27 | Epoch: 243/500 | val Loss: 0.02931 | EVM: 0.9725\n",
      "0:00:02 | Epoch: 244/500 | train Loss: 0.0007459 | EVM: 0.7619\n",
      "0:00:28 | Epoch: 244/500 | val Loss: 0.02907 | EVM: 0.9738\n",
      "0:00:02 | Epoch: 245/500 | train Loss: 0.0007141 | EVM: 0.7436\n",
      "0:00:26 | Epoch: 245/500 | val Loss: 0.02947 | EVM: 0.9747\n",
      "0:00:02 | Epoch: 246/500 | train Loss: 0.0007186 | EVM: 0.6416\n",
      "0:00:27 | Epoch: 246/500 | val Loss: 0.02951 | EVM: 0.9741\n",
      "0:00:02 | Epoch: 247/500 | train Loss: 0.0007451 | EVM: 0.5796\n",
      "0:00:26 | Epoch: 247/500 | val Loss: 0.02931 | EVM: 0.9678\n",
      "0:00:02 | Epoch: 248/500 | train Loss: 0.0006811 | EVM: 0.567\n",
      "0:00:27 | Epoch: 248/500 | val Loss: 0.02953 | EVM: 0.9645\n",
      "0:00:02 | Epoch: 249/500 | train Loss: 0.0006285 | EVM: 0.5889\n",
      "0:00:27 | Epoch: 249/500 | val Loss: 0.02989 | EVM: 0.9549\n",
      "0:00:02 | Epoch: 250/500 | train Loss: 0.0005774 | EVM: 0.6217\n",
      "0:00:26 | Epoch: 250/500 | val Loss: 0.02952 | EVM: 0.9641\n",
      "0:00:02 | Epoch: 251/500 | train Loss: 0.0005762 | EVM: 0.4759\n",
      "0:00:27 | Epoch: 251/500 | val Loss: 0.02981 | EVM: 0.9807\n",
      "0:00:02 | Epoch: 252/500 | train Loss: 0.0005882 | EVM: 0.5031\n",
      "0:00:26 | Epoch: 252/500 | val Loss: 0.02952 | EVM: 0.9761\n",
      "0:00:02 | Epoch: 253/500 | train Loss: 0.0006217 | EVM: 0.6388\n",
      "0:00:27 | Epoch: 253/500 | val Loss: 0.03011 | EVM: 0.9937\n",
      "0:00:02 | Epoch: 254/500 | train Loss: 0.0005395 | EVM: 0.58\n",
      "0:00:27 | Epoch: 254/500 | val Loss: 0.02983 | EVM: 0.9911\n",
      "0:00:02 | Epoch: 255/500 | train Loss: 0.00046 | EVM: 0.5789\n",
      "0:00:27 | Epoch: 255/500 | val Loss: 0.02976 | EVM: 0.9733\n",
      "0:00:02 | Epoch: 256/500 | train Loss: 0.0004479 | EVM: 0.4697\n",
      "0:00:26 | Epoch: 256/500 | val Loss: 0.02986 | EVM: 0.9635\n",
      "0:00:02 | Epoch: 257/500 | train Loss: 0.0004247 | EVM: 0.3704\n",
      "0:00:27 | Epoch: 257/500 | val Loss: 0.03012 | EVM: 1.0\n",
      "0:00:02 | Epoch: 258/500 | train Loss: 0.0004189 | EVM: 0.6727\n",
      "0:00:27 | Epoch: 258/500 | val Loss: 0.02985 | EVM: 0.9829\n",
      "0:00:02 | Epoch: 259/500 | train Loss: 0.0004525 | EVM: 0.6375\n",
      "0:00:27 | Epoch: 259/500 | val Loss: 0.02992 | EVM: 0.9785\n",
      "0:00:02 | Epoch: 260/500 | train Loss: 0.0004156 | EVM: 0.4364\n",
      "0:00:27 | Epoch: 260/500 | val Loss: 0.03003 | EVM: 0.9882\n",
      "0:00:02 | Epoch: 261/500 | train Loss: 0.0004122 | EVM: 0.5021\n",
      "0:00:27 | Epoch: 261/500 | val Loss: 0.03041 | EVM: 0.9746\n",
      "0:00:02 | Epoch: 262/500 | train Loss: 0.0003778 | EVM: 0.4271\n",
      "0:00:27 | Epoch: 262/500 | val Loss: 0.03017 | EVM: 0.9863\n",
      "0:00:02 | Epoch: 263/500 | train Loss: 0.0003669 | EVM: 0.6132\n",
      "0:00:27 | Epoch: 263/500 | val Loss: 0.03022 | EVM: 0.9816\n",
      "0:00:02 | Epoch: 264/500 | train Loss: 0.0003273 | EVM: 0.3495\n",
      "0:00:27 | Epoch: 264/500 | val Loss: 0.0302 | EVM: 0.9768\n",
      "0:00:02 | Epoch: 265/500 | train Loss: 0.0003187 | EVM: 0.5851\n",
      "0:00:27 | Epoch: 265/500 | val Loss: 0.03012 | EVM: 0.9829\n",
      "0:00:02 | Epoch: 266/500 | train Loss: 0.0003199 | EVM: 0.4761\n",
      "0:00:27 | Epoch: 266/500 | val Loss: 0.03034 | EVM: 0.9878\n",
      "0:00:02 | Epoch: 267/500 | train Loss: 0.0003109 | EVM: 0.4303\n",
      "0:00:27 | Epoch: 267/500 | val Loss: 0.03024 | EVM: 0.9867\n",
      "0:00:02 | Epoch: 268/500 | train Loss: 0.000295 | EVM: 0.5189\n",
      "0:00:27 | Epoch: 268/500 | val Loss: 0.03052 | EVM: 0.9861\n",
      "0:00:02 | Epoch: 269/500 | train Loss: 0.0003204 | EVM: 0.3938\n",
      "0:00:27 | Epoch: 269/500 | val Loss: 0.03032 | EVM: 0.9873\n",
      "0:00:02 | Epoch: 270/500 | train Loss: 0.0002797 | EVM: 0.5453\n",
      "0:00:27 | Epoch: 270/500 | val Loss: 0.03028 | EVM: 0.9968\n",
      "0:00:02 | Epoch: 271/500 | train Loss: 0.0003004 | EVM: 0.3881\n",
      "0:00:27 | Epoch: 271/500 | val Loss: 0.03044 | EVM: 0.9835\n",
      "0:00:02 | Epoch: 272/500 | train Loss: 0.0002448 | EVM: 0.3841\n",
      "0:00:27 | Epoch: 272/500 | val Loss: 0.03033 | EVM: 0.9942\n",
      "0:00:02 | Epoch: 273/500 | train Loss: 0.0002048 | EVM: 0.402\n",
      "0:00:27 | Epoch: 273/500 | val Loss: 0.03038 | EVM: 0.9842\n",
      "0:00:02 | Epoch: 274/500 | train Loss: 0.0002191 | EVM: 0.3838\n",
      "0:00:27 | Epoch: 274/500 | val Loss: 0.03062 | EVM: 1.001\n",
      "0:00:02 | Epoch: 275/500 | train Loss: 0.0002362 | EVM: 0.4197\n",
      "0:00:27 | Epoch: 275/500 | val Loss: 0.0307 | EVM: 1.004\n",
      "0:00:02 | Epoch: 276/500 | train Loss: 0.0002469 | EVM: 0.3492\n",
      "0:00:27 | Epoch: 276/500 | val Loss: 0.03043 | EVM: 0.9816\n",
      "0:00:02 | Epoch: 277/500 | train Loss: 0.0001904 | EVM: 0.3825\n",
      "0:00:27 | Epoch: 277/500 | val Loss: 0.0305 | EVM: 0.9876\n",
      "0:00:02 | Epoch: 278/500 | train Loss: 0.0002228 | EVM: 0.3199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:27 | Epoch: 278/500 | val Loss: 0.03072 | EVM: 1.008\n",
      "0:00:02 | Epoch: 279/500 | train Loss: 0.0002063 | EVM: 0.3822\n",
      "0:00:27 | Epoch: 279/500 | val Loss: 0.03053 | EVM: 0.9862\n",
      "0:00:02 | Epoch: 280/500 | train Loss: 0.0001902 | EVM: 0.3222\n",
      "0:00:27 | Epoch: 280/500 | val Loss: 0.03066 | EVM: 0.9916\n",
      "0:00:02 | Epoch: 281/500 | train Loss: 0.0001785 | EVM: 0.3424\n",
      "0:00:27 | Epoch: 281/500 | val Loss: 0.03087 | EVM: 0.9949\n",
      "0:00:02 | Epoch: 282/500 | train Loss: 0.0001799 | EVM: 0.3518\n",
      "0:00:27 | Epoch: 282/500 | val Loss: 0.03081 | EVM: 0.9943\n",
      "0:00:02 | Epoch: 283/500 | train Loss: 0.0001693 | EVM: 0.4039\n",
      "0:00:26 | Epoch: 283/500 | val Loss: 0.03083 | EVM: 0.99\n",
      "0:00:02 | Epoch: 284/500 | train Loss: 0.0001893 | EVM: 0.4397\n",
      "0:00:26 | Epoch: 284/500 | val Loss: 0.03072 | EVM: 0.9924\n",
      "0:00:02 | Epoch: 285/500 | train Loss: 0.0002003 | EVM: 0.3096\n",
      "0:00:27 | Epoch: 285/500 | val Loss: 0.03084 | EVM: 0.9837\n",
      "0:00:02 | Epoch: 286/500 | train Loss: 0.0001795 | EVM: 0.359\n",
      "0:00:27 | Epoch: 286/500 | val Loss: 0.03068 | EVM: 1.012\n",
      "0:00:02 | Epoch: 287/500 | train Loss: 0.000177 | EVM: 0.3311\n",
      "0:00:27 | Epoch: 287/500 | val Loss: 0.03087 | EVM: 0.9937\n",
      "0:00:02 | Epoch: 288/500 | train Loss: 0.00016 | EVM: 0.3323\n",
      "0:00:27 | Epoch: 288/500 | val Loss: 0.03076 | EVM: 0.9924\n",
      "0:00:02 | Epoch: 289/500 | train Loss: 0.0001453 | EVM: 0.2698\n",
      "0:00:26 | Epoch: 289/500 | val Loss: 0.03082 | EVM: 0.9943\n",
      "0:00:02 | Epoch: 290/500 | train Loss: 0.0001346 | EVM: 0.2826\n",
      "0:00:26 | Epoch: 290/500 | val Loss: 0.03109 | EVM: 0.993\n",
      "0:00:02 | Epoch: 291/500 | train Loss: 0.0001372 | EVM: 0.2893\n",
      "0:00:27 | Epoch: 291/500 | val Loss: 0.03083 | EVM: 0.9899\n",
      "0:00:02 | Epoch: 292/500 | train Loss: 0.0001369 | EVM: 0.2703\n",
      "0:00:27 | Epoch: 292/500 | val Loss: 0.03094 | EVM: 0.9938\n",
      "0:00:02 | Epoch: 293/500 | train Loss: 0.0001264 | EVM: 0.2749\n",
      "0:00:27 | Epoch: 293/500 | val Loss: 0.03094 | EVM: 0.9961\n",
      "0:00:02 | Epoch: 294/500 | train Loss: 0.0001222 | EVM: 0.2442\n",
      "0:00:27 | Epoch: 294/500 | val Loss: 0.03086 | EVM: 0.9993\n",
      "0:00:02 | Epoch: 295/500 | train Loss: 9.719e-05 | EVM: 0.2693\n",
      "0:00:27 | Epoch: 295/500 | val Loss: 0.03091 | EVM: 0.9935\n",
      "0:00:02 | Epoch: 296/500 | train Loss: 7.904e-05 | EVM: 0.2394\n",
      "0:00:27 | Epoch: 296/500 | val Loss: 0.03084 | EVM: 0.9974\n",
      "0:00:02 | Epoch: 297/500 | train Loss: 8.469e-05 | EVM: 0.2648\n",
      "0:00:26 | Epoch: 297/500 | val Loss: 0.03091 | EVM: 0.9954\n",
      "0:00:02 | Epoch: 298/500 | train Loss: 0.0001186 | EVM: 0.2516\n",
      "0:00:26 | Epoch: 298/500 | val Loss: 0.03099 | EVM: 0.9996\n",
      "0:00:02 | Epoch: 299/500 | train Loss: 0.0001488 | EVM: 0.3223\n",
      "0:00:27 | Epoch: 299/500 | val Loss: 0.03094 | EVM: 0.9869\n",
      "0:00:02 | Epoch: 300/500 | train Loss: 0.0001495 | EVM: 0.3302\n",
      "0:00:26 | Epoch: 300/500 | val Loss: 0.03107 | EVM: 0.9962\n",
      "0:00:02 | Epoch: 301/500 | train Loss: 0.0001209 | EVM: 0.2429\n",
      "0:00:26 | Epoch: 301/500 | val Loss: 0.03093 | EVM: 0.9944\n",
      "0:00:02 | Epoch: 302/500 | train Loss: 0.0001041 | EVM: 0.278\n",
      "0:00:26 | Epoch: 302/500 | val Loss: 0.03104 | EVM: 1.001\n",
      "0:00:02 | Epoch: 303/500 | train Loss: 0.0001076 | EVM: 0.2245\n",
      "0:00:26 | Epoch: 303/500 | val Loss: 0.03099 | EVM: 1.005\n",
      "0:00:02 | Epoch: 304/500 | train Loss: 9.8e-05 | EVM: 0.2993\n",
      "0:00:26 | Epoch: 304/500 | val Loss: 0.03087 | EVM: 0.9923\n",
      "0:00:02 | Epoch: 305/500 | train Loss: 0.0001056 | EVM: 0.2111\n",
      "0:00:28 | Epoch: 305/500 | val Loss: 0.03083 | EVM: 0.995\n",
      "0:00:02 | Epoch: 306/500 | train Loss: 9.432e-05 | EVM: 0.1872\n",
      "0:00:28 | Epoch: 306/500 | val Loss: 0.03099 | EVM: 1.008\n",
      "0:00:02 | Epoch: 307/500 | train Loss: 0.0001151 | EVM: 0.256\n",
      "0:00:27 | Epoch: 307/500 | val Loss: 0.03089 | EVM: 0.9945\n",
      "0:00:02 | Epoch: 308/500 | train Loss: 0.0001088 | EVM: 0.2353\n",
      "0:00:27 | Epoch: 308/500 | val Loss: 0.03082 | EVM: 0.9905\n",
      "0:00:02 | Epoch: 309/500 | train Loss: 0.000106 | EVM: 0.3585\n",
      "0:00:27 | Epoch: 309/500 | val Loss: 0.03088 | EVM: 0.9957\n",
      "0:00:02 | Epoch: 310/500 | train Loss: 0.0001416 | EVM: 0.3295\n",
      "0:00:27 | Epoch: 310/500 | val Loss: 0.031 | EVM: 0.9922\n",
      "0:00:02 | Epoch: 311/500 | train Loss: 0.0001275 | EVM: 0.2343\n",
      "0:00:27 | Epoch: 311/500 | val Loss: 0.03087 | EVM: 0.9977\n",
      "0:00:02 | Epoch: 312/500 | train Loss: 0.0001201 | EVM: 0.1971\n",
      "0:00:27 | Epoch: 312/500 | val Loss: 0.03106 | EVM: 0.9949\n",
      "0:00:02 | Epoch: 313/500 | train Loss: 0.0001193 | EVM: 0.2877\n",
      "0:00:27 | Epoch: 313/500 | val Loss: 0.03102 | EVM: 1.006\n",
      "0:00:02 | Epoch: 314/500 | train Loss: 0.0001073 | EVM: 0.2486\n",
      "0:00:27 | Epoch: 314/500 | val Loss: 0.0309 | EVM: 0.9937\n",
      "0:00:02 | Epoch: 315/500 | train Loss: 0.0001289 | EVM: 0.3324\n",
      "0:00:27 | Epoch: 315/500 | val Loss: 0.0309 | EVM: 0.9953\n",
      "0:00:02 | Epoch: 316/500 | train Loss: 0.0001623 | EVM: 0.3226\n",
      "0:00:27 | Epoch: 316/500 | val Loss: 0.03094 | EVM: 0.9917\n",
      "0:00:02 | Epoch: 317/500 | train Loss: 0.0001353 | EVM: 0.2396\n",
      "0:00:27 | Epoch: 317/500 | val Loss: 0.03099 | EVM: 0.9978\n",
      "0:00:02 | Epoch: 318/500 | train Loss: 0.0001256 | EVM: 0.2174\n",
      "0:00:27 | Epoch: 318/500 | val Loss: 0.0309 | EVM: 0.9995\n",
      "0:00:02 | Epoch: 319/500 | train Loss: 0.0001288 | EVM: 0.3699\n",
      "0:00:27 | Epoch: 319/500 | val Loss: 0.03096 | EVM: 0.9877\n",
      "0:00:02 | Epoch: 320/500 | train Loss: 0.0001397 | EVM: 0.3487\n",
      "0:00:32 | Epoch: 320/500 | val Loss: 0.03083 | EVM: 1.002\n",
      "0:00:02 | Epoch: 321/500 | train Loss: 0.0001744 | EVM: 0.4918\n",
      "0:00:27 | Epoch: 321/500 | val Loss: 0.03093 | EVM: 0.9914\n",
      "0:00:02 | Epoch: 322/500 | train Loss: 0.0002215 | EVM: 0.2945\n",
      "0:00:28 | Epoch: 322/500 | val Loss: 0.03091 | EVM: 1.009\n",
      "0:00:02 | Epoch: 323/500 | train Loss: 0.0002872 | EVM: 0.3197\n",
      "0:00:27 | Epoch: 323/500 | val Loss: 0.0308 | EVM: 1.006\n",
      "0:00:02 | Epoch: 324/500 | train Loss: 0.0002272 | EVM: 0.25\n",
      "0:00:29 | Epoch: 324/500 | val Loss: 0.03098 | EVM: 0.99\n",
      "0:00:02 | Epoch: 325/500 | train Loss: 0.0001748 | EVM: 0.3074\n",
      "0:00:28 | Epoch: 325/500 | val Loss: 0.03097 | EVM: 1.003\n",
      "0:00:02 | Epoch: 326/500 | train Loss: 0.0001826 | EVM: 0.2436\n",
      "0:00:29 | Epoch: 326/500 | val Loss: 0.03071 | EVM: 1.001\n",
      "0:00:02 | Epoch: 327/500 | train Loss: 0.0002058 | EVM: 0.2465\n",
      "0:00:27 | Epoch: 327/500 | val Loss: 0.03067 | EVM: 1.007\n",
      "0:00:02 | Epoch: 328/500 | train Loss: 0.0001839 | EVM: 0.3088\n",
      "0:00:27 | Epoch: 328/500 | val Loss: 0.03075 | EVM: 0.993\n",
      "0:00:02 | Epoch: 329/500 | train Loss: 0.0002005 | EVM: 0.3532\n",
      "0:00:27 | Epoch: 329/500 | val Loss: 0.0309 | EVM: 0.9945\n",
      "0:00:02 | Epoch: 330/500 | train Loss: 0.0001634 | EVM: 0.2886\n",
      "0:00:28 | Epoch: 330/500 | val Loss: 0.03073 | EVM: 0.9878\n",
      "0:00:02 | Epoch: 331/500 | train Loss: 0.0001581 | EVM: 0.3164\n",
      "0:00:28 | Epoch: 331/500 | val Loss: 0.0306 | EVM: 1.006\n",
      "0:00:02 | Epoch: 332/500 | train Loss: 0.0001389 | EVM: 0.3222\n",
      "0:00:31 | Epoch: 332/500 | val Loss: 0.03071 | EVM: 0.9886\n",
      "0:00:02 | Epoch: 333/500 | train Loss: 0.0001412 | EVM: 0.237\n",
      "0:00:31 | Epoch: 333/500 | val Loss: 0.03079 | EVM: 0.9915\n",
      "0:00:02 | Epoch: 334/500 | train Loss: 0.0001509 | EVM: 0.3218\n",
      "0:00:30 | Epoch: 334/500 | val Loss: 0.03075 | EVM: 1.009\n",
      "0:00:02 | Epoch: 335/500 | train Loss: 0.0001639 | EVM: 0.3016\n",
      "0:00:31 | Epoch: 335/500 | val Loss: 0.03075 | EVM: 0.9928\n",
      "0:00:02 | Epoch: 336/500 | train Loss: 0.0001892 | EVM: 0.3595\n",
      "0:00:30 | Epoch: 336/500 | val Loss: 0.03056 | EVM: 1.007\n",
      "0:00:02 | Epoch: 337/500 | train Loss: 0.0001879 | EVM: 0.4561\n",
      "0:00:30 | Epoch: 337/500 | val Loss: 0.03083 | EVM: 0.999\n",
      "0:00:02 | Epoch: 338/500 | train Loss: 0.0002393 | EVM: 0.4194\n",
      "0:00:30 | Epoch: 338/500 | val Loss: 0.03028 | EVM: 0.9824\n",
      "0:00:02 | Epoch: 339/500 | train Loss: 0.0003154 | EVM: 0.4526\n",
      "0:00:30 | Epoch: 339/500 | val Loss: 0.03049 | EVM: 0.9985\n",
      "0:00:02 | Epoch: 340/500 | train Loss: 0.0003008 | EVM: 0.3928\n",
      "0:00:30 | Epoch: 340/500 | val Loss: 0.0307 | EVM: 1.007\n",
      "0:00:02 | Epoch: 341/500 | train Loss: 0.000267 | EVM: 0.3515\n",
      "0:00:30 | Epoch: 341/500 | val Loss: 0.03046 | EVM: 0.9847\n",
      "0:00:02 | Epoch: 342/500 | train Loss: 0.0002002 | EVM: 0.2983\n",
      "0:00:30 | Epoch: 342/500 | val Loss: 0.03078 | EVM: 1.003\n",
      "0:00:02 | Epoch: 343/500 | train Loss: 0.0001543 | EVM: 0.2833\n",
      "0:00:30 | Epoch: 343/500 | val Loss: 0.03053 | EVM: 0.9849\n",
      "0:00:02 | Epoch: 344/500 | train Loss: 0.0001276 | EVM: 0.2122\n",
      "0:00:30 | Epoch: 344/500 | val Loss: 0.03069 | EVM: 0.9892\n",
      "0:00:02 | Epoch: 345/500 | train Loss: 0.0001246 | EVM: 0.4068\n",
      "0:00:30 | Epoch: 345/500 | val Loss: 0.03059 | EVM: 0.9885\n",
      "0:00:02 | Epoch: 346/500 | train Loss: 0.0001178 | EVM: 0.2551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:30 | Epoch: 346/500 | val Loss: 0.03072 | EVM: 0.9848\n",
      "0:00:02 | Epoch: 347/500 | train Loss: 0.0001069 | EVM: 0.2204\n",
      "0:00:30 | Epoch: 347/500 | val Loss: 0.03057 | EVM: 0.9886\n",
      "0:00:02 | Epoch: 348/500 | train Loss: 8.568e-05 | EVM: 0.1499\n",
      "0:00:30 | Epoch: 348/500 | val Loss: 0.03065 | EVM: 0.9886\n",
      "0:00:02 | Epoch: 349/500 | train Loss: 6.085e-05 | EVM: 0.1685\n",
      "0:00:30 | Epoch: 349/500 | val Loss: 0.03071 | EVM: 0.9974\n",
      "0:00:02 | Epoch: 350/500 | train Loss: 5.098e-05 | EVM: 0.1693\n",
      "0:00:30 | Epoch: 350/500 | val Loss: 0.03067 | EVM: 0.9892\n",
      "0:00:02 | Epoch: 351/500 | train Loss: 5.267e-05 | EVM: 0.163\n",
      "0:00:30 | Epoch: 351/500 | val Loss: 0.03077 | EVM: 0.9956\n",
      "0:00:02 | Epoch: 352/500 | train Loss: 5.08e-05 | EVM: 0.1669\n",
      "0:00:31 | Epoch: 352/500 | val Loss: 0.0307 | EVM: 0.9871\n",
      "0:00:02 | Epoch: 353/500 | train Loss: 4.941e-05 | EVM: 0.1733\n",
      "0:00:30 | Epoch: 353/500 | val Loss: 0.03078 | EVM: 0.9945\n",
      "0:00:02 | Epoch: 354/500 | train Loss: 4.815e-05 | EVM: 0.2034\n",
      "0:00:30 | Epoch: 354/500 | val Loss: 0.03071 | EVM: 0.9927\n",
      "0:00:02 | Epoch: 355/500 | train Loss: 5.156e-05 | EVM: 0.1492\n",
      "0:00:30 | Epoch: 355/500 | val Loss: 0.03072 | EVM: 0.9891\n",
      "0:00:02 | Epoch: 356/500 | train Loss: 5.657e-05 | EVM: 0.2033\n",
      "0:00:30 | Epoch: 356/500 | val Loss: 0.03063 | EVM: 0.9936\n",
      "0:00:02 | Epoch: 357/500 | train Loss: 5.209e-05 | EVM: 0.1531\n",
      "0:00:30 | Epoch: 357/500 | val Loss: 0.03074 | EVM: 0.9962\n",
      "0:00:02 | Epoch: 358/500 | train Loss: 4.415e-05 | EVM: 0.1528\n",
      "0:00:30 | Epoch: 358/500 | val Loss: 0.03081 | EVM: 0.9959\n",
      "0:00:02 | Epoch: 359/500 | train Loss: 4.363e-05 | EVM: 0.1442\n",
      "0:00:30 | Epoch: 359/500 | val Loss: 0.03083 | EVM: 0.993\n",
      "0:00:02 | Epoch: 360/500 | train Loss: 3.817e-05 | EVM: 0.1657\n",
      "0:00:30 | Epoch: 360/500 | val Loss: 0.03075 | EVM: 0.9952\n",
      "0:00:02 | Epoch: 361/500 | train Loss: 3.508e-05 | EVM: 0.1682\n",
      "0:00:30 | Epoch: 361/500 | val Loss: 0.03062 | EVM: 0.9962\n",
      "0:00:02 | Epoch: 362/500 | train Loss: 5.344e-05 | EVM: 0.16\n",
      "0:00:30 | Epoch: 362/500 | val Loss: 0.03072 | EVM: 0.9972\n",
      "0:00:02 | Epoch: 363/500 | train Loss: 5.694e-05 | EVM: 0.1707\n",
      "0:00:30 | Epoch: 363/500 | val Loss: 0.0307 | EVM: 0.992\n",
      "0:00:02 | Epoch: 364/500 | train Loss: 4.958e-05 | EVM: 0.1493\n",
      "0:00:30 | Epoch: 364/500 | val Loss: 0.03065 | EVM: 0.993\n",
      "0:00:02 | Epoch: 365/500 | train Loss: 4.529e-05 | EVM: 0.1272\n",
      "0:00:29 | Epoch: 365/500 | val Loss: 0.03079 | EVM: 0.993\n",
      "0:00:02 | Epoch: 366/500 | train Loss: 3.358e-05 | EVM: 0.157\n",
      "0:00:30 | Epoch: 366/500 | val Loss: 0.0307 | EVM: 0.9892\n",
      "0:00:02 | Epoch: 367/500 | train Loss: 3.103e-05 | EVM: 0.1471\n",
      "0:00:30 | Epoch: 367/500 | val Loss: 0.03075 | EVM: 0.9888\n",
      "0:00:02 | Epoch: 368/500 | train Loss: 2.826e-05 | EVM: 0.1269\n",
      "0:00:30 | Epoch: 368/500 | val Loss: 0.03069 | EVM: 0.993\n",
      "0:00:02 | Epoch: 369/500 | train Loss: 2.67e-05 | EVM: 0.1358\n",
      "0:00:30 | Epoch: 369/500 | val Loss: 0.03073 | EVM: 0.9948\n",
      "0:00:02 | Epoch: 370/500 | train Loss: 3.026e-05 | EVM: 0.1472\n",
      "0:00:30 | Epoch: 370/500 | val Loss: 0.03084 | EVM: 0.9951\n",
      "0:00:02 | Epoch: 371/500 | train Loss: 3.267e-05 | EVM: 0.173\n",
      "0:00:30 | Epoch: 371/500 | val Loss: 0.03067 | EVM: 0.9939\n",
      "0:00:02 | Epoch: 372/500 | train Loss: 3.804e-05 | EVM: 0.1433\n",
      "0:00:30 | Epoch: 372/500 | val Loss: 0.03066 | EVM: 0.9983\n",
      "0:00:02 | Epoch: 373/500 | train Loss: 4.356e-05 | EVM: 0.1454\n",
      "0:00:30 | Epoch: 373/500 | val Loss: 0.0308 | EVM: 0.9885\n",
      "0:00:02 | Epoch: 374/500 | train Loss: 4.541e-05 | EVM: 0.1317\n",
      "0:00:31 | Epoch: 374/500 | val Loss: 0.03064 | EVM: 0.9981\n",
      "0:00:02 | Epoch: 375/500 | train Loss: 5.139e-05 | EVM: 0.1623\n",
      "0:00:32 | Epoch: 375/500 | val Loss: 0.03073 | EVM: 0.9955\n",
      "0:00:02 | Epoch: 376/500 | train Loss: 4.825e-05 | EVM: 0.1316\n",
      "0:00:30 | Epoch: 376/500 | val Loss: 0.03072 | EVM: 0.9973\n",
      "0:00:02 | Epoch: 377/500 | train Loss: 4.277e-05 | EVM: 0.143\n",
      "0:00:30 | Epoch: 377/500 | val Loss: 0.03081 | EVM: 0.9913\n",
      "0:00:02 | Epoch: 378/500 | train Loss: 4.696e-05 | EVM: 0.1718\n",
      "0:00:30 | Epoch: 378/500 | val Loss: 0.03073 | EVM: 0.9937\n",
      "0:00:02 | Epoch: 379/500 | train Loss: 5.155e-05 | EVM: 0.1809\n",
      "0:00:30 | Epoch: 379/500 | val Loss: 0.0308 | EVM: 0.995\n",
      "0:00:02 | Epoch: 380/500 | train Loss: 7.027e-05 | EVM: 0.2038\n",
      "0:00:30 | Epoch: 380/500 | val Loss: 0.03063 | EVM: 0.9905\n",
      "0:00:02 | Epoch: 381/500 | train Loss: 9.467e-05 | EVM: 0.2175\n",
      "0:00:30 | Epoch: 381/500 | val Loss: 0.03079 | EVM: 0.9949\n",
      "0:00:02 | Epoch: 382/500 | train Loss: 0.0001466 | EVM: 0.3479\n",
      "0:00:30 | Epoch: 382/500 | val Loss: 0.03083 | EVM: 0.988\n",
      "0:00:02 | Epoch: 383/500 | train Loss: 0.0001782 | EVM: 0.2949\n",
      "0:00:30 | Epoch: 383/500 | val Loss: 0.03077 | EVM: 0.9717\n",
      "0:00:02 | Epoch: 384/500 | train Loss: 0.0002357 | EVM: 0.4051\n",
      "0:00:30 | Epoch: 384/500 | val Loss: 0.0303 | EVM: 1.003\n",
      "0:00:02 | Epoch: 385/500 | train Loss: 0.0002569 | EVM: 0.383\n",
      "0:00:30 | Epoch: 385/500 | val Loss: 0.03085 | EVM: 0.9935\n",
      "0:00:02 | Epoch: 386/500 | train Loss: 0.0003238 | EVM: 0.5687\n",
      "0:00:30 | Epoch: 386/500 | val Loss: 0.03043 | EVM: 0.9724\n",
      "0:00:02 | Epoch: 387/500 | train Loss: 0.0003716 | EVM: 0.5178\n",
      "0:00:30 | Epoch: 387/500 | val Loss: 0.03063 | EVM: 0.9811\n",
      "0:00:02 | Epoch: 388/500 | train Loss: 0.0003493 | EVM: 0.4487\n",
      "0:00:30 | Epoch: 388/500 | val Loss: 0.03002 | EVM: 0.9924\n",
      "0:00:02 | Epoch: 389/500 | train Loss: 0.0003377 | EVM: 0.4578\n",
      "0:00:30 | Epoch: 389/500 | val Loss: 0.03001 | EVM: 0.979\n",
      "0:00:02 | Epoch: 390/500 | train Loss: 0.0004079 | EVM: 0.324\n",
      "0:00:30 | Epoch: 390/500 | val Loss: 0.03054 | EVM: 0.9878\n",
      "0:00:02 | Epoch: 391/500 | train Loss: 0.0003455 | EVM: 0.4179\n",
      "0:00:30 | Epoch: 391/500 | val Loss: 0.03003 | EVM: 0.9908\n",
      "0:00:02 | Epoch: 392/500 | train Loss: 0.0003154 | EVM: 0.408\n",
      "0:00:30 | Epoch: 392/500 | val Loss: 0.03042 | EVM: 0.9941\n",
      "0:00:02 | Epoch: 393/500 | train Loss: 0.0002887 | EVM: 0.3153\n",
      "0:00:30 | Epoch: 393/500 | val Loss: 0.03021 | EVM: 0.9952\n",
      "0:00:02 | Epoch: 394/500 | train Loss: 0.0002182 | EVM: 0.3741\n",
      "0:00:29 | Epoch: 394/500 | val Loss: 0.03013 | EVM: 0.9617\n",
      "0:00:02 | Epoch: 395/500 | train Loss: 0.0002154 | EVM: 0.421\n",
      "0:00:27 | Epoch: 395/500 | val Loss: 0.03031 | EVM: 0.9874\n",
      "0:00:02 | Epoch: 396/500 | train Loss: 0.0001663 | EVM: 0.2712\n",
      "0:00:27 | Epoch: 396/500 | val Loss: 0.03004 | EVM: 0.9827\n",
      "0:00:02 | Epoch: 397/500 | train Loss: 0.000133 | EVM: 0.32\n",
      "0:00:27 | Epoch: 397/500 | val Loss: 0.03025 | EVM: 0.9866\n",
      "0:00:02 | Epoch: 398/500 | train Loss: 0.0001131 | EVM: 0.2149\n",
      "0:00:27 | Epoch: 398/500 | val Loss: 0.03014 | EVM: 0.9871\n",
      "0:00:02 | Epoch: 399/500 | train Loss: 0.0001144 | EVM: 0.2518\n",
      "0:00:27 | Epoch: 399/500 | val Loss: 0.03032 | EVM: 0.976\n",
      "0:00:02 | Epoch: 400/500 | train Loss: 0.0001039 | EVM: 0.2049\n",
      "0:00:27 | Epoch: 400/500 | val Loss: 0.03025 | EVM: 0.9769\n",
      "0:00:02 | Epoch: 401/500 | train Loss: 9.214e-05 | EVM: 0.182\n",
      "0:00:27 | Epoch: 401/500 | val Loss: 0.03021 | EVM: 0.9788\n",
      "0:00:02 | Epoch: 402/500 | train Loss: 8.696e-05 | EVM: 0.2369\n",
      "0:00:26 | Epoch: 402/500 | val Loss: 0.03042 | EVM: 0.9811\n",
      "0:00:02 | Epoch: 403/500 | train Loss: 8.898e-05 | EVM: 0.2074\n",
      "0:00:26 | Epoch: 403/500 | val Loss: 0.03017 | EVM: 0.979\n",
      "0:00:02 | Epoch: 404/500 | train Loss: 7.889e-05 | EVM: 0.1975\n",
      "0:00:27 | Epoch: 404/500 | val Loss: 0.0303 | EVM: 0.9912\n",
      "0:00:02 | Epoch: 405/500 | train Loss: 6.311e-05 | EVM: 0.1667\n",
      "0:00:27 | Epoch: 405/500 | val Loss: 0.03023 | EVM: 0.9915\n",
      "0:00:02 | Epoch: 406/500 | train Loss: 5.997e-05 | EVM: 0.173\n",
      "0:00:27 | Epoch: 406/500 | val Loss: 0.03035 | EVM: 0.9911\n",
      "0:00:02 | Epoch: 407/500 | train Loss: 6.162e-05 | EVM: 0.2041\n",
      "0:00:30 | Epoch: 407/500 | val Loss: 0.03049 | EVM: 0.9929\n",
      "0:00:02 | Epoch: 408/500 | train Loss: 6.126e-05 | EVM: 0.2144\n",
      "0:00:27 | Epoch: 408/500 | val Loss: 0.03024 | EVM: 0.9826\n",
      "0:00:02 | Epoch: 409/500 | train Loss: 6.969e-05 | EVM: 0.2369\n",
      "0:00:27 | Epoch: 409/500 | val Loss: 0.03026 | EVM: 0.9825\n",
      "0:00:02 | Epoch: 410/500 | train Loss: 9.062e-05 | EVM: 0.2146\n",
      "0:00:26 | Epoch: 410/500 | val Loss: 0.03037 | EVM: 0.9851\n",
      "0:00:02 | Epoch: 411/500 | train Loss: 9.411e-05 | EVM: 0.1717\n",
      "0:00:27 | Epoch: 411/500 | val Loss: 0.03039 | EVM: 0.9858\n",
      "0:00:02 | Epoch: 412/500 | train Loss: 8.15e-05 | EVM: 0.1761\n",
      "0:00:26 | Epoch: 412/500 | val Loss: 0.03013 | EVM: 0.9865\n",
      "0:00:02 | Epoch: 413/500 | train Loss: 7.335e-05 | EVM: 0.2028\n",
      "0:00:26 | Epoch: 413/500 | val Loss: 0.03034 | EVM: 0.9863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:02 | Epoch: 414/500 | train Loss: 8.386e-05 | EVM: 0.2252\n",
      "0:00:26 | Epoch: 414/500 | val Loss: 0.0302 | EVM: 0.978\n",
      "0:00:02 | Epoch: 415/500 | train Loss: 8.706e-05 | EVM: 0.2195\n",
      "0:00:26 | Epoch: 415/500 | val Loss: 0.03034 | EVM: 0.9723\n",
      "0:00:02 | Epoch: 416/500 | train Loss: 0.0001023 | EVM: 0.2337\n",
      "0:00:26 | Epoch: 416/500 | val Loss: 0.03033 | EVM: 0.9933\n",
      "0:00:02 | Epoch: 417/500 | train Loss: 9.48e-05 | EVM: 0.2099\n",
      "0:00:26 | Epoch: 417/500 | val Loss: 0.03026 | EVM: 0.9873\n",
      "0:00:02 | Epoch: 418/500 | train Loss: 8.876e-05 | EVM: 0.2116\n",
      "0:00:27 | Epoch: 418/500 | val Loss: 0.03007 | EVM: 0.977\n",
      "0:00:02 | Epoch: 419/500 | train Loss: 0.0001133 | EVM: 0.2474\n",
      "0:00:26 | Epoch: 419/500 | val Loss: 0.03019 | EVM: 0.9695\n",
      "0:00:02 | Epoch: 420/500 | train Loss: 0.0001107 | EVM: 0.2119\n",
      "0:00:27 | Epoch: 420/500 | val Loss: 0.03047 | EVM: 0.9898\n",
      "0:00:02 | Epoch: 421/500 | train Loss: 0.0001033 | EVM: 0.234\n",
      "0:00:27 | Epoch: 421/500 | val Loss: 0.03036 | EVM: 0.9889\n",
      "0:00:02 | Epoch: 422/500 | train Loss: 0.0001267 | EVM: 0.2332\n",
      "0:00:26 | Epoch: 422/500 | val Loss: 0.03023 | EVM: 0.9713\n",
      "0:00:02 | Epoch: 423/500 | train Loss: 0.0001281 | EVM: 0.2372\n",
      "0:00:27 | Epoch: 423/500 | val Loss: 0.03045 | EVM: 0.9634\n",
      "0:00:02 | Epoch: 424/500 | train Loss: 0.0001544 | EVM: 0.3114\n",
      "0:00:27 | Epoch: 424/500 | val Loss: 0.03031 | EVM: 0.9774\n",
      "0:00:02 | Epoch: 425/500 | train Loss: 0.0001822 | EVM: 0.3802\n",
      "0:00:26 | Epoch: 425/500 | val Loss: 0.03007 | EVM: 0.9895\n",
      "0:00:02 | Epoch: 426/500 | train Loss: 0.0001601 | EVM: 0.3005\n",
      "0:00:26 | Epoch: 426/500 | val Loss: 0.03005 | EVM: 0.9762\n",
      "0:00:02 | Epoch: 427/500 | train Loss: 0.0001534 | EVM: 0.2795\n",
      "0:00:26 | Epoch: 427/500 | val Loss: 0.03002 | EVM: 0.9745\n",
      "0:00:02 | Epoch: 428/500 | train Loss: 0.0001646 | EVM: 0.3495\n",
      "0:00:27 | Epoch: 428/500 | val Loss: 0.03027 | EVM: 0.9851\n",
      "0:00:02 | Epoch: 429/500 | train Loss: 0.0002262 | EVM: 0.2661\n",
      "0:00:26 | Epoch: 429/500 | val Loss: 0.03034 | EVM: 0.9805\n",
      "0:00:02 | Epoch: 430/500 | train Loss: 0.0002334 | EVM: 0.3652\n",
      "0:00:27 | Epoch: 430/500 | val Loss: 0.02994 | EVM: 0.9875\n",
      "0:00:02 | Epoch: 431/500 | train Loss: 0.0002629 | EVM: 0.3641\n",
      "0:00:27 | Epoch: 431/500 | val Loss: 0.03045 | EVM: 0.9754\n",
      "0:00:02 | Epoch: 432/500 | train Loss: 0.0003066 | EVM: 0.432\n",
      "0:00:26 | Epoch: 432/500 | val Loss: 0.03039 | EVM: 0.9768\n",
      "0:00:02 | Epoch: 433/500 | train Loss: 0.0002567 | EVM: 0.3504\n",
      "0:00:28 | Epoch: 433/500 | val Loss: 0.02992 | EVM: 0.9857\n",
      "0:00:02 | Epoch: 434/500 | train Loss: 0.0002082 | EVM: 0.3341\n",
      "0:00:27 | Epoch: 434/500 | val Loss: 0.03008 | EVM: 0.969\n",
      "0:00:02 | Epoch: 435/500 | train Loss: 0.000211 | EVM: 0.2615\n",
      "0:00:26 | Epoch: 435/500 | val Loss: 0.02999 | EVM: 0.9604\n",
      "0:00:02 | Epoch: 436/500 | train Loss: 0.0001977 | EVM: 0.3569\n",
      "0:00:26 | Epoch: 436/500 | val Loss: 0.03013 | EVM: 0.9554\n",
      "0:00:02 | Epoch: 437/500 | train Loss: 0.0001836 | EVM: 0.3333\n",
      "0:00:26 | Epoch: 437/500 | val Loss: 0.03005 | EVM: 0.9677\n",
      "0:00:02 | Epoch: 438/500 | train Loss: 0.0001501 | EVM: 0.2453\n",
      "0:00:27 | Epoch: 438/500 | val Loss: 0.02997 | EVM: 0.9727\n",
      "0:00:02 | Epoch: 439/500 | train Loss: 0.0001362 | EVM: 0.2388\n",
      "0:00:26 | Epoch: 439/500 | val Loss: 0.02988 | EVM: 0.9648\n",
      "0:00:02 | Epoch: 440/500 | train Loss: 0.0001227 | EVM: 0.2938\n",
      "0:00:28 | Epoch: 440/500 | val Loss: 0.03007 | EVM: 0.9831\n",
      "0:00:02 | Epoch: 441/500 | train Loss: 0.0001314 | EVM: 0.242\n",
      "0:00:27 | Epoch: 441/500 | val Loss: 0.03009 | EVM: 0.9669\n",
      "0:00:02 | Epoch: 442/500 | train Loss: 0.0001624 | EVM: 0.3227\n",
      "0:00:26 | Epoch: 442/500 | val Loss: 0.03013 | EVM: 0.96\n",
      "0:00:02 | Epoch: 443/500 | train Loss: 0.0001548 | EVM: 0.2576\n",
      "0:00:26 | Epoch: 443/500 | val Loss: 0.03001 | EVM: 0.9749\n",
      "0:00:02 | Epoch: 444/500 | train Loss: 0.0001622 | EVM: 0.3209\n",
      "0:00:27 | Epoch: 444/500 | val Loss: 0.03016 | EVM: 0.982\n",
      "0:00:02 | Epoch: 445/500 | train Loss: 0.0001713 | EVM: 0.3004\n",
      "0:00:27 | Epoch: 445/500 | val Loss: 0.02994 | EVM: 0.9671\n",
      "0:00:02 | Epoch: 446/500 | train Loss: 0.0001558 | EVM: 0.2503\n",
      "0:00:27 | Epoch: 446/500 | val Loss: 0.02981 | EVM: 0.9666\n",
      "0:00:02 | Epoch: 447/500 | train Loss: 0.0001319 | EVM: 0.2406\n",
      "0:00:27 | Epoch: 447/500 | val Loss: 0.03 | EVM: 0.9774\n",
      "0:00:02 | Epoch: 448/500 | train Loss: 0.0001165 | EVM: 0.1896\n",
      "0:00:26 | Epoch: 448/500 | val Loss: 0.0299 | EVM: 0.9738\n",
      "0:00:02 | Epoch: 449/500 | train Loss: 9.399e-05 | EVM: 0.2191\n",
      "0:00:26 | Epoch: 449/500 | val Loss: 0.03011 | EVM: 0.9734\n",
      "0:00:02 | Epoch: 450/500 | train Loss: 9.497e-05 | EVM: 0.2304\n",
      "0:00:26 | Epoch: 450/500 | val Loss: 0.02977 | EVM: 0.9746\n",
      "0:00:02 | Epoch: 451/500 | train Loss: 6.914e-05 | EVM: 0.1795\n",
      "0:00:26 | Epoch: 451/500 | val Loss: 0.02988 | EVM: 0.9665\n",
      "0:00:02 | Epoch: 452/500 | train Loss: 7.121e-05 | EVM: 0.1871\n",
      "0:00:26 | Epoch: 452/500 | val Loss: 0.02979 | EVM: 0.9634\n",
      "0:00:02 | Epoch: 453/500 | train Loss: 7.766e-05 | EVM: 0.2382\n",
      "0:00:27 | Epoch: 453/500 | val Loss: 0.03005 | EVM: 0.976\n",
      "0:00:02 | Epoch: 454/500 | train Loss: 8.578e-05 | EVM: 0.1859\n",
      "0:00:27 | Epoch: 454/500 | val Loss: 0.02971 | EVM: 0.9602\n",
      "0:00:02 | Epoch: 455/500 | train Loss: 8.488e-05 | EVM: 0.1943\n",
      "0:00:26 | Epoch: 455/500 | val Loss: 0.03004 | EVM: 0.976\n",
      "0:00:02 | Epoch: 456/500 | train Loss: 7.962e-05 | EVM: 0.2058\n",
      "0:00:26 | Epoch: 456/500 | val Loss: 0.02992 | EVM: 0.982\n",
      "0:00:02 | Epoch: 457/500 | train Loss: 9.457e-05 | EVM: 0.2465\n",
      "0:00:27 | Epoch: 457/500 | val Loss: 0.02987 | EVM: 0.9642\n",
      "0:00:02 | Epoch: 458/500 | train Loss: 0.0001201 | EVM: 0.2686\n",
      "0:00:26 | Epoch: 458/500 | val Loss: 0.02995 | EVM: 0.9766\n",
      "0:00:02 | Epoch: 459/500 | train Loss: 9.8e-05 | EVM: 0.2084\n",
      "0:00:26 | Epoch: 459/500 | val Loss: 0.02987 | EVM: 0.972\n",
      "0:00:02 | Epoch: 460/500 | train Loss: 9.432e-05 | EVM: 0.1625\n",
      "0:00:27 | Epoch: 460/500 | val Loss: 0.02983 | EVM: 0.9723\n",
      "0:00:02 | Epoch: 461/500 | train Loss: 8.552e-05 | EVM: 0.1798\n",
      "0:00:26 | Epoch: 461/500 | val Loss: 0.03002 | EVM: 0.9706\n",
      "0:00:02 | Epoch: 462/500 | train Loss: 9.478e-05 | EVM: 0.1851\n",
      "0:00:26 | Epoch: 462/500 | val Loss: 0.02981 | EVM: 0.9715\n",
      "0:00:02 | Epoch: 463/500 | train Loss: 9.059e-05 | EVM: 0.2287\n",
      "0:00:26 | Epoch: 463/500 | val Loss: 0.02958 | EVM: 0.9658\n",
      "0:00:02 | Epoch: 464/500 | train Loss: 9.238e-05 | EVM: 0.1933\n",
      "0:00:26 | Epoch: 464/500 | val Loss: 0.02993 | EVM: 0.9721\n",
      "0:00:02 | Epoch: 465/500 | train Loss: 8.606e-05 | EVM: 0.2226\n",
      "0:00:26 | Epoch: 465/500 | val Loss: 0.02976 | EVM: 0.9644\n",
      "0:00:02 | Epoch: 466/500 | train Loss: 0.0001167 | EVM: 0.3196\n",
      "0:00:26 | Epoch: 466/500 | val Loss: 0.02983 | EVM: 0.9871\n",
      "0:00:02 | Epoch: 467/500 | train Loss: 0.0001381 | EVM: 0.1827\n",
      "0:00:27 | Epoch: 467/500 | val Loss: 0.02991 | EVM: 0.9699\n",
      "0:00:02 | Epoch: 468/500 | train Loss: 0.0001356 | EVM: 0.3077\n",
      "0:00:26 | Epoch: 468/500 | val Loss: 0.03003 | EVM: 0.9686\n",
      "0:00:02 | Epoch: 469/500 | train Loss: 0.0001598 | EVM: 0.2432\n",
      "0:00:26 | Epoch: 469/500 | val Loss: 0.02963 | EVM: 0.9606\n",
      "0:00:02 | Epoch: 470/500 | train Loss: 0.0001873 | EVM: 0.3761\n",
      "0:00:26 | Epoch: 470/500 | val Loss: 0.0296 | EVM: 0.9534\n",
      "0:00:02 | Epoch: 471/500 | train Loss: 0.00019 | EVM: 0.3783\n",
      "0:00:26 | Epoch: 471/500 | val Loss: 0.02988 | EVM: 0.9555\n",
      "0:00:02 | Epoch: 472/500 | train Loss: 0.00021 | EVM: 0.3801\n",
      "0:00:26 | Epoch: 472/500 | val Loss: 0.03001 | EVM: 0.9743\n",
      "0:00:02 | Epoch: 473/500 | train Loss: 0.0001838 | EVM: 0.2936\n",
      "0:00:27 | Epoch: 473/500 | val Loss: 0.02965 | EVM: 0.9825\n",
      "0:00:02 | Epoch: 474/500 | train Loss: 0.0001515 | EVM: 0.2856\n",
      "0:00:27 | Epoch: 474/500 | val Loss: 0.0299 | EVM: 0.9708\n",
      "0:00:02 | Epoch: 475/500 | train Loss: 0.0001652 | EVM: 0.2491\n",
      "0:00:27 | Epoch: 475/500 | val Loss: 0.02974 | EVM: 0.9689\n",
      "0:00:02 | Epoch: 476/500 | train Loss: 0.0001474 | EVM: 0.2863\n",
      "0:00:26 | Epoch: 476/500 | val Loss: 0.02958 | EVM: 0.9762\n",
      "0:00:02 | Epoch: 477/500 | train Loss: 0.0001435 | EVM: 0.2643\n",
      "0:00:26 | Epoch: 477/500 | val Loss: 0.0296 | EVM: 0.9649\n",
      "0:00:02 | Epoch: 478/500 | train Loss: 0.0001375 | EVM: 0.2659\n",
      "0:00:26 | Epoch: 478/500 | val Loss: 0.02963 | EVM: 0.9718\n",
      "0:00:02 | Epoch: 479/500 | train Loss: 0.0001181 | EVM: 0.2614\n",
      "0:00:27 | Epoch: 479/500 | val Loss: 0.02956 | EVM: 0.9795\n",
      "0:00:02 | Epoch: 480/500 | train Loss: 0.0001192 | EVM: 0.2442\n",
      "0:00:27 | Epoch: 480/500 | val Loss: 0.02958 | EVM: 0.9704\n",
      "0:00:02 | Epoch: 481/500 | train Loss: 0.0001286 | EVM: 0.2698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:27 | Epoch: 481/500 | val Loss: 0.02972 | EVM: 0.9674\n",
      "0:00:02 | Epoch: 482/500 | train Loss: 0.000125 | EVM: 0.2019\n",
      "0:00:27 | Epoch: 482/500 | val Loss: 0.02956 | EVM: 0.9751\n",
      "0:00:02 | Epoch: 483/500 | train Loss: 0.0001343 | EVM: 0.3228\n",
      "0:00:28 | Epoch: 483/500 | val Loss: 0.02969 | EVM: 0.9641\n",
      "0:00:02 | Epoch: 484/500 | train Loss: 0.0001073 | EVM: 0.3159\n",
      "0:00:27 | Epoch: 484/500 | val Loss: 0.02963 | EVM: 0.9672\n",
      "0:00:02 | Epoch: 485/500 | train Loss: 0.0001083 | EVM: 0.2587\n",
      "0:00:27 | Epoch: 485/500 | val Loss: 0.02964 | EVM: 0.9611\n",
      "0:00:02 | Epoch: 486/500 | train Loss: 0.0001114 | EVM: 0.234\n",
      "0:00:26 | Epoch: 486/500 | val Loss: 0.02962 | EVM: 0.9739\n",
      "0:00:02 | Epoch: 487/500 | train Loss: 9.853e-05 | EVM: 0.1979\n",
      "0:00:27 | Epoch: 487/500 | val Loss: 0.02964 | EVM: 0.9661\n",
      "0:00:02 | Epoch: 488/500 | train Loss: 9.962e-05 | EVM: 0.2766\n",
      "0:00:27 | Epoch: 488/500 | val Loss: 0.02953 | EVM: 0.9726\n",
      "0:00:02 | Epoch: 489/500 | train Loss: 9.77e-05 | EVM: 0.1986\n",
      "0:00:27 | Epoch: 489/500 | val Loss: 0.02958 | EVM: 0.9654\n",
      "0:00:02 | Epoch: 490/500 | train Loss: 8.297e-05 | EVM: 0.2008\n",
      "0:00:27 | Epoch: 490/500 | val Loss: 0.02948 | EVM: 0.9641\n",
      "0:00:02 | Epoch: 491/500 | train Loss: 0.0001154 | EVM: 0.3059\n",
      "0:00:27 | Epoch: 491/500 | val Loss: 0.02943 | EVM: 0.96\n",
      "0:00:02 | Epoch: 492/500 | train Loss: 0.0001457 | EVM: 0.2885\n",
      "0:00:27 | Epoch: 492/500 | val Loss: 0.02939 | EVM: 0.9661\n",
      "0:00:02 | Epoch: 493/500 | train Loss: 0.0001255 | EVM: 0.2542\n",
      "0:00:27 | Epoch: 493/500 | val Loss: 0.02957 | EVM: 0.9515\n",
      "0:00:02 | Epoch: 494/500 | train Loss: 0.0001446 | EVM: 0.2828\n",
      "0:00:27 | Epoch: 494/500 | val Loss: 0.02958 | EVM: 0.9607\n",
      "0:00:02 | Epoch: 495/500 | train Loss: 0.0001279 | EVM: 0.2685\n",
      "0:00:27 | Epoch: 495/500 | val Loss: 0.02959 | EVM: 0.9537\n",
      "0:00:02 | Epoch: 496/500 | train Loss: 0.0001073 | EVM: 0.2585\n",
      "0:00:27 | Epoch: 496/500 | val Loss: 0.02949 | EVM: 0.9527\n",
      "0:00:02 | Epoch: 497/500 | train Loss: 9.5e-05 | EVM: 0.1621\n",
      "0:00:27 | Epoch: 497/500 | val Loss: 0.02944 | EVM: 0.956\n",
      "0:00:02 | Epoch: 498/500 | train Loss: 8.357e-05 | EVM: 0.2611\n",
      "0:00:27 | Epoch: 498/500 | val Loss: 0.02974 | EVM: 0.9654\n",
      "0:00:02 | Epoch: 499/500 | train Loss: 8.354e-05 | EVM: 0.2575\n",
      "0:00:27 | Epoch: 499/500 | val Loss: 0.02949 | EVM: 0.9547\n",
      "0:00:02 | Epoch: 500/500 | train Loss: 9.827e-05 | EVM: 0.2052\n",
      "0:00:27 | Epoch: 500/500 | val Loss: 0.02979 | EVM: 0.9589\n"
     ]
    }
   ],
   "source": [
    "df_dir = '../data/input/'\n",
    "df0 = pd.read_csv(df_dir+'prbs.csv', index_col=0)\n",
    "\n",
    "condition0 = (df0['N']==13) & (df0['itr']==1) & (df0['form']=='RZ16QAM') & (df0['n']==32) & (df0['equalize']==False) & (df0['baudrate']==28) & (df0['PdBm']==1)\n",
    "sgnl0 = load_pickle(df0[condition0].iloc[0]['data_path'])\n",
    "lc0 = sgnl0.linear_compensation(2500, sgnl0.signal['x_2500'])\n",
    "x0, y0 = data_shaping2(sgnl0.signal['x_0'][16::32], lc0[16::32], max_tap, tap)\n",
    "\n",
    "condition1 = (df0['N']==17) & (df0['itr']==1) & (df0['form']=='RZ16QAM') & (df0['n']==32) & (df0['equalize']==False) & (df0['baudrate']==28) & (df0['PdBm']==1)\n",
    "sgnl1 = load_pickle(df0[condition1].iloc[0]['data_path'])\n",
    "lc1 = sgnl1.linear_compensation(2500, sgnl1.signal['x_2500'])\n",
    "x1, y1 = data_shaping2(sgnl1.signal['x_0'][16::32], lc1[16::32], max_tap, tap)\n",
    "\n",
    "mean = np.mean(x0)\n",
    "std = np.std(x0)\n",
    "\n",
    "train_dataset = Dataset(x=x0, y=y0, mean=mean, std=std)\n",
    "val_dataset = Dataset(x=x1, y=y1, mean=mean, std=std)\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "dataloaders_dict = {'train': train_dataloader, 'val': val_dataloader}\n",
    "\n",
    "model = LSTM(input_dim=4, hidden_dim=hidden_dim, output_dim=2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "\n",
    "model = train_model(model=model, dataloaders_dict=dataloaders_dict, criterion=criterion, optimizer=optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
